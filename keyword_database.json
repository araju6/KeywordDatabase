{
  "keywords": {
    "Convolutional Neural Network": [
      {
        "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
        "url": "https://doi.org/10.1007/bf00344251",
        "abstract": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname \u201cneocognitron\u201d. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \u201cS-cells\u201d, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \u201cC-cells\u201d similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any \u201cteacher\u201d during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.",
        "citations": 5222,
        "reasoning": "Convolutional networks were inspired by biological processes [ 18 ] [ 19 ] [ 20 ] [ 21 ] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex . \u2013 *Claim relates the biological origin of CNNs.* - [\"Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position\"] :-: Fukushima, Kunihiko (1980)."
      },
      {
        "title": "Receptive fields and functional architecture of monkey striate cortex",
        "url": "https://doi.org/10.1113/jphysiol.1968.sp008455",
        "abstract": "1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded.2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent.3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other.4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers (II and the upper two-thirds of III) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer III, and in IV A and IV B. In layer IV B they form a large proportion of the population, whereas complex cells are rare. In layers IV A and IV B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer IV most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and VI contain mostly complex and hypercomplex cells, binocularly driven.5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer IV, the higher orders in the upper and lower layers.",
        "citations": 6513,
        "reasoning": "Their 1968 paper identified two basic visual cell types in the brain: [ 19 ] \u2013 *This claim directly references a paper that outlines the discovery of visual cell types which would go on to be an integral concept of CNNs* - [\"Receptive fields and functional architecture of monkey striate cortex\"] :-: Hubel, D. H.; Wiesel, T. N. (1968)."
      },
      {
        "title": "Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements",
        "url": "https://doi.org/10.1109/tssc.1969.300225",
        "abstract": "A new type of visual feature extracting network has been synthesized, and the response of the network has been simulated on a digital computer. This research has been done as a first step towards the realization of a recognizer of handwritten characters. The design of the network was suggested by biological systems, especially, the visual systems of cat and monkey. The network is composed of analog threshold elements connected in layers. Each analog threshold element receives inputs from a large number of elements in the neighbouring layers and performs its own special functions. It takes care of one restricted part of the photoreceptor layer, on which an input pattem is presented, and it responds to one particular feature of the input pattem, such as brightness contrast, a dot in the pattern, a line segment of a particular orientation, or an end of the line. This means that the network performs parallel processing of the information. With the propagation of the information through the layered network, the input pattern is successively decomposed into dots, groups of line segments of the same orientation, and the ends of these line segments.",
        "citations": 227,
        "reasoning": "In 1969, Kunihiko Fukushima introduced a multilayer visual feature detection network, inspired by the above-mentioned work of Hubel and Wiesel, in which \"All the elements in one layer have the same set of interconnecting coefficients; the arrangement of the elements and their interconnections are all homogeneous over a given layer.\" \u2013 *This sentence describes the introduction of a network with characteristics of a CNN* - [\"Visual feature extraction by a multilayered network of analog threshold elements\"] :-: Kunihiko Fukushima (1969)."
      },
      {
        "title": "An Artificial Neural Network for Spatio-Temporal Bipolar Patterns: Application to Phoneme Classification",
        "url": null,
        "abstract": "An artificial neural network is developed to recognize spatio-temporal bipolar patterns associatively. The function of a formal neuron is generalized by replacing multiplication with convolution, weights with transfer functions, and thresholding with nonlinear transform following adaptation. The Hebbian learn\u00ad ing rule and the delta learning rule are generalized accordingly, resulting in the learning of weights and delays. The neural network which was first developed for spatial patterns was thus generalized for spatio-temporal patterns. It was tested using a set of bipolar input patterns derived from speech signals, showing robust classification of 30 model phonemes.",
        "citations": 52,
        "reasoning": "The term \"convolution\" first appears in neural networks in a paper by Toshiteru Homma, Les Atlas, and Robert Marks II at the first Conference on Neural Information Processing Systems in 1987. \u2013 *This sentence explicitly states when and where the term \"convolution\" was first used in the context of neural networks, citing a specific paper.* - [\"An Artificial Neural Network for Spatio-Temporal Bipolar Patterns: Application to Phoneme Classification\"] :-: <Toshiteru Homma, Les Atlas, Robert Marks II> (<1987>)"
      },
      {
        "title": "Phoneme recognition using time-delay neural networks",
        "url": "https://doi.org/10.1109/29.21701",
        "abstract": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
        "citations": 2574,
        "reasoning": "A TDNN is a 1-D convolutional neural net where the convolution is performed along the time axis of the data. It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation . \u2013 *States that TDNN was the first CNN to utilize weight sharing and backpropagation.* - [\"Phoneme Recognition Using Time-Delay Neural Networks\"] :-: Alexander Waibel et al. (1989)"
      },
      {
        "title": "Backpropagation Applied to Handwritten Zip Code Recognition",
        "url": "https://doi.org/10.1162/neco.1989.1.4.541",
        "abstract": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.",
        "citations": 10875,
        "reasoning": "Yann LeCun et al. (1989) used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. \u2013 *This claim explains how LeCun used backpropagation to learn the convolution kernel coefficients* - [\"Backpropagation Applied to Handwritten Zip Code Recognition\"] :-: Y. LeCun et al. (1989)"
      },
      {
        "title": "Learning algorithms for classification: A comparison on handwritten digit recognition",
        "url": null,
        "abstract": "No abstract found",
        "citations": 380,
        "reasoning": "LeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1995, [54] classifies hand-written numbers on checks ( British English : cheques ) digitized in 32x32 pixel images. \u2013 *This sentence identifies LeNet-5 as a pioneering CNN and references a paper by LeCun et al.* - [\"Learning algorithms for classification: A comparison on handwritten digit recognition\"] :-: Lecun, Y.; Jackel, L. D.; Bottou, L.; Cortes, C.; Denker, J. S.; Drucker, H.; Guyon, I.; Muller, U. A.; Sackinger, E.; Simard, P.; Vapnik, V. (1995)"
      },
      {
        "title": "Gradient-based learning applied to document recognition",
        "url": "https://doi.org/10.1109/5.726791",
        "abstract": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.",
        "citations": 51526,
        "reasoning": "The system was integrated in NCR 's check reading systems, and fielded in several American banks since June 1996, reading millions of checks per day. [55] - *The citation [55] corresponds to the paper \"Gradient-based learning applied to document recognition\" by Lecun et al. which elaborates on practical applications of LeNet, specifically for document recognition* - [\"Gradient-based learning applied to document recognition\"] :-: Lecun, Y.; Bottou, L.; Bengio, Y.; Haffner, P. (1998)"
      }
    ],
    "propagation": [
      {
        "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
        "url": "https://doi.org/10.1007/bf00344251",
        "abstract": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname \u201cneocognitron\u201d. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \u201cS-cells\u201d, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \u201cC-cells\u201d similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any \u201cteacher\u201d during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.",
        "citations": 5222,
        "reasoning": "Convolutional networks were inspired by biological processes [ 18 ] [ 19 ] [ 20 ] [ 21 ] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex . \u2013 *Claim relates the biological origin of CNNs.* - [\"Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position\"] :-: Fukushima, Kunihiko (1980)."
      },
      {
        "title": "Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements",
        "url": "https://doi.org/10.1109/tssc.1969.300225",
        "abstract": "A new type of visual feature extracting network has been synthesized, and the response of the network has been simulated on a digital computer. This research has been done as a first step towards the realization of a recognizer of handwritten characters. The design of the network was suggested by biological systems, especially, the visual systems of cat and monkey. The network is composed of analog threshold elements connected in layers. Each analog threshold element receives inputs from a large number of elements in the neighbouring layers and performs its own special functions. It takes care of one restricted part of the photoreceptor layer, on which an input pattem is presented, and it responds to one particular feature of the input pattem, such as brightness contrast, a dot in the pattern, a line segment of a particular orientation, or an end of the line. This means that the network performs parallel processing of the information. With the propagation of the information through the layered network, the input pattern is successively decomposed into dots, groups of line segments of the same orientation, and the ends of these line segments.",
        "citations": 227,
        "reasoning": "In 1969, Kunihiko Fukushima introduced a multilayer visual feature detection network, inspired by the above-mentioned work of Hubel and Wiesel, in which \"All the elements in one layer have the same set of interconnecting coefficients; the arrangement of the elements and their interconnections are all homogeneous over a given layer.\" \u2013 *This sentence describes the introduction of a network with characteristics of a CNN* - [\"Visual feature extraction by a multilayered network of analog threshold elements\"] :-: Kunihiko Fukushima (1969)."
      },
      {
        "title": "Phoneme recognition using time-delay neural networks",
        "url": "https://doi.org/10.1109/29.21701",
        "abstract": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
        "citations": 2574,
        "reasoning": "A TDNN is a 1-D convolutional neural net where the convolution is performed along the time axis of the data. It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation . \u2013 *States that TDNN was the first CNN to utilize weight sharing and backpropagation.* - [\"Phoneme Recognition Using Time-Delay Neural Networks\"] :-: Alexander Waibel et al. (1989)"
      },
      {
        "title": "Backpropagation Applied to Handwritten Zip Code Recognition",
        "url": "https://doi.org/10.1162/neco.1989.1.4.541",
        "abstract": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.",
        "citations": 10875,
        "reasoning": "Yann LeCun et al. (1989) used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. \u2013 *This claim explains how LeCun used backpropagation to learn the convolution kernel coefficients* - [\"Backpropagation Applied to Handwritten Zip Code Recognition\"] :-: Y. LeCun et al. (1989)"
      },
      {
        "title": "Gradient-based learning applied to document recognition",
        "url": "https://doi.org/10.1109/5.726791",
        "abstract": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.",
        "citations": 51526,
        "reasoning": "The system was integrated in NCR 's check reading systems, and fielded in several American banks since June 1996, reading millions of checks per day. [55] - *The citation [55] corresponds to the paper \"Gradient-based learning applied to document recognition\" by Lecun et al. which elaborates on practical applications of LeNet, specifically for document recognition* - [\"Gradient-based learning applied to document recognition\"] :-: Lecun, Y.; Bottou, L.; Bengio, Y.; Haffner, P. (1998)"
      }
    ],
    "Neocognitron": [
      {
        "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
        "url": "https://doi.org/10.1007/bf00344251",
        "abstract": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname \u201cneocognitron\u201d. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \u201cS-cells\u201d, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \u201cC-cells\u201d similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any \u201cteacher\u201d during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.",
        "citations": 5222,
        "reasoning": "Convolutional networks were inspired by biological processes [ 18 ] [ 19 ] [ 20 ] [ 21 ] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex . \u2013 *Claim relates the biological origin of CNNs.* - [\"Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position\"] :-: Fukushima, Kunihiko (1980)."
      },
      {
        "title": "Receptive fields and functional architecture of monkey striate cortex",
        "url": "https://doi.org/10.1113/jphysiol.1968.sp008455",
        "abstract": "1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded.2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent.3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other.4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers (II and the upper two-thirds of III) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer III, and in IV A and IV B. In layer IV B they form a large proportion of the population, whereas complex cells are rare. In layers IV A and IV B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer IV most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and VI contain mostly complex and hypercomplex cells, binocularly driven.5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer IV, the higher orders in the upper and lower layers.",
        "citations": 6513,
        "reasoning": "Their 1968 paper identified two basic visual cell types in the brain: [ 19 ] \u2013 *This claim directly references a paper that outlines the discovery of visual cell types which would go on to be an integral concept of CNNs* - [\"Receptive fields and functional architecture of monkey striate cortex\"] :-: Hubel, D. H.; Wiesel, T. N. (1968)."
      },
      {
        "title": "Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements",
        "url": "https://doi.org/10.1109/tssc.1969.300225",
        "abstract": "A new type of visual feature extracting network has been synthesized, and the response of the network has been simulated on a digital computer. This research has been done as a first step towards the realization of a recognizer of handwritten characters. The design of the network was suggested by biological systems, especially, the visual systems of cat and monkey. The network is composed of analog threshold elements connected in layers. Each analog threshold element receives inputs from a large number of elements in the neighbouring layers and performs its own special functions. It takes care of one restricted part of the photoreceptor layer, on which an input pattem is presented, and it responds to one particular feature of the input pattem, such as brightness contrast, a dot in the pattern, a line segment of a particular orientation, or an end of the line. This means that the network performs parallel processing of the information. With the propagation of the information through the layered network, the input pattern is successively decomposed into dots, groups of line segments of the same orientation, and the ends of these line segments.",
        "citations": 227,
        "reasoning": "In 1969, Kunihiko Fukushima introduced a multilayer visual feature detection network, inspired by the above-mentioned work of Hubel and Wiesel, in which \"All the elements in one layer have the same set of interconnecting coefficients; the arrangement of the elements and their interconnections are all homogeneous over a given layer.\" \u2013 *This sentence describes the introduction of a network with characteristics of a CNN* - [\"Visual feature extraction by a multilayered network of analog threshold elements\"] :-: Kunihiko Fukushima (1969)."
      }
    ]
  },
  "remaining_to_process": [
    "Feedforward Neural Network",
    "Filter (or kernel) optimization",
    "Deep learning",
    "Computer vision",
    "Image processing",
    "Transformer",
    "Vanishing gradients",
    "Exploding gradients",
    "Backpropagation",
    "Shared weights",
    "Fully-connected layer",
    "Convolution kernels",
    "Cross-correlation kernels",
    "Feature maps",
    "Shift invariant",
    "Space invariant",
    "Artificial neural networks",
    "Convolution kernels or filters",
    "Translation- equivariant responses",
    "Overfitting",
    "Regularization",
    "Weight decay",
    "Skipped connections",
    "Dropout",
    "Receptive field",
    "Visual cortex",
    "Image classification algorithms",
    "Filters (or kernels)",
    "Automated learning",
    "Vision processing",
    "Brain",
    "Object Recognition",
    "Eye Tracking",
    "Neurons",
    "Visual cell types",
    "Cascading model",
    "Pattern recognition",
    "Multilayer visual feature detection network",
    "Analog threshold elements",
    "Interconnecting coefficients",
    "Homogeneous interconnections",
    "ReLU activation function",
    "Lateral inhibition",
    "Convolution",
    "Shift invariance",
    "Filter",
    "Speech recognition",
    "Correlation",
    "Time Delay Neural Network (TDNN)",
    "Phoneme recognition",
    "Shift-invariance",
    "1-D convolutional neural net",
    "Weight sharing",
    "Gradient descent",
    "Pyramidal structure",
    "Spectrograms",
    "Global optimization",
    "Local optimization",
    "2-D CNN",
    "1-D CNN",
    "Kernel Coefficients",
    "Back-propagation",
    "Shift-invariant pattern recognition neural network",
    "Medical Image Object Segmentation",
    "Breast Cancer Detection",
    "Mammograms",
    "Max pooling",
    "TDNNs (Time Delay Neural Networks)",
    "Cresceptron",
    "Spatial averaging",
    "Inhibition",
    "Saturation",
    "Downsampling",
    "LeNet-5",
    "Convolutional Network",
    "Handwritten Digit Recognition",
    "Check Reading Systems",
    "Gradient-based Learning",
    "Document Recognition",
    "Shift-invariant neural network",
    "Image character recognition",
    "Convolutional interconnections",
    "Image feature layers",
    "Generalization ability",
    "Medical image segmentation",
    "Electromyography",
    "De-convolution",
    "Graphics processing units (GPUs)",
    "GPGPU",
    "Deep belief networks",
    "Deep feedforward networks",
    "Image recognition",
    "AlexNet",
    "ImageNet Large Scale Visual Recognition Challenge",
    "Thread-level parallelism",
    "SIMD-level parallelism",
    "Intel Xeon Phi",
    "backpropagation",
    "Propagation",
    "cascading model",
    "Hierarchical Neural Network",
    "Multilayered Neural Network",
    "Convolutional Neural Networks",
    "Hand-designed Kernels",
    "Convolutions (in mammalian vision)",
    "Cognitron",
    "Unsupervised Learning",
    "Self-Organized Learning",
    "Simple Cell",
    "Complex Cell",
    "S-cells",
    "C-cells",
    "Local Feature Extraction",
    "Deformation Tolerance",
    "Selective Attention",
    "Backward Signals",
    "neocognitron",
    "Hubel & Wiesel"
  ]
}