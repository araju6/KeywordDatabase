{
  "keywords": {
    "recurrent neural network": [
      {
        "title": "A logical calculus of the ideas immanent in nervous activity",
        "url": "https://doi.org/10.1007/bf02478259",
        "abstract": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
        "citations": 17012,
        "reasoning": "\"The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles.\" \u2013 *This identifies a key early paper that considered cyclic networks, a foundational concept for RNNs.* - <[\"A logical calculus of the ideas immanent in nervous activity\"] :-: McCulloch, Warren S.; Pitts, Walter (1943)>."
      },
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83100,
        "reasoning": "\"Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.\" \u2013 *This directly identifies the inventors and approximate invention date of LSTMs, a major RNN architecture.* - <[\"Long Short Term Memory\"] :-: Sepp Hochreiter ; J\u00fcrgen Schmidhuber (1995)>."
      },
      {
        "title": "Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms",
        "url": "https://doi.org/10.2307/1419730",
        "abstract": "Abstract : Part I attempts to review the background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons. In Chapter 2, a brief review of the main alternative approaches to the development of brain models is presented. Chapter 3 considers the physiological and psychological criteria for a suitable model, and attempts to evaluate the empirical evidence which is available on several important issues. Chapter 4 contains basic definitions and some of the notation to be used in later sections are presented. Parts II and III are devoted to a summary of the established theoretical results obtained to date. Part II (Chapters 5 through 14) deals with the theory of three-layer series-coupled perceptrons, on which most work has been done to date. Part III (Chapters 15 through 20) deals with the theory of multi-layer and cross-coupled perceptrons. Part IV is concerned with more speculative models and problems for future analysis. Of necessity, the final chapters become increasingly heuristic in character, as the theory of perceptrons is not yet complete, and new possibilities are continually coming to light.",
        "citations": 1856,
        "reasoning": "\"Later, in Principles of Neurodynamics (1961), he described \"closed-loop cross-coupled\" and \"back-coupled\" perceptron networks, and made theoretical and experimental studies for Hebbian learning in these networks, [11] :\u200aChapter 19, 21 and noted that a fully cross-coupled perceptron network is equivalent to an infinitely deep feedforward network.\" \u2013 *This sentence discusses Rosenblatt's work on perceptrons with recurrent connections, which is relevant to the history of RNNs. The claim is expanded to include the reference.* - <[\"PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS\"] :-: Frank Rosenblatt (1961)>."
      }
    ],
    "Term Memory": [
      {
        "title": "A logical calculus of the ideas immanent in nervous activity",
        "url": "https://doi.org/10.1007/bf02478259",
        "abstract": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
        "citations": 17012,
        "reasoning": "\"The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles.\" \u2013 *This identifies a key early paper that considered cyclic networks, a foundational concept for RNNs.* - <[\"A logical calculus of the ideas immanent in nervous activity\"] :-: McCulloch, Warren S.; Pitts, Walter (1943)>."
      },
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83100,
        "reasoning": "\"Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.\" \u2013 *This directly identifies the inventors and approximate invention date of LSTMs, a major RNN architecture.* - <[\"Long Short Term Memory\"] :-: Sepp Hochreiter ; J\u00fcrgen Schmidhuber (1995)>."
      },
      {
        "title": "Sequence to Sequence Learning with Neural Networks",
        "url": "https://doi.org/10.48550/arxiv.1409.3215",
        "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
        "citations": 13804,
        "reasoning": "\"The papers most commonly cited as the originators that produced seq2seq are two papers from 2014.\" \u2013 *This claim indicates that two papers in 2014 are foundational to the development of seq2seq models.* - [\"Sequence to Sequence Learning with Neural Networks\"] :-: <Ilya Sutskever; Oriol Vinyals; Quoc V. Le> (2014)"
      }
    ],
    "convolutional neural network": [
      {
        "title": "Backpropagation Applied to Handwritten Zip Code Recognition",
        "url": "https://doi.org/10.1162/neco.1989.1.4.541",
        "abstract": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.",
        "citations": 10884,
        "reasoning": "\"Backpropagation Applied to Handwritten Zip Code Recognition\" \u2013 *This paper is considered one of the foundational papers in the development of CNNs, specifically focusing on applying backpropagation to train a CNN for handwritten digit recognition.* - [\"Backpropagation Applied to Handwritten Zip Code Recognition\"] :-: LeCun et al. (1989)"
      },
      {
        "title": "Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements",
        "url": "https://doi.org/10.1109/tssc.1969.300225",
        "abstract": "A new type of visual feature extracting network has been synthesized, and the response of the network has been simulated on a digital computer. This research has been done as a first step towards the realization of a recognizer of handwritten characters. The design of the network was suggested by biological systems, especially, the visual systems of cat and monkey. The network is composed of analog threshold elements connected in layers. Each analog threshold element receives inputs from a large number of elements in the neighbouring layers and performs its own special functions. It takes care of one restricted part of the photoreceptor layer, on which an input pattem is presented, and it responds to one particular feature of the input pattem, such as brightness contrast, a dot in the pattern, a line segment of a particular orientation, or an end of the line. This means that the network performs parallel processing of the information. With the propagation of the information through the layered network, the input pattern is successively decomposed into dots, groups of line segments of the same orientation, and the ends of these line segments.",
        "citations": 228,
        "reasoning": "\"In 1969, Kunihiko Fukushima introduced a multilayer visual feature detection network, inspired by the above-mentioned work of Hubel and Wiesel, in which \"All the elements in one layer have the same set of interconnecting coefficients; the arrangement of the elements and their interconnections are all homogeneous over a given layer.\"\" \u2013 *Describes the foundational concept of shared weights in CNNs introduced by Fukushima.* - [\"Visual feature extraction by a multilayered network of analog threshold elements\"] :-: <Kunihiko Fukushima> (1969)"
      },
      {
        "title": "Phoneme recognition using time-delay neural networks",
        "url": "https://doi.org/10.1109/29.21701",
        "abstract": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
        "citations": 2577,
        "reasoning": "\"The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was an early convolutional network exhibiting shift-invariance.\" \u2013 *Identifies the TDNN and its inventors.* - [\"Phoneme Recognition Using Time-Delay Neural Networks\"] :-: <Alex Waibel et al.> (1987)"
      },
      {
        "title": "ImageNet Classification with Deep Convolutional Neural Networks",
        "url": null,
        "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called dropout that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",
        "citations": 68574,
        "reasoning": "\"Subsequently, AlexNet , a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012.\" \u2013 *Identifies AlexNet and its impact.* - [\"ImageNet Classification with Deep Convolutional Neural Networks\"] :-: <Alex Krizhevsky et al.> (2012)"
      }
    ],
    "coupled perceptrons": [
      {
        "title": "A logical calculus of the ideas immanent in nervous activity",
        "url": "https://doi.org/10.1007/bf02478259",
        "abstract": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
        "citations": 17012,
        "reasoning": "\"The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles.\" \u2013 *This identifies a foundational paper that introduced cycles within neural networks, a key concept for recurrence.* - [\"A logical calculus of the ideas immanent in nervous activity\"] :-: <Warren S. McCulloch; Pitts, Walter> (1943)"
      }
    ],
    "Sherrington\u2013Kirkpatrick model": [
      {
        "title": "Stability of the Sherrington-Kirkpatrick solution of a spin glass model",
        "url": "https://doi.org/10.1088/0305-4470/11/5/028",
        "abstract": "The stationary point used by Sherrington and Kirkpatrick (1975) in their evaluation of the free energy of a spin glass by the method of steepest descent is examined carefully. It is found that, although this point is a maximum of the integrand at high temperatures, it is not a maximum in the spin glass phase nor in the ferromagnetic phase at low temperatures. The instability persists in the presence of a magnetic field. Results are given for the limit of stability both for a partly ferromagnetic interaction in the absence of an external field and for a purely random interaction in the presence of a field.",
        "citations": 1982,
        "reasoning": "\"Sherrington and Kirkpatrick proposed the SK model in 1975, and solved it by the replica method.\" \u2013 *This sentence directly mentions the proposal of the SK model and the method used to solve it* - [\"Solution of the Sherrington-Kirkpatrick model\"] :-: Sherrington, D.; Kirkpatrick, S. (1975)."
      },
      {
        "title": "The Sherrington-Kirkpatrick Model",
        "url": "https://doi.org/10.1007/978-1-4614-6289-7",
        "abstract": "No abstract found",
        "citations": 265,
        "reasoning": "\"Sherrington and Kirkpatrick proposed the SK model in 1975, and solved it by the replica method.\" \u2013 *This sentence directly states the original proposal of the SK model and the method used to solve it.* - [\"Sherrington-Kirkpatrick model\"] :-: Sherrington and Kirkpatrick (1975)"
      }
    ]
  },
  "remaining_to_process": [
    "Recurrent neural networks",
    "Sequential data",
    "Recurrent connections",
    "Recurrent unit",
    "Hidden state",
    "Temporal dependencies",
    "Vanishing gradient problem",
    "Long short-term memory (LSTM)",
    "Gated recurrent units (GRUs)",
    "Transformers",
    "Self-attention mechanisms",
    "Recurrent semicircles",
    "Recurrent reciprocal connections",
    "Reverberating circuit",
    "McCulloch-Pitts neuron model",
    "Recurrent inhibition",
    "Close-loop cross-coupled perceptrons",
    "Glauber dynamics",
    "Sherrington-Kirkpatrick model",
    "Hopfield network",
    "Neural history compressor",
    "Very Deep Learning",
    "BRNN",
    "Encoder-decoder",
    "Sequence transduction",
    "Attention mechanisms",
    "Unsupervised Learning",
    "Recurrent Neural Networks (RNNs)",
    "Prediction",
    "Hierarchy",
    "Compressed Representation",
    "Description Length",
    "Generative Model",
    "Vanishing Gradient Problem",
    "Backpropagation",
    "Deep Learning",
    "Chunking",
    "Automatization",
    "Long-term memory (LTM)",
    "Atkinson\u2013Shiffrin memory model",
    "Sensory memory",
    "Short-term memory",
    "Working memory",
    "Explicit memory (declarative memory)",
    "Implicit memory (non-declarative memory)",
    "Episodic memory",
    "Semantic memory",
    "Procedural memory",
    "Emotional conditioning",
    "Recurrent neural networks (RNNs)",
    "Recurrent, reciprocal connections",
    "Closed-loop cross-coupled perceptrons",
    "Back-coupled perceptron networks",
    "Bidirectional recurrent neural networks (BRNN)",
    "Encoder-decoder sequence transduction",
    "Seq2seq",
    "Automatic differentiation",
    "Generative model",
    "Unfolded RNN",
    "Long-term memory",
    "Explicit memory",
    "Declarative memory",
    "Implicit memory",
    "Non-declarative memory",
    "Feedforward Neural Network",
    "Filter Optimization",
    "Shared Weights",
    "Convolution Kernel",
    "Cross-correlation",
    "Feature Maps",
    "Translation Equivariance",
    "Receptive Field",
    "Visual Cortex",
    "Neocognitron",
    "ReLU (Rectified Linear Unit)",
    "Time Delay Neural Network (TDNN)",
    "Shift Invariance",
    "Max Pooling",
    "LeNet-5",
    "GPU Implementation",
    "AlexNet",
    "Feedforward neural networks",
    "Unsupervised stack of RNNs",
    "Description length",
    "Conscious chunker",
    "Subconscious automatizer",
    "Long short-term memory",
    "Gated recurrent units",
    "Recurrent semicircle",
    "Cerebellar cortex",
    "Parallel fiber",
    "Purkinje cells",
    "Granule cells",
    "Vestibulo-ocular reflex",
    "Motor control",
    "Macy conferences",
    "Hebbian learning",
    "Spin glass",
    "Binary activation function",
    "Continuous activation functions",
    "Hebbian learning rule",
    "Associatron",
    "Ising Model",
    "Binary activation functions",
    "Description length minimization",
    "Perceptron",
    "Supervised learning",
    "Binary classifier",
    "Linear classifier",
    "Feature vector",
    "Weights",
    "Artificial neuron network",
    "Mark I Perceptron",
    "Alpha-perceptron",
    "S-units",
    "A-units",
    "R-units",
    "XOR function",
    "Kernel perceptron algorithm",
    "Margin bounds guarantees",
    "Image recognition",
    "Plugboard",
    "Potentiometers",
    "Neurodynamics",
    "Multilayer perceptron",
    "Single-layer perceptron",
    "Linearly separable patterns",
    "Margin bounds",
    "Decision boundaries",
    "Ferromagnetism",
    "Statistical mechanics",
    "Magnetic dipole moments",
    "Spins",
    "Lattice",
    "Phase transition",
    "Mean-field theory",
    "Cayley trees",
    "Branching ratio",
    "Neural networks",
    "Graph maximum cut (Max-Cut)",
    "Combinatorial optimization",
    "Translation-invariant",
    "Ferromagnetic",
    "Zero-field model",
    "d-dimensional lattice",
    "Correlations",
    "Peierls argument",
    "Correlation functions",
    "Free energy",
    "Spontaneous magnetization",
    "Fredholm determinants",
    "Correlation inequalities",
    "Spin correlations",
    "Magnetization",
    "Coupling constants",
    "Simon-Lieb inequality",
    "Critical temperatures",
    "Planar Potts model",
    "Percolation arguments",
    "Correlation",
    "Ordered phase",
    "Disordered phase",
    "Ferromagnetic order",
    "Ising spin correlations",
    "Percolation model",
    "Potts model",
    "Freezing temperature",
    "Ferromagnet",
    "Antiferromagnet",
    "Frustrated interactions",
    "Metastable",
    "Kondo effect",
    "Spin glass freezing temperature",
    "Replica method",
    "Edwards-Anderson order parameter",
    "Replica breaking",
    "Order parameters",
    "Mictomagnet",
    "Ferromagnetic interactions",
    "Antiferromagnetic interactions",
    "Magnetic frustration",
    "SK model",
    "Entropy",
    "Broken symmetry",
    "Replica breaking ansatz",
    "Ising model",
    "Recurrent Semicircles",
    "Spin Glass",
    "Glauber Dynamics",
    "Iterated nets",
    "Attention Mechanisms",
    "Sherrington\u2013Kirkpatrick model",
    "seq2seq",
    "attention mechanisms",
    "transformers",
    "encoder-decoder sequence transduction",
    "Automatizer",
    "Atkinson-Shiffrin memory model",
    "Feedforward structure",
    "Epilepsy",
    "Causalgia",
    "Seq2seq architecture",
    "RNN unfolded in time",
    "Encoder-decoder architecture",
    "Cognitive psychology",
    "Chunkers",
    "Sequential Data",
    "Recurrent Connections",
    "Recurrent Unit",
    "Hidden State",
    "Long Short-Term Memory (LSTM)",
    "Gated Recurrent Units (GRUs)",
    "Self-Attention Mechanisms",
    "Recurrent neural network",
    "Recurrent Neural Network",
    "Sequence to Sequence",
    "Feedforward Neural Networks",
    "Temporal Dependencies",
    "Elman network",
    "Cerebellar Cortex",
    "Purkinje Cells",
    "Granule Cells",
    "seq2seq architecture",
    "Neural History Compressor",
    "Unsupervised Stack of RNNs",
    "Automatic Differentiation",
    "Conscious Chunker",
    "Subconscious Automatizer",
    "Self-attention",
    "Jordan network",
    "Recurrent Neural Networks",
    "Long Short-Term Memory",
    "Gated Recurrent Units",
    "Binary classifiers",
    "Artificial Neuron Network",
    "Alpha-Perceptron",
    "Multilayer Perceptron",
    "Single-layer Perceptron",
    "Kernel Perceptron Algorithm",
    "Image Recognition",
    "Weight Updates",
    "Linearly Separable Patterns",
    "XOR Function",
    "Kernel Perceptron",
    "Margin Bounds",
    "Non-separable case",
    "Recurrent Neural Network (RNN)",
    "Hidden Markov Models",
    "Sequence Learning",
    "Cell",
    "Input Gate",
    "Output Gate",
    "Forget Gate",
    "Time Series Analysis",
    "Speech Recognition",
    "Machine Translation",
    "Constant Error Carousel (CEC)",
    "Input gate",
    "Output gate",
    "Forget gate",
    "Peephole connections",
    "Connectionist Temporal Classification (CTC)",
    "Bidirectional LSTM",
    "Gated recurrent unit (GRU)",
    "Highway network",
    "ResNet architecture",
    "mLSTM",
    "sLSTM",
    "Focused back-propagation",
    "Forget gate (Keep gate)"
  ]
}