{
  "keywords": {
    "recurrent neural network": [
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83100,
        "reasoning": "\"Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.\" \u2013 *This sentence describes the origin of LSTM networks and their impact.* - [\"Long Short-Term Memory\"] :-: Sepp Hochreiter and J\u00fcrgen Schmidhuber (1995)",
        "parent_claim": null
      },
      {
        "title": "A logical calculus of the ideas immanent in nervous activity",
        "url": "https://doi.org/10.1007/bf02478259",
        "abstract": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
        "citations": 17012,
        "reasoning": "\"The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles.\" \u2013 *This sentence explicitly mentions a paper that considered networks with cycles, which is relevant to the development of recurrent neural networks.* - <McCulloch, Warren S.; Pitts, Walter> - [\"A logical calculus of the ideas immanent in nervous activity\"] :-: (1943)",
        "parent_claim": null
      },
      {
        "title": "Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms",
        "url": "https://doi.org/10.2307/1419730",
        "abstract": "Abstract : Part I attempts to review the background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons. In Chapter 2, a brief review of the main alternative approaches to the development of brain models is presented. Chapter 3 considers the physiological and psychological criteria for a suitable model, and attempts to evaluate the empirical evidence which is available on several important issues. Chapter 4 contains basic definitions and some of the notation to be used in later sections are presented. Parts II and III are devoted to a summary of the established theoretical results obtained to date. Part II (Chapters 5 through 14) deals with the theory of three-layer series-coupled perceptrons, on which most work has been done to date. Part III (Chapters 15 through 20) deals with the theory of multi-layer and cross-coupled perceptrons. Part IV is concerned with more speculative models and problems for future analysis. Of necessity, the final chapters become increasingly heuristic in character, as the theory of perceptrons is not yet complete, and new possibilities are continually coming to light.",
        "citations": 1856,
        "reasoning": "\"Later, in Principles of Neurodynamics (1961), he described \"closed-loop cross-coupled\" and \"back-coupled\" perceptron networks, and made theoretical and experimental studies for Hebbian learning in these networks, [ 17 ] :\u200aChapter 19, 21 and noted that a fully cross-coupled perceptron network is equivalent to an infinitely deep feedforward network.\" \u2013 *This sentence identifies Rosenblatt's 1961 book \"Principles of Neurodynamics\" which is a key foundational work, focusing on perceptrons with recurrent and back-coupled connections, and studies on Hebbian learning.* - [\"PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS\"] :-: Frank Rosenblatt (1961)",
        "parent_claim": null
      },
      {
        "title": "Learning Patterns and Pattern Sequences by Self-Organizing Nets of Threshold Elements",
        "url": "https://doi.org/10.1109/t-c.1972.223477",
        "abstract": "Various information-processing capabilities of self-organizing nets of threshold elements are studied. A self-organizing net, learning from patterns or pattern sequences given from outside as stimuli, \"remembers\" some of them as stable equilibrium states or state-transition sequences of the net. A condition where many patterns and pattern sequences are remembered in a net at the same time is shown. The stability degree of their remembrance and recalling under noise disturbances is investigated theoretically. For this purpose, the stability of state transition in an autonomous logical net of threshold elements is studied by the use of characteristics of threshold elements.",
        "citations": 442,
        "reasoning": "\"Similar networks were published by Shun'ichi Amari in 1972, [ 21 ]\" \u2013 *This sentence identifies publications by Shun'ichi Amari in 1972 which are a key foundational work.* - [\"Learning patterns and pattern sequences by self-organizing nets of threshold elements\"] :-: Shun-Ichi Amari (1972)",
        "parent_claim": null
      },
      {
        "title": "Neural networks and physical systems with emergent collective computational abilities.",
        "url": "https://doi.org/10.1073/pnas.79.8.2554",
        "abstract": "Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.",
        "citations": 18605,
        "reasoning": "\"In the 1982 paper, Hopfield applied this recently developed theory to study the Hopfield network with binary activation functions.\" \u2013 *This sentence directly refers to Hopfield's 1982 paper, where he applied the Sherrington-Kirkpatrick model to the study of Hopfield networks, a crucial step in their development.* - [\"Neural networks and physical systems with emergent collective computational abilities\"] :-: J. J. Hopfield (1982)",
        "parent_claim": null
      },
      {
        "title": "Solvable Model of a Spin-Glass",
        "url": "https://doi.org/10.1103/physrevlett.35.1792",
        "abstract": "We consider an Ising model in which the spins are coupled by infinite-ranged random interactions independently distributed with a Gaussian probability density. Both \"spinglass\" and ferromagnetic phases occur. The competition between the phases and the type of order present in each are studied.",
        "citations": 4125,
        "reasoning": "\"The Sherrington\u2013Kirkpatrick model of spin glass, published in 1975, [ 27 ] is the Hopfield network with random initialization.\" \u2013 *This statement directly links the SK model paper to the Hopfield network concept, and is foundational work on recurrent networks* - [\"Solvable Model of a Spin-Glass\"] :-: David Sherrington, Scott Kirkpatrick (1975)",
        "parent_claim": null
      },
      {
        "title": "Sequence to Sequence Learning with Neural Networks",
        "url": "https://doi.org/10.48550/arxiv.1409.3215",
        "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
        "citations": 13804,
        "reasoning": "\"The papers most commonly cited as the originators that produced seq2seq are two papers from 2014. [46] [47]\" \u2013 *This sentence refers directly to the foundational papers for seq2seq models.* - [\"Sequence to Sequence Learning with Neural Networks\"] :-: Ilya Sutskever et al. (2014)",
        "parent_claim": null
      }
    ],
    "LSTM": [
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83100,
        "reasoning": "\"Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.\" \u2013 *This sentence describes the origin of LSTM networks and their impact.* - [\"Long Short-Term Memory\"] :-: Sepp Hochreiter and J\u00fcrgen Schmidhuber (1995)",
        "parent_claim": null
      },
      {
        "title": "Sequence to Sequence Learning with Neural Networks",
        "url": "https://doi.org/10.48550/arxiv.1409.3215",
        "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
        "citations": 13804,
        "reasoning": "\"The papers most commonly cited as the originators that produced seq2seq are two papers from 2014. [46] [47]\" \u2013 *This statement directly identifies papers that are considered foundational for the seq2seq architecture.* - [\"Sequence to Sequence Learning with Neural Networks\"] :-: Ilya Sutskever, Oriol Vinyals, and Quoc V. Le (2014)",
        "parent_claim": null
      },
      {
        "title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
        "url": "https://doi.org/10.1016/j.neunet.2005.06.042",
        "abstract": "No abstract found",
        "citations": 4683,
        "reasoning": "\"Graves, Schmidhuber, 2005) [ 26 ] published LSTM with full backpropagation through time and bidirectional LSTM.\" \u2013 *This sentence directly links Graves and Schmidhuber to the publication of LSTM with backpropagation and bidirectionality* - [\"Framewise phoneme classification with bidirectional LSTM and other neural network architectures\"] :-: Graves, A.; Schmidhuber, J. (2005)",
        "parent_claim": null
      },
      {
        "title": "LSTM recurrent networks learn simple context-free and context-sensitive languages",
        "url": "https://doi.org/10.1109/72.963769",
        "abstract": "Previous work on learning regular languages from exemplary training sequences showed that long short-term memory (LSTM) outperforms traditional recurrent neural networks (RNNs). We demonstrate LSTMs superior performance on context-free language benchmarks for RNNs, and show that it works even better than previous hardwired or highly specialized architectures. To the best of our knowledge, LSTM variants are also the first RNNs to learn a simple context-sensitive language, namely a/sup n/b/sup n/c/sup n/.",
        "citations": 718,
        "reasoning": "\"2001: Gers and Schmidhuber trained LSTM to learn languages unlearnable by traditional models such as Hidden Markov Models. [ 21 ]\" \u2013 *This sentence directly mentions the use of LSTM by Gers and Schmidhuber, thus it is considered a foundational work related to LSTMs* - [\"LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages\"] :-: Gers, F. A.; Schmidhuber, J. (2001).",
        "parent_claim": null
      }
    ]
  },
  "remaining_to_process": [
    "Recurrent Neural Networks (RNNs)",
    "Sequential Data",
    "Recurrent Connections",
    "Recurrent Unit",
    "Hidden State",
    "Vanishing Gradient Problem",
    "Long Short-Term Memory (LSTM)",
    "Gated Recurrent Units (GRUs)",
    "Transformers",
    "Self-Attention Mechanisms",
    "Recurrent neural network",
    "Cerebellar cortex",
    "Purkinje cells",
    "Granule cells",
    "Vestibulo-ocular reflex",
    "Feedforward structure",
    "Reverberating circuit",
    "McCulloch-Pitts neuron model",
    "Recurrent inhibition",
    "Neural feedback loops",
    "close-loop cross-coupled perceptrons",
    "back-coupled perceptron networks",
    "Hebbian learning",
    "Ising model",
    "Glauber dynamics",
    "Sherrington\u2013Kirkpatrick model",
    "Hopfield network",
    "Jordan network",
    "Elman network",
    "Neural history compressor",
    "Bidirectional recurrent neural networks (BRNN)",
    "Encoder-decoder sequence transduction",
    "Attention mechanisms",
    "Vanishing gradient problem",
    "Automatic differentiation",
    "Backpropagation",
    "Description length",
    "Chunking",
    "Automatization",
    "Unsupervised learning",
    "Recurrent Neural Network (RNN)",
    "Hidden Markov Models",
    "Sequence learning",
    "Cell",
    "Input gate",
    "Output gate",
    "Forget gate",
    "Classification",
    "Data processing",
    "Time series analysis",
    "Speech recognition",
    "Machine translation",
    "Speech activity detection",
    "Robot control",
    "Video games",
    "Healthcare",
    "Constant Error Carousel (CEC) units",
    "Peephole connections",
    "Connectionist Temporal Classification (CTC)",
    "Backpropagation Through Time",
    "Bidirectional LSTM",
    "Forget Gate LSTM",
    "Gated Recurrent Unit (GRU)",
    "Highway Network",
    "ResNet",
    "xLSTM",
    "mLSTM",
    "sLSTM",
    "Meta-learning",
    "Neuroevolution",
    "Policy Gradients",
    "Reinforcement learning",
    "Neural Architecture Search",
    "Phoneme Error Rate",
    "Time-Aware LSTM",
    "Recurrent neural networks (RNNs)",
    "Sequential data",
    "Feedforward neural networks",
    "Recurrent connections",
    "Recurrent unit",
    "Hidden state",
    "Long short-term memory (LSTM)",
    "Gated recurrent units (GRUs)",
    "Self-attention mechanisms",
    "Close-loop cross-coupled perceptrons",
    "Hebbian learning rule",
    "Spin glass",
    "Neural History Compressor",
    "Unsupervised Stack of RNNs",
    "Description Length",
    "Conscious Chunker",
    "Subconscious Automatizer",
    "Recurrent neural networks",
    "Temporal dependencies",
    "Long short-term memory",
    "Gated recurrent units",
    "Back-coupled perceptron networks",
    "Binary activation functions",
    "Continuous activation functions",
    "Sequence transduction",
    "Unsupervised stack of RNNs",
    "Conscious chunker",
    "Subconscious automatizer",
    "Recurrent semicircles",
    "Recurrent reciprocal connections",
    "Automatic Differentiation",
    "Very Deep Learning",
    "Chunker",
    "Automatizer",
    "Generative model",
    "Cycles (in neural networks)",
    "Associative Memory",
    "Description Length Minimization",
    "Sherrington-Kirkpatrick model",
    "Seq2seq architecture",
    "Recurrent neural network models",
    "Convolutional neural networks (CNNs)",
    "Automatic image captioning",
    "History compression",
    "Description length minimization"
  ],
  "claim_chains": {
    "LSTM": [
      {
        "claim": "long short-term memory (LSTM) architecture",
        "parent_keyword": "recurrent neural network"
      },
      {
        "claim": "long short-term memory (LSTM) architecture",
        "parent_keyword": "recurrent neural network"
      }
    ]
  }
}