{
  "keywords": {
    "recurrent neural network": [
      {
        "title": "A logical calculus of the ideas immanent in nervous activity",
        "url": "https://doi.org/10.1007/bf02478259",
        "abstract": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
        "citations": 17017,
        "reasoning": "\"The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles.\" \u2013 *This sentence directly mentions a foundational paper (McCulloch and Pitts paper (1943)) that introduced networks with cycles, a key characteristic of recurrent neural networks.* - [\"A logical calculus of the ideas immanent in nervous activity\"] :-: McCulloch, Warren S.; Pitts, Walter (1943)",
        "child_claim": "\"The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles.\" \u2013 *This sentence directly mentions a foundational paper (McCulloch and Pitts paper (1943)) that introduced networks with cycles, a key characteristic of recurrent neural networks.* - [\"A logical calculus of the ideas immanent in nervous activity\"] :-: McCulloch, Warren S.; Pitts, Walter (1943)"
      },
      {
        "title": "Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms",
        "url": "https://doi.org/10.2307/1419730",
        "abstract": "Abstract : Part I attempts to review the background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons. In Chapter 2, a brief review of the main alternative approaches to the development of brain models is presented. Chapter 3 considers the physiological and psychological criteria for a suitable model, and attempts to evaluate the empirical evidence which is available on several important issues. Chapter 4 contains basic definitions and some of the notation to be used in later sections are presented. Parts II and III are devoted to a summary of the established theoretical results obtained to date. Part II (Chapters 5 through 14) deals with the theory of three-layer series-coupled perceptrons, on which most work has been done to date. Part III (Chapters 15 through 20) deals with the theory of multi-layer and cross-coupled perceptrons. Part IV is concerned with more speculative models and problems for future analysis. Of necessity, the final chapters become increasingly heuristic in character, as the theory of perceptrons is not yet complete, and new possibilities are continually coming to light.",
        "citations": 1856,
        "reasoning": "\"Later, in Principles of Neurodynamics (1961), he described \"closed-loop cross-coupled\" and \"back-coupled\" perceptron networks, and made theoretical and experimental studies for Hebbian learning in these networks, [ 17 ] :\u200aChapter 19, 21 and noted that a fully cross-coupled perceptron network is equivalent to an infinitely deep feedforward network.\" \u2013 *This claim identifies a book by Frank Rosenblatt that describes closed-loop cross-coupled and back-coupled perceptron networks and also makes theoretical and experimental studies for Hebbian learning in these networks.* - [\"PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS\"] :-: Frank Rosenblatt (1961)",
        "child_claim": "\"Later, in Principles of Neurodynamics (1961), he described \"closed-loop cross-coupled\" and \"back-coupled\" perceptron networks, and made theoretical and experimental studies for Hebbian learning in these networks, [ 17 ] :\u200aChapter 19, 21 and noted that a fully cross-coupled perceptron network is equivalent to an infinitely deep feedforward network.\" \u2013 *This claim identifies a book by Frank Rosenblatt that describes closed-loop cross-coupled and back-coupled perceptron networks and also makes theoretical and experimental studies for Hebbian learning in these networks.* - [\"PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS\"] :-: Frank Rosenblatt (1961)"
      },
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83296,
        "reasoning": "\"Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.\" \u2013 *This sentence directly claims the invention of LSTMs and attributes it to Hochreiter and Schmidhuber.* - [\"Long Short-Term Memory\"] :-: Hochreiter & Schmidhuber (1995)",
        "child_claim": "\"Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.\" \u2013 *This sentence directly claims the invention of LSTMs and attributes it to Hochreiter and Schmidhuber.* - [\"Long Short-Term Memory\"] :-: Hochreiter & Schmidhuber (1995)"
      },
      {
        "title": "Bidirectional recurrent neural networks",
        "url": "https://doi.org/10.1109/78.650093",
        "abstract": "In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported.",
        "citations": 8861,
        "reasoning": "\"Bidirectional recurrent neural networks (BRNN) uses two RNN that processes the same input in opposite directions. [ 37 ]\" \u2013 *This sentence makes a foundational claim about BRNN architecture and cites a relevant paper.* - [\"Bidirectional recurrent neural networks\"] :-: Schuster & Paliwal (1997)",
        "child_claim": "\"Bidirectional recurrent neural networks (BRNN) uses two RNN that processes the same input in opposite directions. [ 37 ]\" \u2013 *This sentence makes a foundational claim about BRNN architecture and cites a relevant paper.* - [\"Bidirectional recurrent neural networks\"] :-: Schuster & Paliwal (1997)"
      },
      {
        "title": "Sequence to Sequence Learning with Neural Networks",
        "url": "https://doi.org/10.48550/arxiv.1409.3215",
        "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
        "citations": 13811,
        "reasoning": "\"The papers most commonly cited as the originators that produced seq2seq are two papers from 2014. [ 46 ] [ 47 ]\" \u2013 *This sentence claims the foundational seq2seq papers.* - [\"Sequence to Sequence Learning with Neural Networks\"] :-: Sutskever et al. (2014)",
        "child_claim": "\"The papers most commonly cited as the originators that produced seq2seq are two papers from 2014. [ 46 ] [ 47 ]\" \u2013 *This sentence claims the foundational seq2seq papers.* - [\"Sequence to Sequence Learning with Neural Networks\"] :-: Sutskever et al. (2014)"
      },
      {
        "title": "Learning to Forget: Continual Prediction with LSTM",
        "url": "https://doi.org/10.1162/089976600300015015",
        "abstract": "Long short-term memory (LSTM; Hochreiter &amp; Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive \u201cforget gate\u201d that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way.",
        "citations": 4700,
        "reasoning": "\"( Felix Gers , J\u00fcrgen Schmidhuber, and Fred Cummins, 1999) [67 ] introduced the forget gate (also called \"keep gate\") into the LSTM architecture in 1999, enabling the LSTM to reset its own state.\" \u2013 *This sentence highlights the authors and year of publication for the introduction of the forget gate.* - [\"Learning to forget: Continual prediction with LSTM\"] :-: Felix Gers, J\u00fcrgen Schmidhuber, and Fred Cummins (1999)",
        "child_claim": "\"This issue was addressed by the development of the long short-term memory (LSTM) architecture in 1997, making it the standard RNN variant for handling long-term dependencies.\" - *This sentence explicitly states the development of LSTM in 1997, which is considered a foundational paper in the field.* - [\"XXX\"] :-: XXX (1997)"
      },
      {
        "title": "LSTM recurrent networks learn simple context-free and context-sensitive languages",
        "url": "https://doi.org/10.1109/72.963769",
        "abstract": "Previous work on learning regular languages from exemplary training sequences showed that long short-term memory (LSTM) outperforms traditional recurrent neural networks (RNNs). We demonstrate LSTMs superior performance on context-free language benchmarks for RNNs, and show that it works even better than previous hardwired or highly specialized architectures. To the best of our knowledge, LSTM variants are also the first RNNs to learn a simple context-sensitive language, namely a/sup n/b/sup n/c/sup n/.",
        "citations": 718,
        "reasoning": "\"2001: Gers and Schmidhuber trained LSTM to learn languages unlearnable by traditional models such as Hidden Markov Models. [ 21 ]\" \u2013 *This claim directly references a paper where Gers and Schmidhuber used LSTM to learn languages that Hidden Markov Models cannot.* - [\"LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages\"] :-: Gers, F. A.; Schmidhuber, J. (2001).",
        "child_claim": "\"This issue was addressed by the development of the long short-term memory (LSTM) architecture in 1997, making it the standard RNN variant for handling long-term dependencies.\" - *This sentence explicitly states the development of LSTM in 1997, which is considered a foundational paper in the field.* - [\"XXX\"] :-: XXX (1997)"
      }
    ],
    "Term Memory": [
      {
        "title": "A logical calculus of the ideas immanent in nervous activity",
        "url": "https://doi.org/10.1007/bf02478259",
        "abstract": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
        "citations": 17017,
        "reasoning": "\"The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles.\" \u2013 *This sentence directly mentions a foundational paper (McCulloch and Pitts paper (1943)) that introduced networks with cycles, a key characteristic of recurrent neural networks.* - [\"A logical calculus of the ideas immanent in nervous activity\"] :-: McCulloch, Warren S.; Pitts, Walter (1943)",
        "child_claim": "\"The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles.\" \u2013 *This sentence directly mentions a foundational paper (McCulloch and Pitts paper (1943)) that introduced networks with cycles, a key characteristic of recurrent neural networks.* - [\"A logical calculus of the ideas immanent in nervous activity\"] :-: McCulloch, Warren S.; Pitts, Walter (1943)"
      },
      {
        "title": "Neural networks and physical systems with emergent collective computational abilities.",
        "url": "https://doi.org/10.1073/pnas.79.8.2554",
        "abstract": "Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.",
        "citations": 18608,
        "reasoning": "\"In the 1982 paper, Hopfield applied this recently developed theory to study the Hopfield network with binary activation functions.\" \u2013 *This claim highlights a paper by Hopfield where he studied the network with binary activation functions.* - [\"Neural networks and physical systems with emergent collective computational abilities\"] :-: J. J. Hopfield (1982)",
        "child_claim": "\"In the 1982 paper, Hopfield applied this recently developed theory to study the Hopfield network with binary activation functions.\" \u2013 *This claim highlights a paper by Hopfield where he studied the network with binary activation functions.* - [\"Neural networks and physical systems with emergent collective computational abilities\"] :-: J. J. Hopfield (1982)"
      },
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83296,
        "reasoning": "\"Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.\" \u2013 *This sentence directly claims the invention of LSTMs and attributes it to Hochreiter and Schmidhuber.* - [\"Long Short-Term Memory\"] :-: Hochreiter & Schmidhuber (1995)",
        "child_claim": "\"Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.\" \u2013 *This sentence directly claims the invention of LSTMs and attributes it to Hochreiter and Schmidhuber.* - [\"Long Short-Term Memory\"] :-: Hochreiter & Schmidhuber (1995)"
      }
    ],
    "LSTM": [
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83296,
        "reasoning": "\"Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains. [ 35 ] [ 36 ]\" \u2013 *This sentence directly attributes the invention of LSTM networks to Hochreiter and Schmidhuber and cites references.* - [\"Long Short Term Memory\"] :-: Sepp Hochreiter; J\u00fcrgen Schmidhuber (1995)",
        "child_claim": "\"Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains. [ 35 ] [ 36 ]\" \u2013 *This sentence directly attributes the invention of LSTM networks to Hochreiter and Schmidhuber and cites references.* - [\"Long Short Term Memory\"] :-: Sepp Hochreiter; J\u00fcrgen Schmidhuber (1995)"
      },
      {
        "title": "Learning to Forget: Continual Prediction with LSTM",
        "url": "https://doi.org/10.1162/089976600300015015",
        "abstract": "Long short-term memory (LSTM; Hochreiter &amp; Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive \u201cforget gate\u201d that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way.",
        "citations": 4700,
        "reasoning": "\"( Felix Gers , J\u00fcrgen Schmidhuber, and Fred Cummins, 1999) [67 ] introduced the forget gate (also called \"keep gate\") into the LSTM architecture in 1999, enabling the LSTM to reset its own state.\" \u2013 *This sentence highlights the authors and year of publication for the introduction of the forget gate.* - [\"Learning to forget: Continual prediction with LSTM\"] :-: Felix Gers, J\u00fcrgen Schmidhuber, and Fred Cummins (1999)",
        "child_claim": "\"( Felix Gers , J\u00fcrgen Schmidhuber, and Fred Cummins, 1999) [67 ] introduced the forget gate (also called \"keep gate\") into the LSTM architecture in 1999, enabling the LSTM to reset its own state.\" \u2013 *This sentence highlights the authors and year of publication for the introduction of the forget gate.* - [\"Learning to forget: Continual prediction with LSTM\"] :-: Felix Gers, J\u00fcrgen Schmidhuber, and Fred Cummins (1999)"
      },
      {
        "title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
        "url": "https://doi.org/10.1016/j.neunet.2005.06.042",
        "abstract": "No abstract found",
        "citations": 4684,
        "reasoning": "\"Graves, Schmidhuber, 2005) [26] published LSTM with full backpropagation through time and bidirectional LSTM.\" \u2013 *This identifies the paper that introduced full backpropagation through time and bidirectional LSTM.* - [\"Framewise phoneme classification with bidirectional LSTM and other neural network architectures\"] :-: Graves, A.; Schmidhuber, J. (2005)",
        "child_claim": "\"Graves, Schmidhuber, 2005) [26] published LSTM with full backpropagation through time and bidirectional LSTM.\" \u2013 *This identifies the paper that introduced full backpropagation through time and bidirectional LSTM.* - [\"Framewise phoneme classification with bidirectional LSTM and other neural network architectures\"] :-: Graves, A.; Schmidhuber, J. (2005)"
      },
      {
        "title": "LSTM recurrent networks learn simple context-free and context-sensitive languages",
        "url": "https://doi.org/10.1109/72.963769",
        "abstract": "Previous work on learning regular languages from exemplary training sequences showed that long short-term memory (LSTM) outperforms traditional recurrent neural networks (RNNs). We demonstrate LSTMs superior performance on context-free language benchmarks for RNNs, and show that it works even better than previous hardwired or highly specialized architectures. To the best of our knowledge, LSTM variants are also the first RNNs to learn a simple context-sensitive language, namely a/sup n/b/sup n/c/sup n/.",
        "citations": 718,
        "reasoning": "\"2001: Gers and Schmidhuber trained LSTM to learn languages unlearnable by traditional models such as Hidden Markov Models. [ 21 ]\" \u2013 *This claim directly references a paper where Gers and Schmidhuber used LSTM to learn languages that Hidden Markov Models cannot.* - [\"LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages\"] :-: Gers, F. A.; Schmidhuber, J. (2001).",
        "child_claim": "\"2001: Gers and Schmidhuber trained LSTM to learn languages unlearnable by traditional models such as Hidden Markov Models. [ 21 ]\" \u2013 *This claim directly references a paper where Gers and Schmidhuber used LSTM to learn languages that Hidden Markov Models cannot.* - [\"LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages\"] :-: Gers, F. A.; Schmidhuber, J. (2001)."
      }
    ],
    "peephole connections": [
      {
        "title": "A logical calculus of the ideas immanent in nervous activity",
        "url": "https://doi.org/10.1007/bf02478259",
        "abstract": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
        "citations": 17017,
        "reasoning": "\"The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles.\" \u2013 *This sentence refers to a specific paper proposing a neuron model with cycles, a foundational concept for RNNs.* - <Paper in the claim - [\"A logical calculus of the ideas immanent in nervous activity\"] :-: McCulloch, Warren S.; Pitts, Walter (1943)>.",
        "child_claim": "\"The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles.\" \u2013 *This sentence refers to a specific paper proposing a neuron model with cycles, a foundational concept for RNNs.* - <Paper in the claim - [\"A logical calculus of the ideas immanent in nervous activity\"] :-: McCulloch, Warren S.; Pitts, Walter (1943)>."
      },
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83296,
        "reasoning": "\"The most commonly used reference point for LSTM was published in 1997 in the journal Neural Computation . [1]\" \u2013 *This sentence directly states the publication details of a foundational LSTM paper.* - [\"Long short-term memory\"] :-: Sepp Hochreiter ; J\u00fcrgen Schmidhuber (1997)",
        "child_claim": "\"The most commonly used reference point for LSTM was published in 1997 in the journal Neural Computation . [1]\" \u2013 *This sentence directly states the publication details of a foundational LSTM paper.* - [\"Long short-term memory\"] :-: Sepp Hochreiter ; J\u00fcrgen Schmidhuber (1997)"
      },
      {
        "title": "LSTM recurrent networks learn simple context-free and context-sensitive languages",
        "url": "https://doi.org/10.1109/72.963769",
        "abstract": "Previous work on learning regular languages from exemplary training sequences showed that long short-term memory (LSTM) outperforms traditional recurrent neural networks (RNNs). We demonstrate LSTMs superior performance on context-free language benchmarks for RNNs, and show that it works even better than previous hardwired or highly specialized architectures. To the best of our knowledge, LSTM variants are also the first RNNs to learn a simple context-sensitive language, namely a/sup n/b/sup n/c/sup n/.",
        "citations": 718,
        "reasoning": "\" (Gers, Schmidhuber, and Cummins, 2000) added peephole connections. [21] [22]\" \u2013 *This sentence directly states that Gers, Schmidhuber, and Cummins added peephole connections and provides a reference.* - [\"LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages\"] :-: Gers, F. A.; Schmidhuber, J. (2001).",
        "child_claim": "\" (Gers, Schmidhuber, and Cummins, 2000) added peephole connections. [21] [22]\" \u2013 *This sentence directly states that Gers, Schmidhuber, and Cummins added peephole connections and provides a reference.* - [\"LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages\"] :-: Gers, F. A.; Schmidhuber, J. (2001)."
      }
    ]
  },
  "remaining_to_process": [
    "Recurrent Neural Networks (RNNs)",
    "Sequential Data",
    "Feedforward Neural Networks",
    "Recurrent Connections",
    "Temporal Dependencies",
    "Recurrent Unit",
    "Hidden State",
    "Vanishing Gradient Problem",
    "Long Short-Term Memory (LSTM)",
    "Gated Recurrent Units (GRUs)",
    "Transformers",
    "Self-Attention Mechanisms",
    "Recurrent inhibition",
    "reverberating circuit",
    "vestibulo-ocular reflex",
    "McCulloch-Pitts neuron model",
    "Hebbian learning rule",
    "Ising model",
    "Glauber dynamics",
    "Sherrington\u2013Kirkpatrick model",
    "Spin glass",
    "Hopfield network",
    "Binary activation functions",
    "Continuous activation functions",
    "Jordan network",
    "Elman network",
    "Neural history compressor",
    "Encoder-decoder sequence transduction",
    "Attention mechanisms",
    "Vanishing gradient problem",
    "Automatic differentiation",
    "Backpropagation",
    "Description length",
    "Chunking",
    "Automatizer",
    "Generative model",
    "Long-term memory (LTM)",
    "Atkinson\u2013Shiffrin memory model",
    "Sensory memory",
    "Short-term memory",
    "Working memory",
    "Explicit memory (declarative memory)",
    "Implicit memory (non-declarative memory)",
    "Episodic memory",
    "Semantic memory",
    "Procedural memory",
    "Emotional conditioning",
    "Sequential data",
    "Feedforward neural networks",
    "Recurrent connections",
    "Temporal dependencies",
    "Recurrent unit",
    "Hidden state",
    "Long short-term memory (LSTM)",
    "Gated recurrent units (GRUs)",
    "Self-attention mechanisms",
    "Cerebellar cortex",
    "Purkinje cells",
    "Granule cells",
    "Vestibulo-ocular reflex",
    "Reverberating circuit",
    "Cycles",
    "Motor control",
    "Neural feedback loops",
    "close-loop cross-coupled perceptrons",
    "recurrent connections",
    "Principles of Neurodynamics",
    "back-coupled perceptron networks",
    "infinitely deep feedforward network",
    "Bidirectional recurrent neural networks (BRNN)",
    "Seq2seq architecture",
    "Neural History Compressor",
    "Unsupervised Stack of RNNs",
    "Description Length Minimization",
    "Automatic Differentiation",
    "Very Deep Learning",
    "Parallel fiber",
    "Macy conferences",
    "Close-loop cross-coupled perceptrons",
    "Back-coupled perceptron networks",
    "Hebbian learning",
    "Unsupervised stack of RNNs",
    "Chunker",
    "Long-term memory",
    "Explicit memory",
    "Declarative memory",
    "Implicit memory",
    "Non-declarative memory",
    "Recurrent semicircles",
    "Recurrent reciprocal connections",
    "Golgi's method",
    "Negative feedback",
    "Recurrent Neural Network",
    "Hidden Markov Models",
    "Sequence Learning",
    "Cell",
    "Input Gate",
    "Output Gate",
    "Forget Gate",
    "Constant Error Carousel (CEC)",
    "Input gate",
    "Output gate",
    "Forget gate",
    "Keep gate",
    "Peephole connections",
    "Connectionist Temporal Classification",
    "Backpropagation through time",
    "Bidirectional LSTM",
    "Gated Recurrent Unit",
    "Highway network",
    "ResNet architecture",
    "xLSTM",
    "Meta-learning",
    "Speech Recognition",
    "Neuroevolution",
    "Policy Gradients",
    "Reinforcement Learning",
    "Protein Homology Detection",
    "Neural Architecture Search",
    "Handwriting Recognition",
    "Phoneme Error Rate",
    "Time-Aware LSTM (T-LSTM)",
    "Hidden Markov Models (HMM)",
    "Connectionist Temporal Classification (CTC)",
    "Recurrent neural network (RNN)",
    "Hidden Markov models",
    "Sequence learning",
    "LSTM unit",
    "Cell state",
    "Classification",
    "Data processing",
    "Time series analysis",
    "Speech recognition",
    "Machine translation",
    "Speech activity detection",
    "Robot control",
    "Video games",
    "Healthcare",
    "Gated Recurrent Unit (GRU)",
    "Forget gate LSTM",
    "mLSTM",
    "sLSTM",
    "Connected Handwriting Recognition",
    "Time-Aware LSTM",
    "LSTM"
  ],
  "claim_chains": {
    "Term Memory": [
      {
        "claim": "Long Short-Term Memory",
        "parent_keyword": "recurrent neural network"
      }
    ],
    "LSTM": [
      {
        "claim": "\"This issue was addressed by the development of the long short-term memory (LSTM) architecture in 1997, making it the standard RNN variant for handling long-term dependencies.\" - *This sentence explicitly states the development of LSTM in 1997, which is considered a foundational paper in the field.* - [\"XXX\"] :-: XXX (1997)",
        "parent_keyword": "recurrent neural network"
      },
      {
        "claim": "\"This issue was addressed by the development of the long short-term memory (LSTM) architecture in 1997, making it the standard RNN variant for handling long-term dependencies.\" - *This sentence explicitly states the development of LSTM in 1997, which is considered a foundational paper in the field.* - [\"XXX\"] :-: XXX (1997)",
        "parent_keyword": "peephole connections"
      }
    ],
    "peephole connections": [
      {
        "claim": "\"(Gers, Schmidhuber, and Cummins, 2000) added peephole connections.\" \u2013 *This sentence highlights the authors and year of publication for the addition of peephole connections.* - XXX :-: Gers, Schmidhuber, and Cummins (2000)",
        "parent_keyword": "LSTM"
      },
      {
        "claim": "\"(Gers, Schmidhuber, and Cummins, 2000) added peephole connections.\" \u2013 *This sentence highlights the authors and year of publication for the addition of peephole connections.* - XXX :-: Gers, Schmidhuber, and Cummins (2000)",
        "parent_keyword": "recurrent neural network"
      }
    ]
  }
}