{
  "keywords": {
    "Convolutional neural network": [
      {
        "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
        "url": "https://doi.org/10.1007/bf00344251",
        "abstract": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname \u201cneocognitron\u201d. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \u201cS-cells\u201d, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \u201cC-cells\u201d similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any \u201cteacher\u201d during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.",
        "citations": 5222,
        "reasoning": "Convolutional networks were inspired by biological processes [ 18 ] [ 19 ] [ 20 ] [ 21 ] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex . \u2013 *This sentence speaks to the origin of CNNs and provides references to papers that detail the biological inspiration.* - [\"Neocognitron\"] :-: Fukushima (2007)"
      },
      {
        "title": "Receptive fields and functional architecture of monkey striate cortex",
        "url": "https://doi.org/10.1113/jphysiol.1968.sp008455",
        "abstract": "1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded.2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent.3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other.4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers (II and the upper two-thirds of III) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer III, and in IV A and IV B. In layer IV B they form a large proportion of the population, whereas complex cells are rare. In layers IV A and IV B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer IV most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and VI contain mostly complex and hypercomplex cells, binocularly driven.5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer IV, the higher orders in the upper and lower layers.",
        "citations": 6513,
        "reasoning": "Convolutional networks were inspired by biological processes [ 18 ] [ 19 ] [ 20 ] [ 21 ] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex . \u2013 *This sentence speaks to the origin of CNNs and provides references to papers that detail the biological inspiration.* - [\"Receptive fields and functional architecture of monkey striate cortex\"] :-: Hubel & Wiesel (1968)"
      },
      {
        "title": "Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements",
        "url": "https://doi.org/10.1109/tssc.1969.300225",
        "abstract": "A new type of visual feature extracting network has been synthesized, and the response of the network has been simulated on a digital computer. This research has been done as a first step towards the realization of a recognizer of handwritten characters. The design of the network was suggested by biological systems, especially, the visual systems of cat and monkey. The network is composed of analog threshold elements connected in layers. Each analog threshold element receives inputs from a large number of elements in the neighbouring layers and performs its own special functions. It takes care of one restricted part of the photoreceptor layer, on which an input pattem is presented, and it responds to one particular feature of the input pattem, such as brightness contrast, a dot in the pattern, a line segment of a particular orientation, or an end of the line. This means that the network performs parallel processing of the information. With the propagation of the information through the layered network, the input pattern is successively decomposed into dots, groups of line segments of the same orientation, and the ends of these line segments.",
        "citations": 227,
        "reasoning": "In 1969, Kunihiko Fukushima introduced a multilayer visual feature detection network, inspired by the above-mentioned work of Hubel and Wiesel, in which \"All the elements in one layer have the same set of interconnecting coefficients; the arrangement of the elements and their interconnections are all homogeneous over a given layer.\" \u2013 *This sentence describes Fukushima's introduction of a network with convolutional network characteristics.* - [\"Visual feature extraction by a multilayered network of analog threshold elements\"] :-: <Kunihiko Fukushima> (1969)"
      },
      {
        "title": "Phoneme recognition using time-delay neural networks",
        "url": "https://doi.org/10.1109/29.21701",
        "abstract": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
        "citations": 2574,
        "reasoning": "The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was an early convolutional network exhibiting shift-invariance [43]. \u2013 *This claims the invention of TDNN as an early form of CNN and references the first paper describing it* - [\"Phoneme Recognition Using Time-Delay Neural Networks\"] :-: Alex Waibel et al. (1987)"
      },
      {
        "title": "Backpropagation Applied to Handwritten Zip Code Recognition",
        "url": "https://doi.org/10.1162/neco.1989.1.4.541",
        "abstract": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.",
        "citations": 10875,
        "reasoning": "Following the advances in the training of 1-D CNNs by Waibel et al. (1987), Yann LeCun et al. (1989) [ 49 ] used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. \u2013 *This sentence directly states that LeCun et al. used back-propagation to train CNNs, a crucial step in making them practical.* - [\"Backpropagation Applied to Handwritten Zip Code Recognition\"] :-: LeCun et al. (1989)."
      },
      {
        "title": "Learning algorithms for classification: A comparison on handwritten digit recognition",
        "url": null,
        "abstract": "No abstract found",
        "citations": 380,
        "reasoning": "LeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1995, [ 54 ] classifies hand-written numbers on checks ( British English : cheques ) digitized in 32x32 pixel images. \u2013 *This claim directly identifies LeNet-5 and associates it with LeCun et al. and handwritten digit recognition* - [\"Learning algorithms for classification: A comparison on handwritten digit recognition\"] :-: Lecun, Y.; Jackel, L. D.; Bottou, L.; Cortes, C.; Denker, J. S.; Drucker, H.; Guyon, I.; Muller, U. A.; Sackinger, E.; Simard, P.; Vapnik, V. (1995)"
      },
      {
        "title": "Gradient-based learning applied to document recognition",
        "url": "https://doi.org/10.1109/5.726791",
        "abstract": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.",
        "citations": 51526,
        "reasoning": "Gradient-based learning applied to document recognition [ 55 ] - *This claim directly identifies a key paper related to gradient-based learning and document recognition, further solidifying the work's foundation* - [\"Gradient-based learning applied to document recognition\"] :-: Lecun, Y.; Bottou, L.; Bengio, Y.; Haffner, P. (1998)"
      }
    ],
    "Recurrent neural network": [
      {
        "title": "A logical calculus of the ideas immanent in nervous activity",
        "url": "https://doi.org/10.1007/bf02478259",
        "abstract": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
        "citations": 16988,
        "reasoning": "The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles. \u2013 *This claim directly mentions a paper and its contribution to the concept of cycles in networks, a foundational idea for RNNs.* - [\"A logical calculus of the ideas immanent in nervous activity\"] :-: McCulloch, Warren S.; Pitts, Walter (1943)."
      },
      {
        "title": "Principles of Neurodynamics.",
        "url": "https://doi.org/10.2307/2312103",
        "abstract": "No abstract found",
        "citations": 2300,
        "reasoning": "Later, in Principles of Neurodynamics (1961), he described \"closed-loop cross-coupled\" and \"back-coupled\" perceptron networks, and made theoretical and experimental studies for Hebbian learning in these networks, [ 17 ] :\u200aChapter 19, 21 and noted that a fully cross-coupled perceptron network is equivalent to an infinitely deep feedforward network. \u2013 *This sentence explicitly mentions Rosenblatt describing recurrent networks in \"Principles of Neurodynamics\" and is therefore very important* - [\"Principles of Neurodynamics\"] :-: Rosenblatt (1961)"
      }
    ],
    "LSTM": [
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83100,
        "reasoning": "\"Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.\" \u2013 *This claim directly states the inventors and the invention of LSTM.* - [\"Long Short-Term Memory\"] :-: Hochreiter, Sepp ; Schmidhuber, J\u00fcrgen (1995)"
      },
      {
        "title": "LSTM can Solve Hard Long Time Lag Problems",
        "url": null,
        "abstract": "Standard recurrent nets cannot deal with long minimal time lags between relevant signals. Several recent NIPS papers propose alternative methods. We first show: problems used to promote various previous algorithms can be solved more quickly by random weight guessing than by the proposed algorithms. We then use LSTM, our own recent algorithm, to solve a hard problem that can neither be quickly solved by random search nor by any other recurrent net algorithm we are aware of.",
        "citations": 756,
        "reasoning": "LSTM can solve hard long time lag problems [3] \u2013 *While the sentence as is does not explicitly say the reference contains proof, it does imply it as the reference is after the part of the sentence explaining what LSTM can do* - [\"LSTM can solve hard long time lag problems\"] :-: Hochreiter, Sepp; Schmidhuber, J\u00fcrgen (1996)"
      }
    ]
  },
  "remaining_to_process": [
    "Feedforward neural network",
    "Filter optimization",
    "Convolution kernel",
    "Shared weights",
    "Backpropagation",
    "Feature maps",
    "Shift invariant",
    "Space invariant",
    "Translation equivariant",
    "Receptive field",
    "Visual cortex",
    "CNN",
    "Vision processing",
    "Visual cell types",
    "Striate cortex",
    "multilayer visual feature detection network",
    "interconnecting coefficients",
    "homogeneous layer",
    "ReLU (rectified linear unit)",
    "Neocognitron",
    "Lateral inhibition",
    "shift invariance",
    "speech recognition",
    "correlation",
    "filter",
    "Time Delay Neural Network",
    "Phoneme Recognition",
    "Shift-invariance",
    "Weight Sharing",
    "Gradient Descent",
    "Spectrograms",
    "Convolution kernel coefficients",
    "Back-propagation",
    "1-D CNNs",
    "Hand-written numbers",
    "Shift-invariant pattern recognition neural network",
    "Max pooling",
    "TDNNs",
    "Cresceptron",
    "Spatial averaging",
    "Inhibition",
    "Saturation",
    "Downsampling",
    "LeNet-5",
    "Convolutional network",
    "Handwritten digit recognition",
    "Gradient-based learning",
    "Document recognition",
    "Shift-invariant neural network",
    "Image character recognition",
    "Convolutional interconnections",
    "Image feature layers",
    "Generalization ability",
    "Medical image segmentation",
    "Mammograms",
    "Electromyography",
    "De-convolution",
    "Graphics Processing Units (GPUs)",
    "GPGPU",
    "Deep Belief Networks",
    "Deep Feedforward Networks",
    "Image Recognition",
    "Intel Xeon Phi",
    "Recurrent neural networks (RNNs)",
    "Sequential data",
    "Feedforward neural networks",
    "Recurrent connections",
    "Temporal dependencies",
    "Recurrent unit",
    "Hidden state",
    "Vanishing gradient problem",
    "Long short-term memory (LSTM)",
    "Gated recurrent units (GRUs)",
    "Self-attention mechanisms",
    "Transformers",
    "Recurrent semicircle",
    "Recurrent reciprocal connections",
    "Reverberating circuit",
    "Feedback",
    "Feedforward structure",
    "Neural feedback loops",
    "Recurrent inhibition",
    "close-loop cross-coupled perceptrons",
    "Hebbian learning rule",
    "closed-loop cross-coupled perceptron networks",
    "back-coupled perceptron networks",
    "Ising model",
    "Glauber dynamics",
    "Sherrington\u2013Kirkpatrick model",
    "spin glass",
    "Hopfield network",
    "binary activation functions",
    "continuous activation functions",
    "Jordan network",
    "Elman network",
    "Neural history compressor",
    "Bidirectional recurrent neural networks (BRNN)",
    "Encoder-decoder sequence transduction",
    "Attention mechanisms",
    "Sequence transduction",
    "Machine translation",
    "Language modeling",
    "Multilingual Language Processing",
    "Convolutional neural networks (CNNs)",
    "Automatic image captioning",
    "Speech recognition",
    "Large-vocabulary speech recognition",
    "Text-to-speech synthesis",
    "Unsupervised stack of RNNs",
    "Description length",
    "Automatic differentiation",
    "Generative model",
    "Very Deep Learning",
    "Unfolded RNN",
    "Long Short-Term Memory (LSTM)",
    "Recurrent Neural Network (RNN)",
    "Vanishing Gradient Problem",
    "Hidden Markov Models",
    "Sequence Learning",
    "Input Gate",
    "Output Gate",
    "Forget Gate",
    "Cell",
    "Time Series Analysis",
    "Speech Recognition",
    "Machine Translation",
    "Speech Activity Detection",
    "Robot Control",
    "Constant Error Carousel (CEC)",
    "Input gate",
    "Output gate",
    "Forget gate",
    "Peephole connections",
    "Connectionist Temporal Classification (CTC)",
    "Backpropagation through time",
    "Bidirectional LSTM",
    "Gated Recurrent Unit (GRU)",
    "Highway network",
    "ResNet architecture",
    "xLSTM",
    "mLSTM",
    "sLSTM",
    "Meta-learning",
    "Neuroevolution",
    "Policy Gradients",
    "Reinforcement Learning",
    "Protein Homology Detection",
    "Neural Architecture Search",
    "Handwriting Recognition",
    "Phoneme Error Rate",
    "Time-Aware LSTM",
    "Deep learning",
    "Text-to-image model",
    "Diffusion techniques",
    "Generative artificial intelligence",
    "Inpainting",
    "Outpainting",
    "Image-to-image translations",
    "Latent diffusion model",
    "Deep generative artificial neural network",
    "Model weights",
    "GPU",
    "VRAM",
    "Feature Maps",
    "Overfitting",
    "Weight decay",
    "Dropout",
    "Image classification algorithms",
    "Kernels"
  ]
}