{
  "keywords": {
    "recurrent neural network": [
      {
        "title": "A logical calculus of the ideas immanent in nervous activity",
        "url": "https://doi.org/10.1007/bf02478259",
        "abstract": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
        "citations": 17012,
        "reasoning": "\"The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles.\" \u2013 *This identifies a key early paper that considered cyclic networks, a foundational concept for RNNs.* - <[\"A logical calculus of the ideas immanent in nervous activity\"] :-: McCulloch, Warren S.; Pitts, Walter (1943)>."
      },
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83100,
        "reasoning": "\"Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.\" \u2013 *This directly identifies the inventors and approximate invention date of LSTMs, a major RNN architecture.* - <[\"Long Short Term Memory\"] :-: Sepp Hochreiter ; J\u00fcrgen Schmidhuber (1995)>."
      },
      {
        "title": "Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms",
        "url": "https://doi.org/10.2307/1419730",
        "abstract": "Abstract : Part I attempts to review the background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons. In Chapter 2, a brief review of the main alternative approaches to the development of brain models is presented. Chapter 3 considers the physiological and psychological criteria for a suitable model, and attempts to evaluate the empirical evidence which is available on several important issues. Chapter 4 contains basic definitions and some of the notation to be used in later sections are presented. Parts II and III are devoted to a summary of the established theoretical results obtained to date. Part II (Chapters 5 through 14) deals with the theory of three-layer series-coupled perceptrons, on which most work has been done to date. Part III (Chapters 15 through 20) deals with the theory of multi-layer and cross-coupled perceptrons. Part IV is concerned with more speculative models and problems for future analysis. Of necessity, the final chapters become increasingly heuristic in character, as the theory of perceptrons is not yet complete, and new possibilities are continually coming to light.",
        "citations": 1856,
        "reasoning": "\"Later, in Principles of Neurodynamics (1961), he described \"closed-loop cross-coupled\" and \"back-coupled\" perceptron networks, and made theoretical and experimental studies for Hebbian learning in these networks, [11] :\u200aChapter 19, 21 and noted that a fully cross-coupled perceptron network is equivalent to an infinitely deep feedforward network.\" \u2013 *This sentence discusses Rosenblatt's work on perceptrons with recurrent connections, which is relevant to the history of RNNs. The claim is expanded to include the reference.* - <[\"PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS\"] :-: Frank Rosenblatt (1961)>."
      }
    ],
    "Term Memory": [
      {
        "title": "A logical calculus of the ideas immanent in nervous activity",
        "url": "https://doi.org/10.1007/bf02478259",
        "abstract": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
        "citations": 17012,
        "reasoning": "\"The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles.\" \u2013 *This identifies a key early paper that considered cyclic networks, a foundational concept for RNNs.* - <[\"A logical calculus of the ideas immanent in nervous activity\"] :-: McCulloch, Warren S.; Pitts, Walter (1943)>."
      },
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83100,
        "reasoning": "\"Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.\" \u2013 *This directly identifies the inventors and approximate invention date of LSTMs, a major RNN architecture.* - <[\"Long Short Term Memory\"] :-: Sepp Hochreiter ; J\u00fcrgen Schmidhuber (1995)>."
      },
      {
        "title": "Sequence to Sequence Learning with Neural Networks",
        "url": "https://doi.org/10.48550/arxiv.1409.3215",
        "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
        "citations": 13804,
        "reasoning": "\"The papers most commonly cited as the originators that produced seq2seq are two papers from 2014.\" \u2013 *This claim indicates that two papers in 2014 are foundational to the development of seq2seq models.* - [\"Sequence to Sequence Learning with Neural Networks\"] :-: <Ilya Sutskever; Oriol Vinyals; Quoc V. Le> (2014)"
      }
    ],
    "convolutional neural network": [
      {
        "title": "Backpropagation Applied to Handwritten Zip Code Recognition",
        "url": "https://doi.org/10.1162/neco.1989.1.4.541",
        "abstract": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.",
        "citations": 10884,
        "reasoning": "\"Backpropagation Applied to Handwritten Zip Code Recognition\" \u2013 *This paper is considered one of the foundational papers in the development of CNNs, specifically focusing on applying backpropagation to train a CNN for handwritten digit recognition.* - [\"Backpropagation Applied to Handwritten Zip Code Recognition\"] :-: LeCun et al. (1989)"
      },
      {
        "title": "Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements",
        "url": "https://doi.org/10.1109/tssc.1969.300225",
        "abstract": "A new type of visual feature extracting network has been synthesized, and the response of the network has been simulated on a digital computer. This research has been done as a first step towards the realization of a recognizer of handwritten characters. The design of the network was suggested by biological systems, especially, the visual systems of cat and monkey. The network is composed of analog threshold elements connected in layers. Each analog threshold element receives inputs from a large number of elements in the neighbouring layers and performs its own special functions. It takes care of one restricted part of the photoreceptor layer, on which an input pattem is presented, and it responds to one particular feature of the input pattem, such as brightness contrast, a dot in the pattern, a line segment of a particular orientation, or an end of the line. This means that the network performs parallel processing of the information. With the propagation of the information through the layered network, the input pattern is successively decomposed into dots, groups of line segments of the same orientation, and the ends of these line segments.",
        "citations": 228,
        "reasoning": "\"In 1969, Kunihiko Fukushima introduced a multilayer visual feature detection network, inspired by the above-mentioned work of Hubel and Wiesel, in which \"All the elements in one layer have the same set of interconnecting coefficients; the arrangement of the elements and their interconnections are all homogeneous over a given layer.\"\" \u2013 *Describes the foundational concept of shared weights in CNNs introduced by Fukushima.* - [\"Visual feature extraction by a multilayered network of analog threshold elements\"] :-: <Kunihiko Fukushima> (1969)"
      },
      {
        "title": "Phoneme recognition using time-delay neural networks",
        "url": "https://doi.org/10.1109/29.21701",
        "abstract": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
        "citations": 2577,
        "reasoning": "\"The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was an early convolutional network exhibiting shift-invariance.\" \u2013 *Identifies the TDNN and its inventors.* - [\"Phoneme Recognition Using Time-Delay Neural Networks\"] :-: <Alex Waibel et al.> (1987)"
      },
      {
        "title": "ImageNet Classification with Deep Convolutional Neural Networks",
        "url": null,
        "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called dropout that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",
        "citations": 68574,
        "reasoning": "\"Subsequently, AlexNet , a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012.\" \u2013 *Identifies AlexNet and its impact.* - [\"ImageNet Classification with Deep Convolutional Neural Networks\"] :-: <Alex Krizhevsky et al.> (2012)"
      },
      {
        "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
        "url": "https://doi.org/10.1007/bf00344251",
        "abstract": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname \u201cneocognitron\u201d. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \u201cS-cells\u201d, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \u201cC-cells\u201d similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any \u201cteacher\u201d during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.",
        "citations": 5226,
        "reasoning": "\"Convolutional networks were inspired by biological processes [18] [19] [20] [21] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex .\" \u2013 *This claim directly suggests the biological inspiration behind convolutional networks, referencing papers that delve into the visual cortex and early models.* - [\"Neocognitron\"] :-: Fukushima (2007)"
      },
      {
        "title": "Receptive fields and functional architecture of monkey striate cortex",
        "url": "https://doi.org/10.1113/jphysiol.1968.sp008455",
        "abstract": "1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded.2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent.3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other.4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers (II and the upper two-thirds of III) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer III, and in IV A and IV B. In layer IV B they form a large proportion of the population, whereas complex cells are rare. In layers IV A and IV B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer IV most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and VI contain mostly complex and hypercomplex cells, binocularly driven.5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer IV, the higher orders in the upper and lower layers.",
        "citations": 6517,
        "reasoning": "\"Convolutional networks were inspired by biological processes [18] [19] [20] [21] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex .\" \u2013 *This claim directly suggests the biological inspiration behind convolutional networks, referencing papers that delve into the visual cortex and early models.* - [\"Receptive fields and functional architecture of monkey striate cortex\"] :-: Hubel & Wiesel (1968)"
      },
      {
        "title": "An Artificial Neural Network for Spatio-Temporal Bipolar Patterns: Application to Phoneme Classification",
        "url": null,
        "abstract": "An artificial neural network is developed to recognize spatio-temporal bipolar patterns associatively. The function of a formal neuron is generalized by replacing multiplication with convolution, weights with transfer functions, and thresholding with nonlinear transform following adaptation. The Hebbian learn\u00ad ing rule and the delta learning rule are generalized accordingly, resulting in the learning of weights and delays. The neural network which was first developed for spatial patterns was thus generalized for spatio-temporal patterns. It was tested using a set of bipolar input patterns derived from speech signals, showing robust classification of 30 model phonemes.",
        "citations": 52,
        "reasoning": "\"The term \"convolution\" first appears in neural networks in a paper by Toshiteru Homma, Les Atlas, and Robert Marks II at the first Conference on Neural Information Processing Systems in 1987.\" \u2013 *This sentence explicitly states the first appearance of the term \"convolution\" in the context of neural networks, linking it to a specific paper and conference.* - <Homma, Toshiteru; Les Atlas; Robert Marks II (1987) - [\"An Artificial Neural Network for Spatio-Temporal Bipolar Patterns: Application to Phoneme Classification\"] :-: Homma, Atlas, and Marks II (1987)>."
      },
      {
        "title": "Deep learning",
        "url": "https://doi.org/10.1038/nature14539",
        "abstract": "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.",
        "citations": 68803,
        "reasoning": "\"It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation .\" \u2013 *This sentence explicitly states a key architectural contribution of the TDNN in the context of CNNs.* - [\"Deep learning\"] :-: Yann LeCun, Yoshua Bengio, Geoffrey Hinton (2015)"
      },
      {
        "title": "Neural Network Recognizer for Hand-Written Zip Code Digits",
        "url": null,
        "abstract": "This paper describes the construction of a system that recognizes handprinted digits, using a combination of classical techniques and neural-net methods. The system has been trained and tested on real-world data, derived from zip codes seen on actual U.S. Mail. The system rejects a small percentage of the examples as unclassifiable, and achieves a very low error rate on the remaining examples. The system compares favorably with other state-of-the art recognizers. While some of the methods are specific to this task, it is hoped that many of the techniques will be applicable to a wide range of recognition tasks.",
        "citations": 147,
        "reasoning": "\"Denker et al. (1989) designed a 2-D CNN system to recognize hand-written ZIP Code numbers.\" \u2013 *This claim presents the work of Denker et al. in designing a 2-D CNN system, which can be considered a foundational step in the development of CNNs.* - [\"Neural network recognizer for hand-written zip code digits\"] :-: Denker, J S, Gardner, W R, Graf, H. P, Henderson, D, Howard, R E, Hubbard, W, Jackel, L D, BaIrd, H S, and Guyon (1989)"
      }
    ],
    "coupled perceptrons": [
      {
        "title": "A logical calculus of the ideas immanent in nervous activity",
        "url": "https://doi.org/10.1007/bf02478259",
        "abstract": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
        "citations": 17012,
        "reasoning": "\"The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles.\" \u2013 *This identifies a foundational paper that introduced cycles within neural networks, a key concept for recurrence.* - [\"A logical calculus of the ideas immanent in nervous activity\"] :-: <Warren S. McCulloch; Pitts, Walter> (1943)"
      }
    ],
    "Sherrington\u2013Kirkpatrick model": [
      {
        "title": "Stability of the Sherrington-Kirkpatrick solution of a spin glass model",
        "url": "https://doi.org/10.1088/0305-4470/11/5/028",
        "abstract": "The stationary point used by Sherrington and Kirkpatrick (1975) in their evaluation of the free energy of a spin glass by the method of steepest descent is examined carefully. It is found that, although this point is a maximum of the integrand at high temperatures, it is not a maximum in the spin glass phase nor in the ferromagnetic phase at low temperatures. The instability persists in the presence of a magnetic field. Results are given for the limit of stability both for a partly ferromagnetic interaction in the absence of an external field and for a purely random interaction in the presence of a field.",
        "citations": 1982,
        "reasoning": "\"Sherrington and Kirkpatrick proposed the SK model in 1975, and solved it by the replica method.\" \u2013 *This sentence directly mentions the proposal of the SK model and the method used to solve it* - [\"Solution of the Sherrington-Kirkpatrick model\"] :-: Sherrington, D.; Kirkpatrick, S. (1975)."
      },
      {
        "title": "The Sherrington-Kirkpatrick Model",
        "url": "https://doi.org/10.1007/978-1-4614-6289-7",
        "abstract": "No abstract found",
        "citations": 265,
        "reasoning": "\"Sherrington and Kirkpatrick proposed the SK model in 1975, and solved it by the replica method.\" \u2013 *This sentence directly states the original proposal of the SK model and the method used to solve it.* - [\"Sherrington-Kirkpatrick model\"] :-: Sherrington and Kirkpatrick (1975)"
      }
    ],
    "random forest": [
      {
        "title": "Random decision forests",
        "url": "https://doi.org/10.1109/icdar.1995.598994",
        "abstract": "Decision trees are attractive classifiers due to their high execution speed. But trees derived with traditional methods often cannot be grown to arbitrary complexity for possible loss of generalization accuracy on unseen data. The limitation on complexity usually means suboptimal accuracy on training data. Following the principles of stochastic modeling, we propose a method to construct tree-based classifiers whose capacity can be arbitrarily expanded for increases in accuracy for both training and unseen data. The essence of the method is to build multiple trees in randomly selected subspaces of the feature space. Trees in, different subspaces generalize their classification in complementary ways, and their combined classification can be monotonically improved. The validity of the method is demonstrated through experiments on the recognition of handwritten digits.",
        "citations": 4492,
        "reasoning": "\"The first algorithm for random decision forests was created in 1995 by Tin Kam Ho [1] using the random subspace method\" \u2013 *This sentence directly states that Tin Kam Ho created the first algorithm using the random subspace method.* - [\"Random Decision Forests\"] :-: Tin Kam Ho (1995)"
      },
      {
        "title": "Shape Quantization and Recognition with Randomized Trees",
        "url": "https://doi.org/10.1162/neco.1997.9.7.1545",
        "abstract": "We explore a new approach to shape recognition based on a virtually infinite family of binary features (queries) of the image data, designed to accommodate prior information about shape invariance and regularity. Each query corresponds to a spatial arrangement of several local topographic codes (or tags), which are in themselves too primitive and common to be informative about shape. All the discriminating power derives from relative angles and distances among the tags. The important attributes of the queries are a natural partial ordering corresponding to increasing structure and complexity; semi-invariance, meaning that most shapes of a given class will answer the same way to two queries that are successive in the ordering; and stability, since the queries are not based on distinguished points and substructures. No classifier based on the full feature set can be evaluated, and it is impossible to determine a priori which arrangements are informative. Our approach is to select informative features and build tree classifiers at the same time by inductive learning. In effect, each tree provides an approximation to the full posterior where the features chosen depend on the branch that is traversed. Due to the number and nature of the queries, standard decision tree construction based on a fixed-length feature vector is not feasible. Instead we entertain only a small random sample of queries at each node, constrain their complexity to increase with tree depth, and grow multiple trees. The terminal nodes are labeled by estimates of the corresponding posterior distribution over shape classes. An image is classified by sending it down every tree and aggregating the resulting distributions. The method is applied to classifying handwritten digits and synthetic linear and nonlinear deformations of three hundred [Formula: see text] symbols. State-of-the-art error rates are achieved on the National Institute of Standards and Technology database of digits. The principal goal of the experiments on [Formula: see text] symbols is to analyze invariance, generalization error and related issues, and a comparison with artificial neural networks methods is presented in this context. [Figure: see text]",
        "citations": 1238,
        "reasoning": "\"The extension combines Breiman's \"bagging\" idea and random selection of features, introduced first by Ho [1] and later independently by Amit and Geman [11] in order to construct a collection of decision trees with controlled variance.\" \u2013 *This details that Ho introduced random selection of features and Amit and Geman did it independently.* - [\"Shape quantization and recognition with randomized trees\"] :-: Amit Y, Geman D (1997)"
      }
    ],
    "support vector machine": [
      {
        "title": "Support Vector Method for Novelty Detection",
        "url": null,
        "abstract": "Suppose you are given some dataset drawn from an underlying probability distribution P and you want to estimate a simple subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified \u03bd between 0 and 1.\r\n\r\nWe propose a method to approach this problem by trying to estimate a function f which is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. We provide a theoretical analysis of the statistical performance of our algorithm.\r\n\r\nThe algorithm is a natural extension of the support vector algorithm to the case of unlabelled data.",
        "citations": 1765,
        "reasoning": "\"The \"soft margin\" incarnation, as is commonly used in software packages, was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995. [ 2 ]\" \u2013 *This sentence directly mentions the origin of a specific variant of SVMs, the \"soft margin\" version, and names the authors and publication date.* - <[2] - [\"The Support Vector method\"] :-: Vladimir N. Vapnik (1997)>."
      }
    ],
    "generative adversarial model": [
      {
        "title": "GAN\uff08Generative Adversarial Nets\uff09",
        "url": "https://doi.org/10.3156/jsoft.29.5_177_2",
        "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to \u00bd everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
        "citations": 23357,
        "reasoning": "\"The concept was initially developed by Ian Goodfellow and his colleagues in June 2014.\" \u2013 *This sentence directly states the original developers and the approximate date of the development of GANs.* - [\"Generative Adversarial Nets\"] :-: Goodfellow, Ian; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua (2014)"
      }
    ],
    "LSTM": [
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83100,
        "reasoning": "\"Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.\" \u2013 *This directly references the inventors and year of invention of LSTM networks* - <Paper in the claim - [\"Long Short Term Memory\"] :-: Sepp Hochreiter ; J\u00fcrgen Schmidhuber (1995)>."
      },
      {
        "title": "LSTM can Solve Hard Long Time Lag Problems",
        "url": null,
        "abstract": "Standard recurrent nets cannot deal with long minimal time lags between relevant signals. Several recent NIPS papers propose alternative methods. We first show: problems used to promote various previous algorithms can be solved more quickly by random weight guessing than by the proposed algorithms. We then use LSTM, our own recent algorithm, to solve a hard problem that can neither be quickly solved by random search nor by any other recurrent net algorithm we are aware of.",
        "citations": 757,
        "reasoning": "\"An LSTM unit is typically composed of a cell and three gates : an input gate, an output gate, [ 3 ] and a forget gate. [ 4 ]\" \u2013 *This sentence describes the fundamental components of an LSTM unit.* - <Paper in the claim (from references if available) - [\"LSTM can solve hard long time lag problems\"] :-: Sepp Hochreiter; J\u00fcrgen Schmidhuber (1996)>."
      },
      {
        "title": "Learning to Forget: Continual Prediction with LSTM",
        "url": "https://doi.org/10.1162/089976600300015015",
        "abstract": "Long short-term memory (LSTM; Hochreiter &amp; Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive \u201cforget gate\u201d that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way.",
        "citations": 4696,
        "reasoning": "<Paper in the claim (from references if available) - [\"Learning to Forget: Continual Prediction with LSTM\"] :-: Felix A. Gers; J\u00fcrgen Schmidhuber; Fred Cummins (2000)>."
      },
      {
        "title": "LSTM recurrent networks learn simple context-free and context-sensitive languages",
        "url": "https://doi.org/10.1109/72.963769",
        "abstract": "Previous work on learning regular languages from exemplary training sequences showed that long short-term memory (LSTM) outperforms traditional recurrent neural networks (RNNs). We demonstrate LSTMs superior performance on context-free language benchmarks for RNNs, and show that it works even better than previous hardwired or highly specialized architectures. To the best of our knowledge, LSTM variants are also the first RNNs to learn a simple context-sensitive language, namely a/sup n/b/sup n/c/sup n/.",
        "citations": 718,
        "reasoning": "\"2001: Gers and Schmidhuber trained LSTM to learn languages unlearnable by traditional models such as Hidden Markov Models. [ 21 ]\" \u2013 *This claim mentions a specific paper that directly contributes to the development and capabilities of LSTM, showcasing its advantage over Hidden Markov Models.* - [\"LSTM Recurrent Networks Learn Simple Context Free and Context Sensitive Languages\"] :-: Gers and Schmidhuber (2001)"
      },
      {
        "title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
        "url": "https://doi.org/10.1016/j.neunet.2005.06.042",
        "abstract": "No abstract found",
        "citations": 4683,
        "reasoning": "\"Graves, Schmidhuber, 2005) [ 26 ] published LSTM with full backpropagation through time and bidirectional LSTM.\" \u2013 *This sentence explicitly states the publication of LSTM with specific features, indicating a foundational paper.* - [\"Framewise phoneme classification with bidirectional LSTM and other neural network architectures\"] :-: Graves, A.; Schmidhuber, J. (2005)."
      }
    ],
    "diffusion model": [
      {
        "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
        "url": "https://doi.org/10.48550/arxiv.2011.13456",
        "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",
        "citations": 887,
        "reasoning": "\"Score-Based Generative Modeling through Stochastic Differential Equations\" \u2013 *Provides the name of the paper and the information regarding how score based generative modelling relates to diffusion models, as well as being a key reference in the field.* - [\"Score-Based Generative Modeling through Stochastic Differential Equations\"] :-: Song, Yang; Sohl-Dickstein, Jascha; Kingma, Diederik P.; Kumar, Abhishek; Ermon, Stefano; Poole, Ben (2021)."
      },
      {
        "title": "Denoising Diffusion Probabilistic Models",
        "url": "https://doi.org/10.48550/arxiv.2006.11239",
        "abstract": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion",
        "citations": 4466,
        "reasoning": "\"Denoising Diffusion Probabilistic Models\" \u2013 *Provides the name of the paper and the information regarding how denoising diffusion probabilistic models relates to diffusion models, as well as being a key reference in the field.* - [\"Denoising Diffusion Probabilistic Models\"] :-: Ho, Jonathan; Jain, Ajay; Abbeel, Pieter (2020)."
      }
    ]
  },
  "remaining_to_process": [
    "Recurrent neural networks",
    "Sequential data",
    "Recurrent connections",
    "Recurrent unit",
    "Hidden state",
    "Temporal dependencies",
    "Vanishing gradient problem",
    "Long short-term memory (LSTM)",
    "Gated recurrent units (GRUs)",
    "Transformers",
    "Self-attention mechanisms",
    "Recurrent semicircles",
    "Recurrent reciprocal connections",
    "Reverberating circuit",
    "McCulloch-Pitts neuron model",
    "Recurrent inhibition",
    "Close-loop cross-coupled perceptrons",
    "Glauber dynamics",
    "Sherrington-Kirkpatrick model",
    "Hopfield network",
    "Neural history compressor",
    "Very Deep Learning",
    "BRNN",
    "Encoder-decoder",
    "Sequence transduction",
    "Attention mechanisms",
    "Unsupervised Learning",
    "Recurrent Neural Networks (RNNs)",
    "Prediction",
    "Hierarchy",
    "Compressed Representation",
    "Description Length",
    "Generative Model",
    "Vanishing Gradient Problem",
    "Backpropagation",
    "Deep Learning",
    "Chunking",
    "Automatization",
    "Long-term memory (LTM)",
    "Atkinson\u2013Shiffrin memory model",
    "Sensory memory",
    "Short-term memory",
    "Working memory",
    "Explicit memory (declarative memory)",
    "Implicit memory (non-declarative memory)",
    "Episodic memory",
    "Semantic memory",
    "Procedural memory",
    "Emotional conditioning",
    "Recurrent neural networks (RNNs)",
    "Recurrent, reciprocal connections",
    "Closed-loop cross-coupled perceptrons",
    "Back-coupled perceptron networks",
    "Bidirectional recurrent neural networks (BRNN)",
    "Encoder-decoder sequence transduction",
    "Seq2seq",
    "Automatic differentiation",
    "Generative model",
    "Unfolded RNN",
    "Long-term memory",
    "Explicit memory",
    "Declarative memory",
    "Implicit memory",
    "Non-declarative memory",
    "Feedforward Neural Network",
    "Filter Optimization",
    "Shared Weights",
    "Convolution Kernel",
    "Cross-correlation",
    "Feature Maps",
    "Translation Equivariance",
    "Receptive Field",
    "Visual Cortex",
    "Neocognitron",
    "ReLU (Rectified Linear Unit)",
    "Time Delay Neural Network (TDNN)",
    "Shift Invariance",
    "Max Pooling",
    "LeNet-5",
    "GPU Implementation",
    "AlexNet",
    "Feedforward neural networks",
    "Unsupervised stack of RNNs",
    "Description length",
    "Conscious chunker",
    "Subconscious automatizer",
    "Long short-term memory",
    "Gated recurrent units",
    "Recurrent semicircle",
    "Cerebellar cortex",
    "Parallel fiber",
    "Purkinje cells",
    "Granule cells",
    "Vestibulo-ocular reflex",
    "Motor control",
    "Macy conferences",
    "Hebbian learning",
    "Spin glass",
    "Binary activation function",
    "Continuous activation functions",
    "Hebbian learning rule",
    "Associatron",
    "Ising Model",
    "Binary activation functions",
    "Description length minimization",
    "Perceptron",
    "Supervised learning",
    "Binary classifier",
    "Linear classifier",
    "Feature vector",
    "Weights",
    "Artificial neuron network",
    "Mark I Perceptron",
    "Alpha-perceptron",
    "S-units",
    "A-units",
    "R-units",
    "XOR function",
    "Kernel perceptron algorithm",
    "Margin bounds guarantees",
    "Image recognition",
    "Plugboard",
    "Potentiometers",
    "Neurodynamics",
    "Multilayer perceptron",
    "Single-layer perceptron",
    "Linearly separable patterns",
    "Margin bounds",
    "Decision boundaries",
    "Ferromagnetism",
    "Statistical mechanics",
    "Magnetic dipole moments",
    "Spins",
    "Lattice",
    "Phase transition",
    "Mean-field theory",
    "Cayley trees",
    "Branching ratio",
    "Neural networks",
    "Graph maximum cut (Max-Cut)",
    "Combinatorial optimization",
    "Translation-invariant",
    "Ferromagnetic",
    "Zero-field model",
    "d-dimensional lattice",
    "Correlations",
    "Peierls argument",
    "Correlation functions",
    "Free energy",
    "Spontaneous magnetization",
    "Fredholm determinants",
    "Correlation inequalities",
    "Spin correlations",
    "Magnetization",
    "Coupling constants",
    "Simon-Lieb inequality",
    "Critical temperatures",
    "Planar Potts model",
    "Percolation arguments",
    "Correlation",
    "Ordered phase",
    "Disordered phase",
    "Ferromagnetic order",
    "Ising spin correlations",
    "Percolation model",
    "Potts model",
    "Freezing temperature",
    "Ferromagnet",
    "Antiferromagnet",
    "Frustrated interactions",
    "Metastable",
    "Kondo effect",
    "Spin glass freezing temperature",
    "Replica method",
    "Edwards-Anderson order parameter",
    "Replica breaking",
    "Order parameters",
    "Mictomagnet",
    "Ferromagnetic interactions",
    "Antiferromagnetic interactions",
    "Magnetic frustration",
    "SK model",
    "Entropy",
    "Broken symmetry",
    "Replica breaking ansatz",
    "Ising model",
    "Recurrent Semicircles",
    "Spin Glass",
    "Glauber Dynamics",
    "Iterated nets",
    "Attention Mechanisms",
    "Sherrington\u2013Kirkpatrick model",
    "seq2seq",
    "attention mechanisms",
    "transformers",
    "encoder-decoder sequence transduction",
    "Automatizer",
    "Atkinson-Shiffrin memory model",
    "Feedforward structure",
    "Epilepsy",
    "Causalgia",
    "Seq2seq architecture",
    "RNN unfolded in time",
    "Encoder-decoder architecture",
    "Cognitive psychology",
    "Chunkers",
    "Sequential Data",
    "Recurrent Connections",
    "Recurrent Unit",
    "Hidden State",
    "Long Short-Term Memory (LSTM)",
    "Gated Recurrent Units (GRUs)",
    "Self-Attention Mechanisms",
    "Recurrent neural network",
    "Recurrent Neural Network",
    "Sequence to Sequence",
    "Feedforward Neural Networks",
    "Temporal Dependencies",
    "Elman network",
    "Cerebellar Cortex",
    "Purkinje Cells",
    "Granule Cells",
    "seq2seq architecture",
    "Neural History Compressor",
    "Unsupervised Stack of RNNs",
    "Automatic Differentiation",
    "Conscious Chunker",
    "Subconscious Automatizer",
    "Self-attention",
    "Jordan network",
    "Recurrent Neural Networks",
    "Long Short-Term Memory",
    "Gated Recurrent Units",
    "Binary classifiers",
    "Artificial Neuron Network",
    "Alpha-Perceptron",
    "Multilayer Perceptron",
    "Single-layer Perceptron",
    "Kernel Perceptron Algorithm",
    "Image Recognition",
    "Weight Updates",
    "Linearly Separable Patterns",
    "XOR Function",
    "Kernel Perceptron",
    "Margin Bounds",
    "Non-separable case",
    "Recurrent Neural Network (RNN)",
    "Hidden Markov Models",
    "Sequence Learning",
    "Cell",
    "Input Gate",
    "Output Gate",
    "Forget Gate",
    "Time Series Analysis",
    "Speech Recognition",
    "Machine Translation",
    "Constant Error Carousel (CEC)",
    "Input gate",
    "Output gate",
    "Forget gate",
    "Peephole connections",
    "Connectionist Temporal Classification (CTC)",
    "Bidirectional LSTM",
    "Gated recurrent unit (GRU)",
    "Highway network",
    "ResNet architecture",
    "mLSTM",
    "sLSTM",
    "Focused back-propagation",
    "Forget gate (Keep gate)",
    "Source Language",
    "Target Language",
    "High-level programming language",
    "Low-level programming language",
    "Assembly language",
    "Object code",
    "Machine code",
    "Executable program",
    "Cross-compiler",
    "Bootstrap compiler",
    "Decompiler",
    "Source-to-source compiler",
    "Transpiler",
    "Language rewriter",
    "Compiler-compiler",
    "Preprocessing",
    "Lexical analysis",
    "Parsing",
    "Semantic analysis",
    "Syntax-directed translation",
    "Intermediate representation",
    "Code optimization",
    "Code generation",
    "Assembly Language",
    "High-Level Language",
    "Backus-Naur Form (BNF)",
    "Syntax",
    "Semantics",
    "Algol 60",
    "BCPL",
    "BLISS",
    "C",
    "Portable C Compiler (PCC)",
    "Object-Oriented Programming (OOP)",
    "Compiler-Compiler",
    "Ada",
    "GNU Compiler Collection (GCC)",
    "Scripting Languages",
    "Interpreter",
    "Portable C Compiler",
    "High-level Programming Language",
    "Low-level Programming Language",
    "Object Code",
    "Machine Code",
    "Executable Program",
    "Bootstrap Compiler",
    "Source-to-source Compiler",
    "Language Rewriter",
    "Lexical Analysis",
    "Semantic Analysis",
    "Syntax-directed Translation",
    "Intermediate Representation",
    "Code Optimization",
    "Code Generation",
    "Assembly Languages",
    "High-level language",
    "Grammar",
    "Optimization",
    "Source Code",
    "Target Code",
    "Virtual Machine",
    "Object-Oriented Programming",
    "Shell Programs",
    "Ensemble learning",
    "Classification",
    "Regression",
    "Decision trees",
    "Random subspace method",
    "Stochastic discrimination",
    "Bagging",
    "Random selection of features",
    "Randomized decision tree algorithm",
    "Majority voting",
    "Oblique hyperplanes",
    "Feature dimensions",
    "Random subspace selection",
    "Randomized node optimization",
    "CART",
    "Generalization error",
    "Kernel Random Forest (KeRF)",
    "Adaptive Nearest Neighbor",
    "Centered Random Forest",
    "Uniform Random Forest",
    "Support Vector Machines (SVMs)",
    "Support Vector Networks",
    "Supervised Learning",
    "Max-Margin Models",
    "Regression Analysis",
    "Statistical Learning",
    "VC Theory",
    "Kernel Trick",
    "Kernel Function",
    "Feature Space",
    "Linear Classification",
    "Non-linear Classification",
    "Noisy Data",
    "Regression Tasks",
    "Support Vector Clustering",
    "Kernel trick",
    "Maximum-margin hyperplanes",
    "Soft margin",
    "Non-linear classification",
    "Unsupervised learning",
    "Maximum-margin hyperplane",
    "Generative adversarial network",
    "Generative artificial intelligence",
    "Generator",
    "Discriminator",
    "Zero-sum game",
    "Semi-supervised learning",
    "Fully supervised learning",
    "Reinforcement learning",
    "Artificial Curiosity",
    "Gradient descent",
    "Stochasticity",
    "Noise-contrastive estimation",
    "Loss function",
    "Adversarial Machine Learning",
    "Robust controllers",
    "Game theoretic",
    "Minimizer policy",
    "Maximizer policy",
    "Disturbance",
    "Image Enhancement",
    "Pixel-accuracy",
    "Realistic Textures",
    "Sherrington-Kirkpatrick Model",
    "Hopfield Network",
    "Jordan Network",
    "Elman Network",
    "Bidirectional Recurrent Neural Networks (BRNN)",
    "Encoder-Decoder Sequence Transduction",
    "Seq2Seq Architecture",
    "LSTM networks",
    "bidirectional LSTM architecture",
    "Description Length Minimization",
    "Cells",
    "Keep gate",
    "Backpropagation through time",
    "ResNet",
    "xLSTM",
    "Meta-learning",
    "Neural architecture search",
    "Policy gradients",
    "Protein homology detection",
    "Phoneme error rate",
    "Time-aware LSTM (T-LSTM)",
    "Forget gate (keep gate)",
    "Connectionist Temporal Classification",
    "Backpropagation Through Time",
    "Gated Recurrent Unit",
    "Highway Network",
    "Speech recognition",
    "Neuroevolution",
    "Handwriting recognition",
    "Time-aware LSTM",
    "Neural feedback loops",
    "Machine translation",
    "Language modeling",
    "Multilingual Language Processing",
    "Automatic image captioning",
    "Cycles (in networks)",
    "close-loop cross-coupled perceptrons",
    "closed-loop cross-coupled",
    "back-coupled perceptron networks",
    "statistical mechanics",
    "spin glass",
    "binary activation functions",
    "continuous activation functions",
    "Chunker",
    "Forget Gate LSTM",
    "Cycles in neural networks",
    "Image captioning",
    "Filter optimization",
    "Feature maps",
    "Shift invariant",
    "Space invariant",
    "Shared weights",
    "Receptive field",
    "Visual cortex",
    "Vision processing",
    "Neurons",
    "Visual stimuli",
    "Visual cell types",
    "Striate cortex",
    "Multilayer visual feature detection network",
    "Convolutional network",
    "ReLU (rectified linear unit)",
    "ReLU activation function",
    "Lateral inhibition",
    "Convolution",
    "Shift invariance",
    "Filter",
    "Spatio-Temporal Bipolar Patterns",
    "Phoneme Classification",
    "Phoneme Recognition",
    "Shift-invariance",
    "Weight sharing",
    "Spectrograms",
    "Time-invariantly",
    "Frequency shifts",
    "2-D CNN",
    "1-D CNN",
    "Back-propagation",
    "Shift-invariant pattern recognition neural network",
    "Max pooling",
    "TDNNs",
    "Cresceptron",
    "Spatial averaging",
    "Downsampling unit",
    "Convolutional Network",
    "Handwritten Digit Recognition",
    "Check reading systems",
    "Shift-invariant neural network",
    "Convolutional interconnection",
    "Electromyography",
    "De-convolution",
    "Graphics processing units (GPUs)",
    "GPGPU",
    "Deep belief networks",
    "Deep feedforward networks",
    "Thread-level parallelism",
    "SIMD-level parallelism",
    "Feedforward neural network",
    "Filter (or kernel) optimization",
    "Deep learning",
    "Convolution-based networks",
    "Computer vision",
    "Image processing",
    "Transformer",
    "Vanishing gradients",
    "Exploding gradients",
    "Convolution kernel",
    "Cross-correlation kernel",
    "Artificial neural networks",
    "Fully connected networks",
    "Overfitting",
    "Regularization",
    "Weight decay",
    "Dropout",
    "Biological processes",
    "Pre-processing",
    "Image classification algorithms",
    "shift invariance",
    "speech recognition",
    "Cascaded convolution",
    "Cross-correlation kernels",
    "Translation equivariance",
    "Animal visual cortex",
    "Deconvolutional networks",
    "Feature learning",
    "Neuron",
    "ReLU (rectified linear unit) activation function",
    "Phoneme recognition",
    "1-D convolutional neural net",
    "Hand-written ZIP Code numbers",
    "Kernel Coefficients",
    "Medical image object segmentation",
    "Breast cancer detection",
    "Mammograms",
    "Time Delay Neural Networks (TDNNs)",
    "Inhibition",
    "Saturation",
    "Hand-written number recognition",
    "Check reading system",
    "Image character recognition",
    "Convolutional interconnections",
    "Image feature layers",
    "Convolutional neural networks (CNNs)",
    "Intel Xeon Phi",
    "Cell State",
    "Policy Gradients",
    "Reinforcement Learning",
    "Neural Architecture Search",
    "Phoneme Error Rate",
    "Time-Aware LSTM (T-LSTM)",
    "Latent variable generative models",
    "Forward diffusion process",
    "Reverse sampling process",
    "Markov chains",
    "Noise conditioned score networks",
    "Stochastic differential equations",
    "Variational inference",
    "U-nets",
    "Text-encoders",
    "Cross-attention modules"
  ]
}