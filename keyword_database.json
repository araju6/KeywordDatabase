{
  "keywords": {
    "convolutional neural network": [
      {
        "title": "Backpropagation Applied to Handwritten Zip Code Recognition",
        "url": "https://doi.org/10.1162/neco.1989.1.4.541",
        "abstract": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.",
        "citations": 10898,
        "claim": "\"For example, for each neuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 \u00d7 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels, [5] only 25 weights for each convolutional layer are required to process 5x5-sized tiles.\" \u2013 *This sentence illustrates the efficiency of convolutional layers through shared weights and cascaded convolutions, which is a core characteristic of CNNs.* - [\"Backpropagation Applied to Handwritten Zip Code Recognition\"] :-: LeCun, Y.; Boser, B.; Denker, J. S.; Henderson, D.; Howard, R. E.; Hubbard, W.; Jackel, L. D. (1989)",
        "reasoning": "The paper's description of efficient image processing through cascaded convolutions and shared weights, fundamental characteristics of convolutional neural networks, directly supports its association with that keyword.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
        "url": "https://doi.org/10.1007/bf00344251",
        "abstract": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname \u201cneocognitron\u201d. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \u201cS-cells\u201d, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \u201cC-cells\u201d similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any \u201cteacher\u201d during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.",
        "citations": 5233,
        "claim": "\"Convolutional networks were inspired by biological processes [18] [19] [20] [21] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex.\" \u2013 *This sentence notes the biological inspiration for CNNs, linking them to the structure of the animal visual cortex.* - [\"Neocognitron\"] :-: Fukushima, K. (2007)",
        "reasoning": "The paper is associated with 'convolutional neural network' because it describes the \"Neocognitron,\" a precursor to modern CNNs, directly inspired by the animal visual cortex's organization, which the source material explicitly links to the biological inspiration behind convolutional networks.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Parallel distributed processing model with local space-invariant interconnections and its optical architecture",
        "url": "https://doi.org/10.1364/ao.29.004790",
        "abstract": "This paper proposes a parallel distributed processing model with local space-invariant interconnections, which is more readily implemented by optics and is able to classify patterns correctly, even if they have been shifted or distorted. Error backpropagation is used as a training algorithm. Computer simulation results presented indicate that the processing is effective and the network can deal with the shifted or distorted patterns. Moreover, the optical implementation architecture using matched filters for the model is discussed.",
        "citations": 201,
        "claim": "\"The \" neocognitron \" [ 18 ] was introduced by Fukushima in 1980.\" \u2013 *This claim directly states the introduction of the Neocognitron and references a paper associated with it.* - [\"Parallel distributed processing model with local space-invariant interconnections and its optical architecture\"] :-: Zhang (1990)",
        "reasoning": "The paper is associated with 'convolutional neural network' because it discusses the Neocognitron, an early model cited as a precursor to convolutional neural networks.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements",
        "url": "https://doi.org/10.1109/tssc.1969.300225",
        "abstract": "A new type of visual feature extracting network has been synthesized, and the response of the network has been simulated on a digital computer. This research has been done as a first step towards the realization of a recognizer of handwritten characters. The design of the network was suggested by biological systems, especially, the visual systems of cat and monkey. The network is composed of analog threshold elements connected in layers. Each analog threshold element receives inputs from a large number of elements in the neighbouring layers and performs its own special functions. It takes care of one restricted part of the photoreceptor layer, on which an input pattem is presented, and it responds to one particular feature of the input pattem, such as brightness contrast, a dot in the pattern, a line segment of a particular orientation, or an end of the line. This means that the network performs parallel processing of the information. With the propagation of the information through the layered network, the input pattern is successively decomposed into dots, groups of line segments of the same orientation, and the ends of these line segments.",
        "citations": 230,
        "claim": "\"Fukushima's ReLU activation function was not used in his neocognitron since all the weights were nonnegative; lateral inhibition was used instead.\" - *This claim, while discussing the ReLU activation function, implies that the neocognitron and its properties are described in Fukushima's earlier work.* - [\"Visual feature extraction by a multilayered network of analog threshold elements\"] :-: Fukushima (1969)",
        "reasoning": "The paper is associated with 'convolutional neural network' because it describes Fukushima's neocognitron, a precursor to modern CNNs, as evidenced by the reference to \"Visual feature extraction by a multilayered network of analog threshold elements\" which details its architecture and functionality.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Phoneme recognition using time-delay neural networks",
        "url": "https://doi.org/10.1109/29.21701",
        "abstract": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
        "citations": 2579,
        "claim": "\"The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was an early convolutional network exhibiting shift-invariance.\" \u2013 *This sentence explicitly states that TDNNs were an early form of CNN and also highlights a key characteristic, shift-invariance.* - [\"Phoneme Recognition Using Time-Delay Neural Networks\"] :-: Alex Waibel (1987)",
        "reasoning": "The paper's identification of Time-Delay Neural Networks (TDNNs) for phoneme recognition, as an early form of convolutional neural network exhibiting shift-invariance, directly aligns it with the 'convolutional neural network' keyword.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Deep learning",
        "url": "https://doi.org/10.1038/nature14539",
        "abstract": "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.",
        "citations": 69457,
        "claim": "\"It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation.\" \u2013 *This sentence claims TDNN was the first CNN to utilize weight sharing with gradient descent and backpropagation, important concepts in CNNs.* - [\"Deep learning\"] :-: Yann LeCun, Yoshua Bengio, Geoffrey Hinton (2015)",
        "reasoning": "The paper is associated with 'convolutional neural network' because it details the first CNN utilizing weight sharing with gradient descent and backpropagation, key concepts within the field of convolutional neural networks.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Learning Internal Representations by Error Propagation",
        "url": "https://doi.org/10.21236/ada164453",
        "abstract": "This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion",
        "citations": 16342,
        "claim": "\"Rumelhart, David E. ; Hinton, Geoffrey E. ; Williams, Ronald J. (1986b). \\\"8. Learning Internal Representations by Error Propagation\\\" . In Rumelhart, David E. ; McClelland, James L. (eds.). Parallel Distributed Processing\u00a0: Explorations in the Microstructure of Cognition . Vol.\u00a01\u00a0: Foundations. Cambridge: MIT Press.\" - *This identifies a chapter by Rumelhart, Hinton, and Williams in Parallel Distributed Processing that is also highly relevant to the development of backpropagation.* - [\"Learning Internal Representations by Error Propagation\"] :-: Rumelhart, David E. ; Hinton, Geoffrey E. ; Williams, Ronald J. (1986)",
        "reasoning": "The paper on learning internal representations by error propagation is associated with 'convolutional neural network' because it is foundational to backpropagation, a core training algorithm frequently used in convolutional neural networks.",
        "parent_keyword": null,
        "child_claim": "\"Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections.\" \u2013 *This sentence describes a key advantage of CNNs compared to earlier neural networks, which is directly related to their architecture and training.* - [XXX] :-: XXX (XXX)",
        "child_keyword": "backpropagation"
      },
      {
        "title": "Cognitron: A self-organizing multilayered neural network",
        "url": "https://doi.org/10.1007/bf00342633",
        "abstract": "A new hypothesis for the organization of synapses between neurons is proposed: \u201cThe synapse from neuron x to neuron y is reinforced when x fires provided that no neuron in the vicinity of y is firing stronger than y \u201d. By introducing this hypothesis, a new algorithm with which a multilayered neural network is effectively organized can be deduced. A self-organizing multilayered neural network, which is named \u201ccognitron\u201d, is constructed following this algorithm, and is simulated on a digital computer. Unlike the organization of a usual brain models such as a three-layered perceptron, the self-organization of a cognitron progresses favorably without having a \u201cteacher\u201d which instructs in all particulars how the individual cells respond. After repetitive presentations of several stimulus patterns, the cognitron is self-organized in such a way that the receptive fields of the cells become relatively larger in a deeper layer. Each cell in the final layer integrates the information from whole parts of the first layer and selectively responds to a specific stimulus pattern or a feature.",
        "citations": 703,
        "claim": "\"In 1975 he improved it to the Cognitron\" \u2013 *Claims a paper exists for the Cognitron architecture* - [\"Cognitron: A self-organizing multilayered neural network\"] :-: Kunihiko Fukushima (1975)",
        "reasoning": "The paper is associated with 'convolutional neural network' because it introduces the Cognitron architecture, which is recognized as an early precursor and a key inspiration in the development of convolutional neural networks.",
        "parent_keyword": null,
        "child_claim": "\"It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer.\" \u2013 *This describes the architecture of the shift-invariant neural network as a modification of the Neocognitron.* - XXX - [XXX] :-: XXX (XXX)",
        "child_keyword": "neocognitron"
      }
    ],
    "backpropagation": [
      {
        "title": "Applied Optimal Control",
        "url": "https://doi.org/10.1201/9781315137667",
        "abstract": "No abstract found",
        "citations": 5996,
        "claim": "\"Hecht-Nielsen [ 19 ] credits the Robbins\u2013Monro algorithm (1951) [ 20 ] and Arthur Bryson and Yu-Chi Ho 's Applied Optimal Control (1969) as presages of backpropagation.\" \u2013 *This sentence attributes the Robbins-Monro algorithm and Bryson and Ho's book as precursors to backpropagation.* - [\"Applied Optimal Control\"] :-: Arthur Bryson and Yu-Chi Ho (1969)",
        "reasoning": "This paper is associated with 'backpropagation' because it cites \"Applied Optimal Control\" by Arthur Bryson and Yu-Chi Ho (1969) as a precursor to backpropagation, a key training method relevant to convolutional neural networks as 'backpropagation' is a child keyword of 'convolutional neural network' based on the claim that CNNs address vanishing/exploding gradients seen during backpropagation in earlier neural networks.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections.\" \u2013 *This sentence describes a key advantage of CNNs compared to earlier neural networks, which is directly related to their architecture and training.* - [XXX] :-: XXX (XXX)",
        "child_keyword": "backpropagation"
      },
      {
        "title": "Optimal shape design, reverse mode of automatic differentiation and turbulence",
        "url": "https://doi.org/10.2514/6.1997-99",
        "abstract": "No abstract found",
        "citations": 64,
        "claim": "\"Modern backpropagation was first published by Seppo Linnainmaa as \"reverse mode of automatic differentiation \" (1970)\" \u2013 *Direct claim associating Linnainmaa with a foundational paper.* - [\"reverse mode of automatic differentiation\"] :-: Seppo Linnainmaa (1970)",
        "reasoning": "The paper is associated with 'backpropagation' because it's the foundational \"reverse mode of automatic differentiation\" paper by Seppo Linnainmaa, a concept directly linked to 'backpropagation', which itself is a child keyword of 'convolutional neural network' due to its role in training CNNs and overcoming issues like vanishing gradients.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections.\" \u2013 *This sentence describes a key advantage of CNNs compared to earlier neural networks, which is directly related to their architecture and training.* - [XXX] :-: XXX (XXX)",
        "child_keyword": "backpropagation"
      },
      {
        "title": "Applications of advances in nonlinear sensitivity analysis",
        "url": "https://doi.org/10.1007/bfb0006203",
        "abstract": "The following paper summarizes the major properties and applications of a collection of algorithms involving differentiation and optimization at minimum cost. The areas of application include the sensitivity analysis of models, new work in statistical or econometric estimation, optimization, artificial intelligence and neuron modelling. The details, references and derivations can be obtained by requesting \u201eSensitivity Analysis Methods for Nonlinear Systems\u201c from Forecast Analysis and Evaluation Team, Quality Assurance, OSS/EIA, Room 7413, Department of Energy, Washington, DC 20461.",
        "citations": 253,
        "claim": "\"In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard.\" \u2013 *Direct claim associating Werbos with backpropagation on MLPs.* - [\"Applications of advances in nonlinear sensitivity analysis\"] :-: Paul Werbos (1982)",
        "reasoning": "This paper is associated with 'backpropagation' because it details Paul Werbos's application of backpropagation to MLPs, a technique relevant to the broader context of 'convolutional neural networks' as 'backpropagation' is a child keyword stemming from the advantages of CNNs over previous networks trained with backpropagation.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections.\" \u2013 *This sentence describes a key advantage of CNNs compared to earlier neural networks, which is directly related to their architecture and training.* - [XXX] :-: XXX (XXX)",
        "child_keyword": "backpropagation"
      },
      {
        "title": "Learning representations by back-propagating errors",
        "url": "https://doi.org/10.1038/323533a0",
        "abstract": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1 .",
        "citations": 26932,
        "claim": "\"Rumelhart; Hinton; Williams (1986). \\\"Learning representations by back-propagating errors\\\" (PDF) . Nature . 323 (6088): 533\u2013 536.\" \u2013 *This identifies a paper by Rumelhart, Hinton, and Williams in Nature that is highly relevant to the development of backpropagation.* - [\"Learning representations by back-propagating errors\"] :-: Rumelhart; Hinton; Williams (1986)",
        "reasoning": "This paper is associated with 'backpropagation' because it is the seminal work by Rumelhart, Hinton, and Williams directly cited in the claim detailing the development of backpropagation, a concept derived from the advantages of convolutional neural networks in preventing vanishing/exploding gradients, as stated in its parent keyword description.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections.\" \u2013 *This sentence describes a key advantage of CNNs compared to earlier neural networks, which is directly related to their architecture and training.* - [XXX] :-: XXX (XXX)",
        "child_keyword": "backpropagation"
      },
      {
        "title": "Learning Internal Representations by Error Propagation",
        "url": "https://doi.org/10.21236/ada164453",
        "abstract": "This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion",
        "citations": 16342,
        "claim": "\"Rumelhart, David E. ; Hinton, Geoffrey E. ; Williams, Ronald J. (1986b). \\\"8. Learning Internal Representations by Error Propagation\\\" . In Rumelhart, David E. ; McClelland, James L. (eds.). Parallel Distributed Processing\u00a0: Explorations in the Microstructure of Cognition . Vol.\u00a01\u00a0: Foundations. Cambridge: MIT Press.\" - *This identifies a chapter by Rumelhart, Hinton, and Williams in Parallel Distributed Processing that is also highly relevant to the development of backpropagation.* - [\"Learning Internal Representations by Error Propagation\"] :-: Rumelhart, David E. ; Hinton, Geoffrey E. ; Williams, Ronald J. (1986)",
        "reasoning": "This research paper is associated with 'backpropagation' because it is the foundational work by Rumelhart, Hinton, and Williams on error propagation, a key concept linked to convolutional neural networks as a solution to vanishing/exploding gradients (which 'backpropagation' is a child of), and directly addresses the development of the backpropagation algorithm itself.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections.\" \u2013 *This sentence describes a key advantage of CNNs compared to earlier neural networks, which is directly related to their architecture and training.* - [XXX] :-: XXX (XXX)",
        "child_keyword": "backpropagation"
      }
    ],
    "cascading model": [
      {
        "title": "Robust Real-Time Face Detection",
        "url": "https://doi.org/10.1023/b:visi.0000013087.49260.fb",
        "abstract": "This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second.",
        "citations": 14156,
        "claim": "\"The first cascading classifier was the face detector of Viola and Jones (2001).\" \u2013 *This sentence directly claims the first cascading classifier was the Viola and Jones face detector.* - [\"Robust Real-Time Face Detection\"] :-: Paul Viola and Michael Jones (2001)",
        "reasoning": "The paper is associated with 'cascading model' because it describes the first cascading classifier, as claimed in the paper, and 'cascading model' is a child keyword of 'convolutional neural network' due to Hubel and Wiesel's proposal of a cascading model for pattern recognition within the context of convolutional neural networks.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.\" \u2013 *This mentions that Hubel and Wiesel proposed a cascading model, which is relevant to convolutional neural networks.* - [XXX] :-: Hubel and Wiesel (XXX)",
        "child_keyword": "cascading model"
      }
    ],
    "relu activation function": [],
    "invariant neural network": [
      {
        "title": "Receptive fields of single neurones in the cat's striate cortex",
        "url": "https://doi.org/10.1113/jphysiol.1959.sp006308",
        "abstract": "No abstract found",
        "citations": 4459,
        "claim": "\"In 1969, Kunihiko Fukushima introduced a multilayer visual feature detection network, inspired by the above-mentioned work of Hubel and Wiesel, in which \"All the elements in one layer have the same set of interconnecting coefficients; the arrangement of the elements and their interconnections are all homogeneous over a given layer.\" \" \u2013 *This sentence describes Fukushima's network and its inspiration, which is relevant to the development of convolutional networks.* - [\"Receptive fields of single neurones in the cat's striate cortex\"] :-: Hubel, DH; Wiesel, TN (1959)",
        "reasoning": "This paper is associated with 'invariant neural network', a child keyword of 'convolutional neural network', because the original claim about Fukushima's network relates to the development of convolutional networks, and 'invariant neural network' was derived from 'convolutional neural network' based on the identification of an early shift-invariant neural network.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"A shift-invariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988.\" \u2013 *This identifies an early shift-invariant neural network for image character recognition.* - XXX - [XXX] :-: Wei Zhang et al. (1988)",
        "child_keyword": "invariant neural network"
      },
      {
        "title": "Parallel distributed processing model with local space-invariant interconnections and its optical architecture",
        "url": "https://doi.org/10.1364/ao.29.004790",
        "abstract": "This paper proposes a parallel distributed processing model with local space-invariant interconnections, which is more readily implemented by optics and is able to classify patterns correctly, even if they have been shifted or distorted. Error backpropagation is used as a training algorithm. Computer simulation results presented indicate that the processing is effective and the network can deal with the shifted or distorted patterns. Moreover, the optical implementation architecture using matched filters for the model is discussed.",
        "citations": 201,
        "claim": "\"Zhang, Wei (1990). \"Parallel distributed processing model with local space-invariant interconnections and its optical architecture\"\" \u2013 *This is the full reference for reference 18 in the above claim.* - <[18] - [\"Parallel distributed processing model with local space-invariant interconnections and its optical architecture\"] :-: Zhang (1990)>.",
        "reasoning": "This paper is associated with 'invariant neural network' because it is the full reference for an early shift-invariant neural network proposed by Wei Zhang et al. for image character recognition, a specific example that led to the derivation of 'invariant neural network' as a child keyword of 'convolutional neural network'.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"A shift-invariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988.\" \u2013 *This identifies an early shift-invariant neural network for image character recognition.* - XXX - [XXX] :-: Wei Zhang et al. (1988)",
        "child_keyword": "invariant neural network"
      },
      {
        "title": "Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements",
        "url": "https://doi.org/10.1109/tssc.1969.300225",
        "abstract": "A new type of visual feature extracting network has been synthesized, and the response of the network has been simulated on a digital computer. This research has been done as a first step towards the realization of a recognizer of handwritten characters. The design of the network was suggested by biological systems, especially, the visual systems of cat and monkey. The network is composed of analog threshold elements connected in layers. Each analog threshold element receives inputs from a large number of elements in the neighbouring layers and performs its own special functions. It takes care of one restricted part of the photoreceptor layer, on which an input pattem is presented, and it responds to one particular feature of the input pattem, such as brightness contrast, a dot in the pattern, a line segment of a particular orientation, or an end of the line. This means that the network performs parallel processing of the information. With the propagation of the information through the layered network, the input pattern is successively decomposed into dots, groups of line segments of the same orientation, and the ends of these line segments.",
        "citations": 230,
        "claim": "\"Fukushima, K. (1969). \"Visual feature extraction by a multilayered network of analog threshold elements\"\" \u2013 *Claims that the rectifier has become popular with CNNs and deep neural networks. Cites the paper in which Fukushima introduced a multilayered network.* - <[42] - [\"Visual feature extraction by a multilayered network of analog threshold elements\"] :-: Fukushima (1969)>.",
        "reasoning": "The paper on visual feature extraction by Fukushima is associated with 'invariant neural network', a child of 'convolutional neural network', because Fukushima introduced a multilayered network that served as an early inspiration for convolutional neural networks, which later developed into shift-invariant networks like the one proposed by Wei Zhang et al. also under this keyword.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"A shift-invariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988.\" \u2013 *This identifies an early shift-invariant neural network for image character recognition.* - XXX - [XXX] :-: Wei Zhang et al. (1988)",
        "child_keyword": "invariant neural network"
      }
    ],
    "neocognitron": [
      {
        "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
        "url": "https://doi.org/10.1007/bf00344251",
        "abstract": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname \u201cneocognitron\u201d. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \u201cS-cells\u201d, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \u201cC-cells\u201d similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any \u201cteacher\u201d during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.",
        "citations": 5233,
        "claim": "\"The neocognitron is a hierarchical, multilayered artificial neural network proposed by Kunihiko Fukushima in 1979.\" \u2013 *States the foundational paper for the neocognitron.* - [\"Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position\"] :-: Kunihiko Fukushima (1980)",
        "reasoning": "The paper is associated with 'neocognitron' because it is the foundational paper proposing the original Neocognitron model, a concept directly derived from the parent keyword 'convolutional neural network' based on architectural modifications and shift-invariant properties.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer.\" \u2013 *This describes the architecture of the shift-invariant neural network as a modification of the Neocognitron.* - XXX - [XXX] :-: XXX (XXX)",
        "child_keyword": "neocognitron"
      },
      {
        "title": "Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements",
        "url": "https://doi.org/10.1109/tssc.1969.300225",
        "abstract": "A new type of visual feature extracting network has been synthesized, and the response of the network has been simulated on a digital computer. This research has been done as a first step towards the realization of a recognizer of handwritten characters. The design of the network was suggested by biological systems, especially, the visual systems of cat and monkey. The network is composed of analog threshold elements connected in layers. Each analog threshold element receives inputs from a large number of elements in the neighbouring layers and performs its own special functions. It takes care of one restricted part of the photoreceptor layer, on which an input pattem is presented, and it responds to one particular feature of the input pattem, such as brightness contrast, a dot in the pattern, a line segment of a particular orientation, or an end of the line. This means that the network performs parallel processing of the information. With the propagation of the information through the layered network, the input pattern is successively decomposed into dots, groups of line segments of the same orientation, and the ends of these line segments.",
        "citations": 230,
        "claim": "\"Previously in 1969, he published a similar architecture, but with hand-designed kernels inspired by convolutions in mammalian vision.\" \u2013 *Claims the existence of a previous paper with a similar architecture.* - [\"Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements\"] :-: Kunihiko Fukushima (1969)",
        "reasoning": "This paper is associated with 'neocognitron' because it's the \"similar architecture\" claimed in the paper's description, and 'neocognitron' is a child of 'convolutional neural network', derived from the claim that shift-invariant neural networks are modified Neocognitrons.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer.\" \u2013 *This describes the architecture of the shift-invariant neural network as a modification of the Neocognitron.* - XXX - [XXX] :-: XXX (XXX)",
        "child_keyword": "neocognitron"
      },
      {
        "title": "Cognitron: A self-organizing multilayered neural network",
        "url": "https://doi.org/10.1007/bf00342633",
        "abstract": "A new hypothesis for the organization of synapses between neurons is proposed: \u201cThe synapse from neuron x to neuron y is reinforced when x fires provided that no neuron in the vicinity of y is firing stronger than y \u201d. By introducing this hypothesis, a new algorithm with which a multilayered neural network is effectively organized can be deduced. A self-organizing multilayered neural network, which is named \u201ccognitron\u201d, is constructed following this algorithm, and is simulated on a digital computer. Unlike the organization of a usual brain models such as a three-layered perceptron, the self-organization of a cognitron progresses favorably without having a \u201cteacher\u201d which instructs in all particulars how the individual cells respond. After repetitive presentations of several stimulus patterns, the cognitron is self-organized in such a way that the receptive fields of the cells become relatively larger in a deeper layer. Each cell in the final layer integrates the information from whole parts of the first layer and selectively responds to a specific stimulus pattern or a feature.",
        "citations": 703,
        "claim": "\"In 1975 he improved it to the Cognitron\" \u2013 *Claims a paper exists for the Cognitron architecture* - [\"Cognitron: A self-organizing multilayered neural network\"] :-: Kunihiko Fukushima (1975)",
        "reasoning": "The paper is associated with 'neocognitron' because it's a child keyword of 'convolutional neural network,' derived from a claim that the paper describes a modification of the Neocognitron architecture, a successor of the earlier Cognitron architecture.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer.\" \u2013 *This describes the architecture of the shift-invariant neural network as a modification of the Neocognitron.* - XXX - [XXX] :-: XXX (XXX)",
        "child_keyword": "neocognitron"
      },
      {
        "title": "Receptive fields of single neurones in the cat's striate cortex",
        "url": "https://doi.org/10.1113/jphysiol.1959.sp006308",
        "abstract": "No abstract found",
        "citations": 4459,
        "claim": "\"The neocognitron was inspired by the model proposed by Hubel & Wiesel in 1959. They found two types of cells in the visual primary cortex called simple cell and complex cell , and also proposed a cascading model of these two types of cells for use in pattern recognition tasks.\" \u2013 *States the inspiration of the Neocognitron as a paper by Hubel & Wiesel* - [\"Receptive fields of single neurones in the cat's striate cortex\"] :-: DH Hubel and TN Wiesel (1959)",
        "reasoning": "This paper is associated with the keyword 'neocognitron' because it discusses the inspiration for the Neocognitron, a concept directly linked to 'convolutional neural network' as a child keyword through its architectural modification based on the Neocognitron.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer.\" \u2013 *This describes the architecture of the shift-invariant neural network as a modification of the Neocognitron.* - XXX - [XXX] :-: XXX (XXX)",
        "child_keyword": "neocognitron"
      }
    ]
  },
  "remaining_to_process": [
    "filter optimization",
    "shared weights",
    "convolution kernel",
    "feature maps",
    "receptive field",
    "visual cortex",
    "vanishing gradients",
    "exploding gradients",
    "regularization",
    "vision processing",
    "visual cell types",
    "pattern recognition",
    "visual field",
    "multilayer visual feature detection network",
    "interconnecting coefficients",
    "homogeneous arrangement of elements",
    "relu (rectified linear unit) activation function",
    "lateral inhibition",
    "signal processing",
    "filter",
    "speech recognition",
    "correlation",
    "time delay neural network (tdnn)",
    "shift-invariance",
    "weight sharing",
    "gradient descent",
    "pyramidal structure",
    "local optimization",
    "spectrograms",
    "phoneme recognition",
    "2-d cnn",
    "hand-written zip code numbers",
    "1-d cnn",
    "back-propagation",
    "convolution kernel coefficients",
    "hand-written numbers",
    "alphabets recognition",
    "shift-invariant pattern recognition neural network",
    "medical image object segmentation",
    "breast cancer detection",
    "mammograms",
    "max pooling",
    "cresceptron",
    "spatial averaging",
    "inhibition",
    "saturation",
    "downsampling unit",
    "patch",
    "tdnns (time delay neural networks)",
    "lenet-5",
    "hand-written number classification",
    "check reading systems",
    "shift-invariant neural network",
    "convolutional interconnections",
    "image feature layers",
    "electromyography",
    "de-convolution",
    "graphics processing unit (gpu)",
    "convolutional neural network (cnn)",
    "deep feedforward networks",
    "deep belief networks",
    "gpgpu",
    "image recognition",
    "alexnet",
    "imagenet large scale visual recognition challenge",
    "thread-level parallelism",
    "simd-level parallelism",
    "intel xeon phi",
    "gradient computation",
    "neural network training",
    "chain rule",
    "loss function",
    "weights",
    "stochastic gradient descent",
    "adaptive moment estimation",
    "reverse mode of automatic differentiation",
    "reverse accumulation",
    "optimal control theory",
    "adjoint state method",
    "robbins-monro algorithm",
    "squared error loss",
    "multilayer perceptron (mlp)",
    "jacobian matrix",
    "mlps",
    "dynamic model estimation",
    "nationalism prediction",
    "social communications prediction",
    "local minimum",
    "global minimum",
    "nettalk",
    "alvinn",
    "lenet",
    "td-gammon",
    "boltzmann machine",
    "reinforcement learning",
    "machine vision",
    "natural language processing",
    "language structure learning",
    "event-related potential",
    "photonic processor",
    "robbins\u2013monro algorithm",
    "ensemble learning",
    "classifiers",
    "voting ensembles",
    "stacking ensembles",
    "multiexpert systems",
    "multistage",
    "positive samples",
    "negative images",
    "object detection",
    "image processing",
    "facial detection",
    "facial recognition",
    "rectified linear unit",
    "activation function",
    "artificial neural networks",
    "deep neural nets",
    "computer vision",
    "computational neuroscience",
    "ramp function",
    "half-wave rectification",
    "softplus activation function",
    "rectification",
    "biological neural networks",
    "visual feature extraction",
    "hierarchical neural networks",
    "convolutional neural networks (cnns)",
    "object recognition",
    "average pooling",
    "intensity equivariance",
    "restricted boltzmann machines",
    "sparse representation",
    "unsupervised pre-training",
    "deep networks",
    "functional analysis",
    "cross-correlation",
    "deconvolution",
    "integral",
    "shift",
    "commutativity",
    "probability",
    "statistics",
    "acoustics",
    "spectroscopy",
    "geophysics",
    "engineering",
    "physics",
    "differential equations",
    "euclidean space",
    "periodic functions",
    "discrete-time fourier transform",
    "discrete convolution",
    "numerical analysis",
    "numerical linear algebra",
    "finite impulse response filters",
    "impulse response",
    "linear time-invariant system",
    "shift invariant system",
    "time-invariant system",
    "operations research",
    "applied mathematics",
    "global maximum",
    "minimization problem",
    "non-linear function",
    "non-convex function",
    "local maximum",
    "plant propagation",
    "seeds",
    "cuttings",
    "plant parts",
    "ripening",
    "dispersal",
    "detachment",
    "pruning",
    "horticulture",
    "agriculture",
    "plant breeding",
    "forest crops",
    "fibre crops",
    "herbal medicine",
    "traditional medicine",
    "vegetative parts",
    "asexually-reproducing plants",
    "feedforward neural network",
    "filter (kernel) optimization",
    "deep learning",
    "fully-connected layer",
    "space invariance",
    "translation equivariance",
    "overfitting",
    "weight decay",
    "dropout",
    "biological processes",
    "image classification",
    "automated learning",
    "brain",
    "neurons",
    "visual stimuli",
    "firing",
    "contralateral visual field",
    "convolutional network",
    "cnn architecture",
    "supervised learning algorithms",
    "unsupervised learning algorithms",
    "convolution in time",
    "temporal dimension",
    "frequency shifts",
    "2-d cnn system",
    "kernel coefficients",
    "breast cancer detection in mammograms",
    "tdnns",
    "speaker-independent isolated word recognition system",
    "handwritten digit classification",
    "image character recognition",
    "fully connected layer",
    "generalization ability",
    "medical image segmentation",
    "convolution-based design",
    "convolved signals",
    "graphics processing units (gpus)",
    "relu activation function",
    "convolution",
    "shift invariance",
    "hierarchical artificial neural network",
    "handwritten character recognition",
    "convolutional neural networks",
    "hand-designed kernels",
    "convolutions in mammalian vision",
    "cognitron",
    "unsupervised learning",
    "self-organized learning",
    "visual primary cortex",
    "simple cell",
    "complex cell",
    "s-cells",
    "c-cells",
    "local feature extraction",
    "local feature integration",
    "selective attention",
    "news organization",
    "24-hour news coverage",
    "cable news channel",
    "viewership",
    "programming",
    "bias (left-wing, pro-israeli)",
    "editorial policy",
    "cable news network",
    "newscast",
    "cnn airport",
    "cnn newsource",
    "bureaus",
    "local stations",
    "regional networks",
    "foreign-language networks"
  ]
}