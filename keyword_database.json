{
  "keywords": {
    "recurrent neural network": [
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83296,
        "reasoning": "\"Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.\" \u2013 *Claims the invention of LSTMs* - [\"Long Short Term Memory\"] :-: Sepp Hochreiter ; J\u00fcrgen Schmidhuber (1995)",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Bidirectional recurrent neural networks",
        "url": "https://doi.org/10.1109/78.650093",
        "abstract": "In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported.",
        "citations": 8861,
        "reasoning": "\"Bidirectional recurrent neural networks (BRNN) uses two RNN that processes the same input in opposite directions.\" \u2013 *Introduces BRNNs.* - [\"Bidirectional recurrent neural networks\"] :-: Schuster, Mike, and Kuldip K. Paliwal (1997)",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling",
        "url": "https://doi.org/10.21437/interspeech.2014-80",
        "abstract": "Long Short-Term Memory (LSTM) is a specific recurrent neural network (RNN) architecture that was designed to model temporal sequences and their long-range dependencies more accurately than conventional RNNs. In this paper, we explore LSTM RNN architectures for large scale acoustic modeling in speech recognition. We recently showed that LSTM RNNs are more effective than DNNs and conventional RNNs for acoustic modeling, considering moderately-sized models trained on a single machine. Here, we introduce the first distributed training of LSTM RNNs using asynchronous stochastic gradient descent optimization on a large cluster of machines. We show that a two-layer deep LSTM RNN where each LSTM layer has a linear recurrent projection layer can exceed state-of-the-art speech recognition performance. This architecture makes more effective use of model parameters than the others considered, converges quickly, and outperforms a deep feed forward neural network having an order of magnitude more parameters. Index Terms: Long Short-Term Memory, LSTM, recurrent neural network, RNN, speech recognition, acoustic modeling.",
        "citations": 2826,
        "reasoning": "\"Long Short-Term Memory recurrent neural network architectures for large scale acoustic modeling\" \u2013 *Paper about LSTM RNN for acoustic modeling* - [\"Long Short-Term Memory recurrent neural network architectures for large scale acoustic modeling\"] :-: Sak, Ha\u015fim; Senior, Andrew; Beaufays, Fran\u00e7oise (2014)",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Neural networks and physical systems with emergent collective computational abilities.",
        "url": "https://doi.org/10.1073/pnas.79.8.2554",
        "abstract": "Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.",
        "citations": 18608,
        "reasoning": "\"In the 1982 paper, Hopfield applied this recently developed theory to study the Hopfield network with binary activation functions.\" \u2013 *Hopfield's work on neural networks with binary activation functions.* - <Paper in the claim - [\"Neural networks and physical systems with emergent collective computational abilities\"] :-: J. J. Hopfield (1982)>.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Principles of Neurodynamics.",
        "url": "https://doi.org/10.2307/2312103",
        "abstract": "No abstract found",
        "citations": 2302,
        "reasoning": "\"Later, in Principles of Neurodynamics (1961), he described \"closed-loop cross-coupled\" and \"back-coupled\" perceptron networks, and made theoretical and experimental studies for Hebbian learning in these networks\" \u2013 *This sentence continues to describe Rosenblatt's early work on perceptrons with recurrent connections and Hebbian learning.* - <Paper in the claim - [\"Principles of Neurodynamics\"] :-: Frank Rosenblatt (1961)>.",
        "parent_keyword": null,
        "child_claim": "\"Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology .\" \u2013 *Identifies two influential early RNN architectures.* - [\"Jordan network\"] :-: XXX (1986)",
        "child_keyword": "rnn"
      }
    ],
    "rnn": [
      {
        "title": "Principles of Neurodynamics.",
        "url": "https://doi.org/10.2307/2312103",
        "abstract": "No abstract found",
        "citations": 2302,
        "reasoning": "\"Later, in Principles of Neurodynamics (1961), he described \"closed-loop cross-coupled\" and \"back-coupled\" perceptron networks, and made theoretical and experimental studies for Hebbian learning in these networks\" \u2013 *This sentence continues to describe Rosenblatt's early work on perceptrons with recurrent connections and Hebbian learning.* - <Paper in the claim - [\"Principles of Neurodynamics\"] :-: Frank Rosenblatt (1961)>.",
        "parent_keyword": "recurrent neural network",
        "child_claim": "\"Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology .\" \u2013 *Identifies two influential early RNN architectures.* - [\"Jordan network\"] :-: XXX (1986)",
        "child_keyword": "rnn"
      },
      {
        "title": "Neural networks and physical systems with emergent collective computational abilities.",
        "url": "https://doi.org/10.1073/pnas.79.8.2554",
        "abstract": "Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.",
        "citations": 18608,
        "reasoning": "\"In the 1982 paper, Hopfield applied this recently developed theory to study the Hopfield network with binary activation functions.\" \u2013 *This sentence describes Hopfield's application of the SK model to neural networks.* - <Paper in the claim - [\"Neural networks and physical systems with emergent collective computational abilities\"] :-: J. J. Hopfield (1982)>.",
        "parent_keyword": "recurrent neural network",
        "child_claim": "\"Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology .\" \u2013 *Identifies two influential early RNN architectures.* - [\"Jordan network\"] :-: XXX (1986)",
        "child_keyword": "rnn"
      },
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83296,
        "reasoning": "\"Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.\" \u2013 *States the inventors and invention year of LSTM networks.* - [36] - [\"Long Short-Term Memory\"] :-: Hochreiter, Sepp ; Schmidhuber, J\u00fcrgen (1997)",
        "parent_keyword": "recurrent neural network",
        "child_claim": "\"Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology .\" \u2013 *Identifies two influential early RNN architectures.* - [\"Jordan network\"] :-: XXX (1986)",
        "child_keyword": "rnn"
      },
      {
        "title": "Bidirectional recurrent neural networks",
        "url": "https://doi.org/10.1109/78.650093",
        "abstract": "In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported.",
        "citations": 8861,
        "reasoning": "\"Bidirectional recurrent neural networks (BRNN) uses two RNN that processes the same input in opposite directions.\" - *States the definition of Bidirectional recurrent neural networks (BRNN)* - [37] - [\"Bidirectional recurrent neural networks\"] :-: Schuster, Mike, and Kuldip K. Paliwal (1997)",
        "parent_keyword": "recurrent neural network",
        "child_claim": "\"Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology .\" \u2013 *Identifies two influential early RNN architectures.* - [\"Jordan network\"] :-: XXX (1986)",
        "child_keyword": "rnn"
      }
    ],
    "convolutional neural network": [
      {
        "title": "Parallel distributed processing model with local space-invariant interconnections and its optical architecture",
        "url": "https://doi.org/10.1364/ao.29.004790",
        "abstract": "This paper proposes a parallel distributed processing model with local space-invariant interconnections, which is more readily implemented by optics and is able to classify patterns correctly, even if they have been shifted or distorted. Error backpropagation is used as a training algorithm. Computer simulation results presented indicate that the processing is effective and the network can deal with the shifted or distorted patterns. Moreover, the optical implementation architecture using matched filters for the model is discussed.",
        "citations": 201,
        "reasoning": "\"The \" neocognitron \" [18] was introduced by Fukushima in 1980.\" \u2013 *This sentence directly states the introduction of the Neocognitron, a key precursor to CNNs.* - [\"Parallel distributed processing model with local space-invariant interconnections and its optical architecture\"] :-: Zhang (1990)",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Learning Internal Representations by Error Propagation",
        "url": "https://doi.org/10.21236/ada164453",
        "abstract": "This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion",
        "citations": 16342,
        "reasoning": "\"Rumelhart, David E. ; Hinton, Geoffrey E. ; Williams, Ronald J. (1986b). '8. Learning Internal Representations by Error Propagation' . In Rumelhart, David E. ; McClelland, James L. (eds.). Parallel Distributed Processing\u00a0: Explorations in the Microstructure of Cognition . Vol.\u00a01\u00a0: Foundations. Cambridge: MIT Press.\" - *This sentence explicitly refers to a foundational paper on backpropagation.* - [\"Learning Internal Representations by Error Propagation\"] :-: Rumelhart, David E. ; Hinton, Geoffrey E. ; Williams, Ronald J. (1986)",
        "parent_keyword": null,
        "child_claim": "\"Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections.\" \u2013 *This claim speaks about the advantages of CNN architectures that mitigate a known problem of earlier neural networks.* - XXX - XXX",
        "child_keyword": "backpropagation"
      },
      {
        "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
        "url": "https://doi.org/10.1007/bf00344251",
        "abstract": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname \u201cneocognitron\u201d. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \u201cS-cells\u201d, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \u201cC-cells\u201d similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any \u201cteacher\u201d during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.",
        "citations": 5233,
        "reasoning": "\"The neocognitron is a hierarchical, multilayered artificial neural network proposed by Kunihiko Fukushima in 1979.\" \u2013 *This identifies the original paper that proposed the neocognitron.* - [\"Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position\"] :-: Fukushima, Kunihiko (1980)",
        "parent_keyword": null,
        "child_claim": "\"The \" neocognitron \" [18] was introduced by Fukushima in 1980.\" \u2013 *Although the sentence claims Neocognitron was introduced by Fukushima, reference [18] actually refers to Zhang's paper. This sentence should be considered.* - XXX :-: Fukushima (1980)",
        "child_keyword": "neocognitron"
      },
      {
        "title": "Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements",
        "url": "https://doi.org/10.1109/tssc.1969.300225",
        "abstract": "A new type of visual feature extracting network has been synthesized, and the response of the network has been simulated on a digital computer. This research has been done as a first step towards the realization of a recognizer of handwritten characters. The design of the network was suggested by biological systems, especially, the visual systems of cat and monkey. The network is composed of analog threshold elements connected in layers. Each analog threshold element receives inputs from a large number of elements in the neighbouring layers and performs its own special functions. It takes care of one restricted part of the photoreceptor layer, on which an input pattem is presented, and it responds to one particular feature of the input pattem, such as brightness contrast, a dot in the pattern, a line segment of a particular orientation, or an end of the line. This means that the network performs parallel processing of the information. With the propagation of the information through the layered network, the input pattern is successively decomposed into dots, groups of line segments of the same orientation, and the ends of these line segments.",
        "citations": 230,
        "reasoning": "\"Previously in 1969, he published a similar architecture, but with hand-designed kernels inspired by convolutions in mammalian vision.\" \u2013 *This claim refers to an earlier paper by Fukushima that laid the groundwork for the neocognitron.* - [\"Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements\"] :-: Fukushima, Kunihiko (1969)",
        "parent_keyword": null,
        "child_claim": "\"The \" neocognitron \" [18] was introduced by Fukushima in 1980.\" \u2013 *Although the sentence claims Neocognitron was introduced by Fukushima, reference [18] actually refers to Zhang's paper. This sentence should be considered.* - XXX :-: Fukushima (1980)",
        "child_keyword": "neocognitron"
      },
      {
        "title": "Cognitron: A self-organizing multilayered neural network",
        "url": "https://doi.org/10.1007/bf00342633",
        "abstract": "A new hypothesis for the organization of synapses between neurons is proposed: \u201cThe synapse from neuron x to neuron y is reinforced when x fires provided that no neuron in the vicinity of y is firing stronger than y \u201d. By introducing this hypothesis, a new algorithm with which a multilayered neural network is effectively organized can be deduced. A self-organizing multilayered neural network, which is named \u201ccognitron\u201d, is constructed following this algorithm, and is simulated on a digital computer. Unlike the organization of a usual brain models such as a three-layered perceptron, the self-organization of a cognitron progresses favorably without having a \u201cteacher\u201d which instructs in all particulars how the individual cells respond. After repetitive presentations of several stimulus patterns, the cognitron is self-organized in such a way that the receptive fields of the cells become relatively larger in a deeper layer. Each cell in the final layer integrates the information from whole parts of the first layer and selectively responds to a specific stimulus pattern or a feature.",
        "citations": 700,
        "reasoning": "\"In 1975 he improved it to the Cognitron\" \u2013 *This sentence identifies the paper that introduced the Cognitron.* - [\"Cognitron: A self-organizing multilayered neural network\"] :-: Fukushima, Kunihiko (1975)",
        "parent_keyword": null,
        "child_claim": "\"The \" neocognitron \" [18] was introduced by Fukushima in 1980.\" \u2013 *Although the sentence claims Neocognitron was introduced by Fukushima, reference [18] actually refers to Zhang's paper. This sentence should be considered.* - XXX :-: Fukushima (1980)",
        "child_keyword": "neocognitron"
      },
      {
        "title": "Deep learning",
        "url": "https://doi.org/10.1038/nature14539",
        "abstract": "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.",
        "citations": 68822,
        "reasoning": "\"It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation.\" \u2013 *This sentence highlights a key architectural feature (weight sharing) and training method (gradient descent with backpropagation) that are fundamental to modern CNNs, suggesting it's a foundational implementation.* - [\"Deep learning\"] :-: Yann LeCun, Yoshua Bengio, Geoffrey Hinton (2015)",
        "parent_keyword": null,
        "child_claim": "\"Subsequently, AlexNet , a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012.\" \u2013 *This highlights AlexNet's victory, a pivotal moment for CNNs, due to its GPU implementation.* - <XXX - [AlexNet] :-: Alex Krizhevsky et al. (2012)>",
        "child_keyword": "based cnn"
      }
    ],
    "backpropagation": [
      {
        "title": "Learning representations by back-propagating errors",
        "url": "https://doi.org/10.1038/323533a0",
        "abstract": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1 .",
        "citations": 26932,
        "reasoning": "\"Rumelhart; Hinton; Williams (1986). 'Learning representations by back-propagating errors' (PDF) . Nature . 323 (6088): 533\u2013 536.\" \u2013 *This sentence explicitly refers to a foundational paper on backpropagation.* - [\"Learning representations by back-propagating errors\"] :-: Rumelhart; Hinton; Williams (1986)",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections.\" \u2013 *This claim speaks about the advantages of CNN architectures that mitigate a known problem of earlier neural networks.* - XXX - XXX",
        "child_keyword": "backpropagation"
      },
      {
        "title": "Learning Internal Representations by Error Propagation",
        "url": "https://doi.org/10.21236/ada164453",
        "abstract": "This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion",
        "citations": 16342,
        "reasoning": "\"Rumelhart, David E. ; Hinton, Geoffrey E. ; Williams, Ronald J. (1986b). '8. Learning Internal Representations by Error Propagation' . In Rumelhart, David E. ; McClelland, James L. (eds.). Parallel Distributed Processing\u00a0: Explorations in the Microstructure of Cognition . Vol.\u00a01\u00a0: Foundations. Cambridge: MIT Press.\" - *This sentence explicitly refers to a foundational paper on backpropagation.* - [\"Learning Internal Representations by Error Propagation\"] :-: Rumelhart, David E. ; Hinton, Geoffrey E. ; Williams, Ronald J. (1986)",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections.\" \u2013 *This claim speaks about the advantages of CNN architectures that mitigate a known problem of earlier neural networks.* - XXX - XXX",
        "child_keyword": "backpropagation"
      }
    ],
    "relu activation function": [
      {
        "title": "Phone recognition with deep sparse rectifier neural networks",
        "url": "https://doi.org/10.1109/icassp.2013.6639016",
        "abstract": "Rectifier neurons differ from standard ones only in that the sigmoid activation function is replaced by the rectifier function, max(0, x). This modification requires only minimal changes to any existing neural net implementation, but makes it more effective. In particular, we show that a deep architecture of rectifier neurons can attain the same recognition accuracy as deep neural networks, but without the need for pre-training. With 4-5 hidden layers of rectifier neurons we report 20.8% and 19.8% phone error rates on TIMIT (with CI and CD units, respectively), which are competitive with the best results on this database.",
        "citations": 66,
        "reasoning": "\"L\u00e1szl\u00f3 T\u00f3th (2013). Phone Recognition with Deep Sparse Rectifier Neural Networks (PDF) . ICASSP .\" \u2013 *This claim references a paper that uses rectifier neural networks.* - [\"Phone Recognition with Deep Sparse Rectifier Neural Networks\"] :-: L\u00e1szl\u00f3 T\u00f3th (2013).",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"In the same paper, Fukushima also introduced the ReLU (rectified linear unit) activation function .\" \u2013 *This mentions Fukushima's introduction of the ReLU activation function, a crucial component in modern neural networks.* - [XXX] :-: Fukushima (XXX)",
        "child_keyword": "relu activation function"
      }
    ],
    "neocognitron": [
      {
        "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
        "url": "https://doi.org/10.1007/bf00344251",
        "abstract": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname \u201cneocognitron\u201d. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \u201cS-cells\u201d, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \u201cC-cells\u201d similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any \u201cteacher\u201d during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.",
        "citations": 5233,
        "reasoning": "\"The neocognitron is a hierarchical, multilayered artificial neural network proposed by Kunihiko Fukushima in 1979.\" \u2013 *This identifies the original paper that proposed the neocognitron.* - [\"Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position\"] :-: Fukushima, Kunihiko (1980)",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"The \" neocognitron \" [18] was introduced by Fukushima in 1980.\" \u2013 *Although the sentence claims Neocognitron was introduced by Fukushima, reference [18] actually refers to Zhang's paper. This sentence should be considered.* - XXX :-: Fukushima (1980)",
        "child_keyword": "neocognitron"
      },
      {
        "title": "Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements",
        "url": "https://doi.org/10.1109/tssc.1969.300225",
        "abstract": "A new type of visual feature extracting network has been synthesized, and the response of the network has been simulated on a digital computer. This research has been done as a first step towards the realization of a recognizer of handwritten characters. The design of the network was suggested by biological systems, especially, the visual systems of cat and monkey. The network is composed of analog threshold elements connected in layers. Each analog threshold element receives inputs from a large number of elements in the neighbouring layers and performs its own special functions. It takes care of one restricted part of the photoreceptor layer, on which an input pattem is presented, and it responds to one particular feature of the input pattem, such as brightness contrast, a dot in the pattern, a line segment of a particular orientation, or an end of the line. This means that the network performs parallel processing of the information. With the propagation of the information through the layered network, the input pattern is successively decomposed into dots, groups of line segments of the same orientation, and the ends of these line segments.",
        "citations": 230,
        "reasoning": "\"Previously in 1969, he published a similar architecture, but with hand-designed kernels inspired by convolutions in mammalian vision.\" \u2013 *This claim refers to an earlier paper by Fukushima that laid the groundwork for the neocognitron.* - [\"Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements\"] :-: Fukushima, Kunihiko (1969)",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"The \" neocognitron \" [18] was introduced by Fukushima in 1980.\" \u2013 *Although the sentence claims Neocognitron was introduced by Fukushima, reference [18] actually refers to Zhang's paper. This sentence should be considered.* - XXX :-: Fukushima (1980)",
        "child_keyword": "neocognitron"
      },
      {
        "title": "Cognitron: A self-organizing multilayered neural network",
        "url": "https://doi.org/10.1007/bf00342633",
        "abstract": "A new hypothesis for the organization of synapses between neurons is proposed: \u201cThe synapse from neuron x to neuron y is reinforced when x fires provided that no neuron in the vicinity of y is firing stronger than y \u201d. By introducing this hypothesis, a new algorithm with which a multilayered neural network is effectively organized can be deduced. A self-organizing multilayered neural network, which is named \u201ccognitron\u201d, is constructed following this algorithm, and is simulated on a digital computer. Unlike the organization of a usual brain models such as a three-layered perceptron, the self-organization of a cognitron progresses favorably without having a \u201cteacher\u201d which instructs in all particulars how the individual cells respond. After repetitive presentations of several stimulus patterns, the cognitron is self-organized in such a way that the receptive fields of the cells become relatively larger in a deeper layer. Each cell in the final layer integrates the information from whole parts of the first layer and selectively responds to a specific stimulus pattern or a feature.",
        "citations": 700,
        "reasoning": "\"In 1975 he improved it to the Cognitron\" \u2013 *This sentence identifies the paper that introduced the Cognitron.* - [\"Cognitron: A self-organizing multilayered neural network\"] :-: Fukushima, Kunihiko (1975)",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"The \" neocognitron \" [18] was introduced by Fukushima in 1980.\" \u2013 *Although the sentence claims Neocognitron was introduced by Fukushima, reference [18] actually refers to Zhang's paper. This sentence should be considered.* - XXX :-: Fukushima (1980)",
        "child_keyword": "neocognitron"
      }
    ],
    "convolutional interconnections": [
      {
        "title": "Parallel distributed processing model with local space-invariant interconnections and its optical architecture",
        "url": "https://doi.org/10.1364/ao.29.004790",
        "abstract": "This paper proposes a parallel distributed processing model with local space-invariant interconnections, which is more readily implemented by optics and is able to classify patterns correctly, even if they have been shifted or distorted. Error backpropagation is used as a training algorithm. Computer simulation results presented indicate that the processing is effective and the network can deal with the shifted or distorted patterns. Moreover, the optical implementation architecture using matched filters for the model is discussed.",
        "citations": 201,
        "reasoning": "\"Parallel distributed processing model with local space-invariant interconnections and its optical architecture\" \u2013 *although not explicitly linked by the text, it is possible that reference [18] contains key information on space-invariant interconnections and is connected to the first claim.* - [\"Parallel distributed processing model with local space-invariant interconnections and its optical architecture\"] :-: Zhang (1990)",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer.\" \u2013 *This claim suggests a direct lineage to the Neocognitron which is a key component of the history of CNNs.* - XXX - [\"Neocognitron\"] :-: XXX (XXX)",
        "child_keyword": "convolutional interconnections"
      }
    ],
    "based cnn": [
      {
        "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
        "url": "https://doi.org/10.1007/bf00344251",
        "abstract": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname \u201cneocognitron\u201d. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \u201cS-cells\u201d, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \u201cC-cells\u201d similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any \u201cteacher\u201d during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.",
        "citations": 5233,
        "reasoning": "\"Convolutional networks were inspired by biological processes [ 18 ] [ 19 ] [ 20 ] [ 21 ] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex .\" \u2013 *This claim establishes the biological inspiration for CNNs, linking their design to the structure of the animal visual cortex.* - [\"Neocognitron\"] :-: K. Fukushima (2007)",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"Subsequently, AlexNet , a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012.\" \u2013 *This highlights AlexNet's victory, a pivotal moment for CNNs, due to its GPU implementation.* - <XXX - [AlexNet] :-: Alex Krizhevsky et al. (2012)>",
        "child_keyword": "based cnn"
      },
      {
        "title": "Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements",
        "url": "https://doi.org/10.1109/tssc.1969.300225",
        "abstract": "A new type of visual feature extracting network has been synthesized, and the response of the network has been simulated on a digital computer. This research has been done as a first step towards the realization of a recognizer of handwritten characters. The design of the network was suggested by biological systems, especially, the visual systems of cat and monkey. The network is composed of analog threshold elements connected in layers. Each analog threshold element receives inputs from a large number of elements in the neighbouring layers and performs its own special functions. It takes care of one restricted part of the photoreceptor layer, on which an input pattem is presented, and it responds to one particular feature of the input pattem, such as brightness contrast, a dot in the pattern, a line segment of a particular orientation, or an end of the line. This means that the network performs parallel processing of the information. With the propagation of the information through the layered network, the input pattern is successively decomposed into dots, groups of line segments of the same orientation, and the ends of these line segments.",
        "citations": 230,
        "reasoning": "\"Fukushima's ReLU activation function was not used in his neocognitron since all the weights were nonnegative; lateral inhibition was used instead.\" \u2013 *Highlights a design choice in Fukushima's Neocognitron that differs from modern CNNs.* - [\"Visual feature extraction by a multilayered network of analog threshold elements\"] :-: Fukushima (1969)",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"Subsequently, AlexNet , a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012.\" \u2013 *This highlights AlexNet's victory, a pivotal moment for CNNs, due to its GPU implementation.* - <XXX - [AlexNet] :-: Alex Krizhevsky et al. (2012)>",
        "child_keyword": "based cnn"
      },
      {
        "title": "Deep learning",
        "url": "https://doi.org/10.1038/nature14539",
        "abstract": "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.",
        "citations": 68822,
        "reasoning": "\"It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation.\" \u2013 *This sentence highlights a key architectural feature (weight sharing) and training method (gradient descent with backpropagation) that are fundamental to modern CNNs, suggesting it's a foundational implementation.* - [\"Deep learning\"] :-: Yann LeCun, Yoshua Bengio, Geoffrey Hinton (2015)",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"Subsequently, AlexNet , a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012.\" \u2013 *This highlights AlexNet's victory, a pivotal moment for CNNs, due to its GPU implementation.* - <XXX - [AlexNet] :-: Alex Krizhevsky et al. (2012)>",
        "child_keyword": "based cnn"
      }
    ]
  },
  "remaining_to_process": [
    "recurrent neural networks (rnns)",
    "sequential data",
    "feedforward neural networks",
    "recurrent connections",
    "temporal dependencies",
    "recurrent unit",
    "hidden state",
    "vanishing gradient problem",
    "long short-term memory (lstm)",
    "gated recurrent units (grus)",
    "transformers",
    "self-attention mechanisms",
    "neuroscience",
    "cerebellar cortex",
    "parallel fiber",
    "purkinje cells",
    "granule cells",
    "vestibulo-ocular reflex",
    "feedforward structure",
    "reverberating circuit",
    "short-term memory",
    "mcculloch-pitts neuron model",
    "cycles",
    "epilepsy",
    "causalgia",
    "recurrent inhibition",
    "negative feedback",
    "motor control",
    "neural feedback loops",
    "macy conferences",
    "close-loop cross-coupled perceptrons",
    "back-coupled perceptron networks",
    "hebbian learning",
    "ising model",
    "glauber dynamics",
    "sherrington\u2013kirkpatrick model",
    "hopfield network",
    "spin glass",
    "binary activation functions",
    "continuous activation functions",
    "iterated nets",
    "neural history compressor",
    "bidirectional recurrent neural networks (brnn)",
    "encoder-decoder",
    "sequence transduction",
    "seq2seq architecture",
    "attention mechanisms",
    "convolutional neural networks (cnns)",
    "image captioning",
    "unsupervised stack of rnns",
    "description length minimization",
    "conscious chunker",
    "subconscious automatizer",
    "automatic differentiation",
    "very deep learning",
    "history compression",
    "recurrent neural network (rnn)",
    "hidden markov models",
    "sequence learning",
    "long-term memory",
    "lstm unit",
    "cell state",
    "input gate",
    "output gate",
    "forget gate",
    "focused back-propagation",
    "constant error carousel (cec) units",
    "forget gate (keep gate)",
    "peephole connections",
    "recurrent semicircles",
    "recurrent reciprocal connections",
    "feedback",
    "cycles (in networks)",
    "negative feedback mechanism",
    "hebbian learning rule",
    "closed-loop cross-coupled perceptron networks",
    "associative memory",
    "seq2seq",
    "encoder-decoder architecture",
    "description length",
    "neural networks",
    "gradient magnitudes",
    "loss function",
    "forward propagation",
    "network depth",
    "weights",
    "hyperbolic tangent activation function",
    "exploding gradient problem",
    "supervised deep artificial neural networks",
    "feedforward networks",
    "recurrent networks",
    "backpropagation through time",
    "perceptron networks",
    "statistical mechanics",
    "sherrington-kirkpatrick model",
    "cognitive psychology",
    "unsupervised learning",
    "stack of rnns",
    "generative model",
    "cycles (in neural networks)",
    "jordan network",
    "encoder-decoder sequence transduction",
    "time-step",
    "input sequence",
    "algorithmic efficiency",
    "time complexity",
    "space complexity",
    "cycle sort",
    "timsort",
    "big o notation",
    "recurrent, reciprocal connections",
    "excitatory loops",
    "cycles in networks",
    "lstm",
    "brnn",
    "bidirectional lstm",
    "speech recognition",
    "machine translation",
    "language modeling",
    "multilingual language processing",
    "cnns",
    "automatic image captioning",
    "chunking",
    "automatizer",
    "vanishing gradient",
    "3-layered perceptron networks",
    "infinitely deep feedforward network",
    "rnn",
    "feedforward neural network",
    "filter optimization",
    "kernel optimization",
    "vanishing gradients",
    "exploding gradients",
    "shared weights",
    "convolution",
    "cross-correlation",
    "feature maps",
    "shift invariant",
    "space invariant",
    "translation equivariant",
    "translation invariance",
    "fully connected networks",
    "overfitting",
    "regularization",
    "weight decay",
    "dropout",
    "receptive field",
    "filters",
    "kernels",
    "automated learning",
    "vision processing",
    "visual cortex",
    "neurons",
    "visual stimuli",
    "visual space",
    "contralateral visual field",
    "pattern recognition",
    "multilayer visual feature detection network",
    "interconnecting coefficients",
    "homogeneous layers",
    "relu (rectified linear unit) activation function",
    "lateral inhibition",
    "correlation",
    "filter",
    "convolution in time",
    "time delay neural network (tdnn)",
    "phoneme recognition",
    "shift-invariance",
    "1-d convolutional neural net",
    "gradient descent",
    "pyramidal structure",
    "spectrograms",
    "time invariance",
    "frequency shifts",
    "2-d cnn",
    "1-d cnn",
    "convolution kernel",
    "back-propagation",
    "hand-written zip code numbers",
    "hand-written numbers",
    "alphabets recognition",
    "shift-invariant pattern recognition neural network",
    "medical image object segmentation",
    "breast cancer detection in mammograms",
    "tdnns",
    "speaker-independent isolated word recognition system",
    "cresceptron",
    "spatial averaging",
    "inhibition",
    "saturation",
    "downsampling unit",
    "lenet-5",
    "convolutional network",
    "hand-written number classification",
    "check reading systems",
    "32x32 pixel images",
    "shift-invariant neural network",
    "medical image segmentation",
    "electromyography",
    "de-convolution",
    "graphics processing units (gpus)",
    "deep feedforward networks",
    "deep belief networks",
    "gpgpu",
    "imagenet large scale visual recognition challenge",
    "alexnet",
    "thread-level parallelism",
    "simd-level parallelism",
    "intel xeon phi",
    "neural network",
    "gradient computation",
    "chain rule",
    "stochastic gradient descent",
    "adaptive moment estimation",
    "reverse mode of automatic differentiation",
    "reverse accumulation",
    "back-propagating error correction",
    "optimal control theory",
    "adjoint state method",
    "robbins\u2013monro algorithm",
    "jacobian matrix",
    "adaline",
    "squared error loss",
    "multilayer perceptron (mlp)",
    "network sparsity",
    "mlps (multilayer perceptrons)",
    "global minimum",
    "local minimum",
    "discrete signals",
    "nested differentiable functions",
    "nettalk",
    "alvinn",
    "lenet",
    "td-gammon",
    "reinforcement learning",
    "boltzmann machine",
    "machine vision",
    "natural language processing",
    "language structure learning",
    "event-related potential (erp)",
    "n400",
    "p600",
    "photonic processor",
    "optimal control",
    "photoreceptor cell",
    "visual phototransduction",
    "rods",
    "cones",
    "intrinsically photosensitive retinal ganglion cells",
    "scotopic vision",
    "photopic vision",
    "membrane potential",
    "retina",
    "ensemble learning",
    "classifiers",
    "voting",
    "stacking",
    "multiexpert systems",
    "multistage systems",
    "image processing",
    "object detection",
    "object tracking",
    "facial detection",
    "facial recognition",
    "rectified linear unit",
    "activation function",
    "artificial neural networks",
    "neuron",
    "ramp function",
    "half-wave rectification",
    "deep neural nets",
    "computer vision",
    "computational neuroscience",
    "softplus activation function",
    "relu (rectified linear unit)",
    "biological neural networks",
    "visual feature extraction",
    "hierarchical neural networks",
    "neural firing rates",
    "input current",
    "recurrent neural network",
    "logistic sigmoid",
    "hyperbolic tangent",
    "object recognition",
    "average pooling",
    "intensity equivariance",
    "image recognition",
    "restricted boltzmann machines",
    "sparse representation",
    "unsupervised pre-training",
    "supervised tasks",
    "cognitron",
    "convolutional neural networks",
    "s-cells",
    "c-cells",
    "simple cell",
    "complex cell",
    "hierarchical neural network",
    "multilayered neural network",
    "shift-invariant system",
    "time-invariant system",
    "signal processing",
    "translation-equivariant responses",
    "cnn",
    "brain",
    "firing",
    "homogeneous interconnections",
    "cnn architecture",
    "data-trainable system",
    "time delay neural network",
    "convolutional neural net",
    "far-distance speech recognition",
    "speaker-independent isolated word recognition",
    "spatial averaging with inhibition and saturation",
    "courtesy amount reading systems",
    "image character recognition",
    "mammograms",
    "convolution-based design",
    "deconvolution",
    "gpu",
    "neocognitron",
    "operations research",
    "numerical analysis",
    "global maximum",
    "minimization problem",
    "nonlinear function",
    "non-convex function",
    "global minimizers",
    "local optimization",
    "local minima",
    "local maxima",
    "pooling layer",
    "downsampling",
    "aggregation",
    "local pooling",
    "translation-invariant vision",
    "retinal ganglion cells",
    "receptive fields",
    "depth perception",
    "disparity-selective mechanism",
    "global pooling",
    "speech processing",
    "1-dimensional convolution",
    "max pooling",
    "convolutional neural network (cnn)",
    "filter (kernel) optimization",
    "cross-correlation kernel",
    "space invariance",
    "translation equivariance",
    "visual field",
    "filter (signal processing)",
    "frequency invariance",
    "two-dimensional convolution",
    "hand-written zip code recognition",
    "convolution kernel coefficients",
    "hand-written number recognition",
    "alphabet recognition",
    "tdnns (time delay neural networks)",
    "image feature layers",
    "fully connected layer",
    "generalization ability",
    "weight architecture",
    "relu",
    "shift invariance",
    "dimensional convolution",
    "propagation",
    "invariant pattern recognition neural network",
    "tdnn",
    "deep learning",
    "living organisms",
    "temporal dimension",
    "time and frequency shifts",
    "time delay neural networks (tdnns)",
    "convolved signals",
    "graphics processing unit (gpu)",
    "backpropagation",
    "invariant neural network",
    "transformer",
    "fully-connected layer",
    "cascaded convolution",
    "convolutional layer",
    "pre-processing",
    "hand-engineered filters",
    "spectrogram",
    "breast cancer detection",
    "visual cell types",
    "cascading model",
    "trainable system",
    "convolutional interconnections",
    "based cnns"
  ]
}