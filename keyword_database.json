{
  "keywords": {
    "recurrent neural network": [
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83100,
        "reasoning": "Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains. \u2013 *This claim directly states the invention of LSTM networks and names the researchers involved.* - [\"Long Short Term Memory\"] :-: Sepp Hochreiter ; J\u00fcrgen Schmidhuber (1995)"
      }
    ],
    "LSTM": [
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83100,
        "reasoning": "Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains. \u2013 *This claim directly states the invention of LSTM networks and names the researchers involved.* - [\"Long Short Term Memory\"] :-: Sepp Hochreiter ; J\u00fcrgen Schmidhuber (1995)"
      },
      {
        "title": "LSTM can Solve Hard Long Time Lag Problems",
        "url": null,
        "abstract": "Standard recurrent nets cannot deal with long minimal time lags between relevant signals. Several recent NIPS papers propose alternative methods. We first show: problems used to promote various previous algorithms can be solved more quickly by random weight guessing than by the proposed algorithms. We then use LSTM, our own recent algorithm, to solve a hard problem that can neither be quickly solved by random search nor by any other recurrent net algorithm we are aware of.",
        "citations": 756,
        "reasoning": "An early version of LSTM was published in 1995 in a technical report by Sepp Hochreiter and J\u00fcrgen Schmidhuber, [5] then published in the NIPS 1996 conference. [6] \u2013 *This sentence directly states the publication of an early version of LSTM with specific authors and venues.* - [\"LSTM can solve hard long time lag problems\"] :-: Sepp Hochreiter and J\u00fcrgen Schmidhuber (1996)"
      }
    ],
    "gated recurrent units": [
      {
        "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation",
        "url": "https://doi.org/10.3115/v1/d14-1179",
        "abstract": "Kyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014.",
        "citations": 21914,
        "reasoning": "Gated recurrent units ( GRUs ) are a gating mechanism in recurrent neural networks , introduced in 2014 by Kyunghyun Cho et al. [ 1 ] \u2013 *This sentence introduces the GRU and directly cites the paper that introduced them* - [\"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\"] :-: Cho, Kyunghyun et al. (2014)."
      }
    ]
  },
  "remaining_to_process": [
    "Diffusion Models",
    "Generative Models",
    "Latent Variable Models",
    "Forward Diffusion Process",
    "Reverse Sampling Process",
    "Markov Chains",
    "Denoising Diffusion Probabilistic Models",
    "Noise Conditioned Score Networks",
    "Stochastic Differential Equations",
    "Variational Inference",
    "U-nets",
    "Transformers",
    "Computer Vision",
    "Image Denoising",
    "Inpainting",
    "Super-Resolution",
    "Image Generation",
    "Video Generation",
    "Gaussian Noise",
    "Text-Encoders",
    "Cross-Attention Modules",
    "Text-Conditioned Generation",
    "Natural Language Processing",
    "Text Generation",
    "Summarization",
    "Sound Generation",
    "Reinforcement Learning",
    "Recurrent neural networks (RNNs)",
    "Sequential data",
    "Feedforward neural networks",
    "Recurrent connections",
    "Temporal dependencies",
    "Recurrent unit",
    "Hidden state",
    "Vanishing gradient problem",
    "Long short-term memory (LSTM)",
    "Gated recurrent units (GRUs)",
    "Self-attention mechanisms",
    "Recurrent Semicircles",
    "Recurrent reciprocal connections",
    "Reverberating circuit",
    "McCulloch-Pitts neuron model",
    "Recurrent inhibition",
    "Close-loop cross-coupled perceptrons",
    "Back-coupled perceptron networks",
    "Ising Model",
    "Glauber Dynamics",
    "Sherrington-Kirkpatrick Model",
    "Hopfield Network",
    "BRNN",
    "Jordan Network",
    "Elman Network",
    "Neural History Compressor",
    "Bidirectional LSTM",
    "Sequence Transduction",
    "Encoder-Decoder",
    "Attention Mechanisms",
    "Neural history compressor",
    "Unsupervised stack of RNNs",
    "Description length",
    "Conscious chunker",
    "Subconscious automatizer",
    "recurrent neural network (RNN)",
    "vanishing gradient problem",
    "hidden Markov models",
    "sequence learning",
    "short-term memory",
    "long-term memory",
    "LSTM unit",
    "cell",
    "input gate",
    "output gate",
    "forget gate",
    "time series analysis",
    "speech recognition",
    "machine translation",
    "speech activity detection",
    "robot control",
    "Constant Error Carousel (CEC)",
    "Input gate",
    "Output gate",
    "Forget gate",
    "Peephole connections",
    "Connectionist Temporal Classification (CTC)",
    "Gated recurrent unit (GRU)",
    "Highway network",
    "ResNet",
    "xLSTM",
    "mLSTM",
    "sLSTM",
    "Meta-learning",
    "Neuroevolution",
    "Policy gradients",
    "Neural architecture search",
    "Phoneme error rate",
    "Time-aware LSTM (T-LSTM)",
    "Gated Recurrent Units (GRUs)",
    "Gating Mechanism",
    "Recurrent Neural Networks",
    "Long Short-Term Memory (LSTM)",
    "Context Vector",
    "Output Gate",
    "Parameters",
    "Polyphonic Music Modeling",
    "Speech Signal Modeling",
    "LSTM"
  ]
}