{
  "keywords": {
    "convolutional neural network": [
      {
        "title": "Backpropagation Applied to Handwritten Zip Code Recognition",
        "url": "https://doi.org/10.1162/neco.1989.1.4.541",
        "abstract": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.",
        "citations": 10898,
        "claim": "\"For example, for each neuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 \u00d7 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels, [5] only 25 weights for each convolutional layer are required to process 5x5-sized tiles.\" \u2013 *This sentence illustrates the efficiency of convolutional layers through shared weights and cascaded convolutions, which is a core characteristic of CNNs.* - [\"Backpropagation Applied to Handwritten Zip Code Recognition\"] :-: LeCun, Y.; Boser, B.; Denker, J. S.; Henderson, D.; Howard, R. E.; Hubbard, W.; Jackel, L. D. (1989)",
        "reasoning": "The paper's description of efficient image processing through cascaded convolutions and shared weights, fundamental characteristics of convolutional neural networks, directly supports its association with that keyword.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
        "url": "https://doi.org/10.1007/bf00344251",
        "abstract": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname \u201cneocognitron\u201d. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \u201cS-cells\u201d, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \u201cC-cells\u201d similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any \u201cteacher\u201d during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.",
        "citations": 5233,
        "claim": "\"Convolutional networks were inspired by biological processes [18] [19] [20] [21] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex.\" \u2013 *This sentence notes the biological inspiration for CNNs, linking them to the structure of the animal visual cortex.* - [\"Neocognitron\"] :-: Fukushima, K. (2007)",
        "reasoning": "The paper is associated with 'convolutional neural network' because it describes the \"Neocognitron,\" a precursor to modern CNNs, directly inspired by the animal visual cortex's organization, which the source material explicitly links to the biological inspiration behind convolutional networks.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Parallel distributed processing model with local space-invariant interconnections and its optical architecture",
        "url": "https://doi.org/10.1364/ao.29.004790",
        "abstract": "This paper proposes a parallel distributed processing model with local space-invariant interconnections, which is more readily implemented by optics and is able to classify patterns correctly, even if they have been shifted or distorted. Error backpropagation is used as a training algorithm. Computer simulation results presented indicate that the processing is effective and the network can deal with the shifted or distorted patterns. Moreover, the optical implementation architecture using matched filters for the model is discussed.",
        "citations": 201,
        "claim": "\"The \" neocognitron \" [ 18 ] was introduced by Fukushima in 1980.\" \u2013 *This claim directly states the introduction of the Neocognitron and references a paper associated with it.* - [\"Parallel distributed processing model with local space-invariant interconnections and its optical architecture\"] :-: Zhang (1990)",
        "reasoning": "The paper is associated with 'convolutional neural network' because it discusses the Neocognitron, an early model cited as a precursor to convolutional neural networks.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements",
        "url": "https://doi.org/10.1109/tssc.1969.300225",
        "abstract": "A new type of visual feature extracting network has been synthesized, and the response of the network has been simulated on a digital computer. This research has been done as a first step towards the realization of a recognizer of handwritten characters. The design of the network was suggested by biological systems, especially, the visual systems of cat and monkey. The network is composed of analog threshold elements connected in layers. Each analog threshold element receives inputs from a large number of elements in the neighbouring layers and performs its own special functions. It takes care of one restricted part of the photoreceptor layer, on which an input pattem is presented, and it responds to one particular feature of the input pattem, such as brightness contrast, a dot in the pattern, a line segment of a particular orientation, or an end of the line. This means that the network performs parallel processing of the information. With the propagation of the information through the layered network, the input pattern is successively decomposed into dots, groups of line segments of the same orientation, and the ends of these line segments.",
        "citations": 230,
        "claim": "\"Fukushima's ReLU activation function was not used in his neocognitron since all the weights were nonnegative; lateral inhibition was used instead.\" - *This claim, while discussing the ReLU activation function, implies that the neocognitron and its properties are described in Fukushima's earlier work.* - [\"Visual feature extraction by a multilayered network of analog threshold elements\"] :-: Fukushima (1969)",
        "reasoning": "The paper is associated with 'convolutional neural network' because it describes Fukushima's neocognitron, a precursor to modern CNNs, as evidenced by the reference to \"Visual feature extraction by a multilayered network of analog threshold elements\" which details its architecture and functionality.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Phoneme recognition using time-delay neural networks",
        "url": "https://doi.org/10.1109/29.21701",
        "abstract": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
        "citations": 2579,
        "claim": "\"The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was an early convolutional network exhibiting shift-invariance.\" \u2013 *This sentence explicitly states that TDNNs were an early form of CNN and also highlights a key characteristic, shift-invariance.* - [\"Phoneme Recognition Using Time-Delay Neural Networks\"] :-: Alex Waibel (1987)",
        "reasoning": "The paper's identification of Time-Delay Neural Networks (TDNNs) for phoneme recognition, as an early form of convolutional neural network exhibiting shift-invariance, directly aligns it with the 'convolutional neural network' keyword.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Deep learning",
        "url": "https://doi.org/10.1038/nature14539",
        "abstract": "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.",
        "citations": 69457,
        "claim": "\"It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation.\" \u2013 *This sentence claims TDNN was the first CNN to utilize weight sharing with gradient descent and backpropagation, important concepts in CNNs.* - [\"Deep learning\"] :-: Yann LeCun, Yoshua Bengio, Geoffrey Hinton (2015)",
        "reasoning": "The paper is associated with 'convolutional neural network' because it details the first CNN utilizing weight sharing with gradient descent and backpropagation, key concepts within the field of convolutional neural networks.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Learning Internal Representations by Error Propagation",
        "url": "https://doi.org/10.21236/ada164453",
        "abstract": "This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion",
        "citations": 16342,
        "claim": "\"Rumelhart, David E. ; Hinton, Geoffrey E. ; Williams, Ronald J. (1986b). \\\"8. Learning Internal Representations by Error Propagation\\\" . In Rumelhart, David E. ; McClelland, James L. (eds.). Parallel Distributed Processing\u00a0: Explorations in the Microstructure of Cognition . Vol.\u00a01\u00a0: Foundations. Cambridge: MIT Press.\" - *This identifies a chapter by Rumelhart, Hinton, and Williams in Parallel Distributed Processing that is also highly relevant to the development of backpropagation.* - [\"Learning Internal Representations by Error Propagation\"] :-: Rumelhart, David E. ; Hinton, Geoffrey E. ; Williams, Ronald J. (1986)",
        "reasoning": "The paper on learning internal representations by error propagation is associated with 'convolutional neural network' because it is foundational to backpropagation, a core training algorithm frequently used in convolutional neural networks.",
        "parent_keyword": null,
        "child_claim": "\"Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections.\" \u2013 *This sentence describes a key advantage of CNNs compared to earlier neural networks, which is directly related to their architecture and training.* - [XXX] :-: XXX (XXX)",
        "child_keyword": "backpropagation"
      },
      {
        "title": "Cognitron: A self-organizing multilayered neural network",
        "url": "https://doi.org/10.1007/bf00342633",
        "abstract": "A new hypothesis for the organization of synapses between neurons is proposed: \u201cThe synapse from neuron x to neuron y is reinforced when x fires provided that no neuron in the vicinity of y is firing stronger than y \u201d. By introducing this hypothesis, a new algorithm with which a multilayered neural network is effectively organized can be deduced. A self-organizing multilayered neural network, which is named \u201ccognitron\u201d, is constructed following this algorithm, and is simulated on a digital computer. Unlike the organization of a usual brain models such as a three-layered perceptron, the self-organization of a cognitron progresses favorably without having a \u201cteacher\u201d which instructs in all particulars how the individual cells respond. After repetitive presentations of several stimulus patterns, the cognitron is self-organized in such a way that the receptive fields of the cells become relatively larger in a deeper layer. Each cell in the final layer integrates the information from whole parts of the first layer and selectively responds to a specific stimulus pattern or a feature.",
        "citations": 703,
        "claim": "\"In 1975 he improved it to the Cognitron\" \u2013 *Claims a paper exists for the Cognitron architecture* - [\"Cognitron: A self-organizing multilayered neural network\"] :-: Kunihiko Fukushima (1975)",
        "reasoning": "The paper is associated with 'convolutional neural network' because it introduces the Cognitron architecture, which is recognized as an early precursor and a key inspiration in the development of convolutional neural networks.",
        "parent_keyword": null,
        "child_claim": "\"It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer.\" \u2013 *This describes the architecture of the shift-invariant neural network as a modification of the Neocognitron.* - XXX - [XXX] :-: XXX (XXX)",
        "child_keyword": "neocognitron"
      }
    ],
    "backpropagation": [
      {
        "title": "Applied Optimal Control",
        "url": "https://doi.org/10.1201/9781315137667",
        "abstract": "No abstract found",
        "citations": 5996,
        "claim": "\"Hecht-Nielsen [ 19 ] credits the Robbins\u2013Monro algorithm (1951) [ 20 ] and Arthur Bryson and Yu-Chi Ho 's Applied Optimal Control (1969) as presages of backpropagation.\" \u2013 *This sentence attributes the Robbins-Monro algorithm and Bryson and Ho's book as precursors to backpropagation.* - [\"Applied Optimal Control\"] :-: Arthur Bryson and Yu-Chi Ho (1969)",
        "reasoning": "This paper is associated with 'backpropagation' because it cites \"Applied Optimal Control\" by Arthur Bryson and Yu-Chi Ho (1969) as a precursor to backpropagation, a key training method relevant to convolutional neural networks as 'backpropagation' is a child keyword of 'convolutional neural network' based on the claim that CNNs address vanishing/exploding gradients seen during backpropagation in earlier neural networks.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections.\" \u2013 *This sentence describes a key advantage of CNNs compared to earlier neural networks, which is directly related to their architecture and training.* - [XXX] :-: XXX (XXX)",
        "child_keyword": "backpropagation"
      },
      {
        "title": "Optimal shape design, reverse mode of automatic differentiation and turbulence",
        "url": "https://doi.org/10.2514/6.1997-99",
        "abstract": "No abstract found",
        "citations": 64,
        "claim": "\"Modern backpropagation was first published by Seppo Linnainmaa as \"reverse mode of automatic differentiation \" (1970)\" \u2013 *Direct claim associating Linnainmaa with a foundational paper.* - [\"reverse mode of automatic differentiation\"] :-: Seppo Linnainmaa (1970)",
        "reasoning": "The paper is associated with 'backpropagation' because it's the foundational \"reverse mode of automatic differentiation\" paper by Seppo Linnainmaa, a concept directly linked to 'backpropagation', which itself is a child keyword of 'convolutional neural network' due to its role in training CNNs and overcoming issues like vanishing gradients.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections.\" \u2013 *This sentence describes a key advantage of CNNs compared to earlier neural networks, which is directly related to their architecture and training.* - [XXX] :-: XXX (XXX)",
        "child_keyword": "backpropagation"
      },
      {
        "title": "Applications of advances in nonlinear sensitivity analysis",
        "url": "https://doi.org/10.1007/bfb0006203",
        "abstract": "The following paper summarizes the major properties and applications of a collection of algorithms involving differentiation and optimization at minimum cost. The areas of application include the sensitivity analysis of models, new work in statistical or econometric estimation, optimization, artificial intelligence and neuron modelling. The details, references and derivations can be obtained by requesting \u201eSensitivity Analysis Methods for Nonlinear Systems\u201c from Forecast Analysis and Evaluation Team, Quality Assurance, OSS/EIA, Room 7413, Department of Energy, Washington, DC 20461.",
        "citations": 253,
        "claim": "\"In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard.\" \u2013 *Direct claim associating Werbos with backpropagation on MLPs.* - [\"Applications of advances in nonlinear sensitivity analysis\"] :-: Paul Werbos (1982)",
        "reasoning": "This paper is associated with 'backpropagation' because it details Paul Werbos's application of backpropagation to MLPs, a technique relevant to the broader context of 'convolutional neural networks' as 'backpropagation' is a child keyword stemming from the advantages of CNNs over previous networks trained with backpropagation.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections.\" \u2013 *This sentence describes a key advantage of CNNs compared to earlier neural networks, which is directly related to their architecture and training.* - [XXX] :-: XXX (XXX)",
        "child_keyword": "backpropagation"
      },
      {
        "title": "Learning representations by back-propagating errors",
        "url": "https://doi.org/10.1038/323533a0",
        "abstract": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1 .",
        "citations": 26932,
        "claim": "\"Rumelhart; Hinton; Williams (1986). \\\"Learning representations by back-propagating errors\\\" (PDF) . Nature . 323 (6088): 533\u2013 536.\" \u2013 *This identifies a paper by Rumelhart, Hinton, and Williams in Nature that is highly relevant to the development of backpropagation.* - [\"Learning representations by back-propagating errors\"] :-: Rumelhart; Hinton; Williams (1986)",
        "reasoning": "This paper is associated with 'backpropagation' because it is the seminal work by Rumelhart, Hinton, and Williams directly cited in the claim detailing the development of backpropagation, a concept derived from the advantages of convolutional neural networks in preventing vanishing/exploding gradients, as stated in its parent keyword description.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections.\" \u2013 *This sentence describes a key advantage of CNNs compared to earlier neural networks, which is directly related to their architecture and training.* - [XXX] :-: XXX (XXX)",
        "child_keyword": "backpropagation"
      },
      {
        "title": "Learning Internal Representations by Error Propagation",
        "url": "https://doi.org/10.21236/ada164453",
        "abstract": "This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion",
        "citations": 16342,
        "claim": "\"Rumelhart, David E. ; Hinton, Geoffrey E. ; Williams, Ronald J. (1986b). \\\"8. Learning Internal Representations by Error Propagation\\\" . In Rumelhart, David E. ; McClelland, James L. (eds.). Parallel Distributed Processing\u00a0: Explorations in the Microstructure of Cognition . Vol.\u00a01\u00a0: Foundations. Cambridge: MIT Press.\" - *This identifies a chapter by Rumelhart, Hinton, and Williams in Parallel Distributed Processing that is also highly relevant to the development of backpropagation.* - [\"Learning Internal Representations by Error Propagation\"] :-: Rumelhart, David E. ; Hinton, Geoffrey E. ; Williams, Ronald J. (1986)",
        "reasoning": "This research paper is associated with 'backpropagation' because it is the foundational work by Rumelhart, Hinton, and Williams on error propagation, a key concept linked to convolutional neural networks as a solution to vanishing/exploding gradients (which 'backpropagation' is a child of), and directly addresses the development of the backpropagation algorithm itself.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by the regularization that comes from using shared weights over fewer connections.\" \u2013 *This sentence describes a key advantage of CNNs compared to earlier neural networks, which is directly related to their architecture and training.* - [XXX] :-: XXX (XXX)",
        "child_keyword": "backpropagation"
      }
    ],
    "cascading model": [
      {
        "title": "Robust Real-Time Face Detection",
        "url": "https://doi.org/10.1023/b:visi.0000013087.49260.fb",
        "abstract": "This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the \u201cIntegral Image\u201d which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a \u201ccascade\u201d which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second.",
        "citations": 14156,
        "claim": "\"The first cascading classifier was the face detector of Viola and Jones (2001).\" \u2013 *This sentence directly claims the first cascading classifier was the Viola and Jones face detector.* - [\"Robust Real-Time Face Detection\"] :-: Paul Viola and Michael Jones (2001)",
        "reasoning": "The paper is associated with 'cascading model' because it describes the first cascading classifier, as claimed in the paper, and 'cascading model' is a child keyword of 'convolutional neural network' due to Hubel and Wiesel's proposal of a cascading model for pattern recognition within the context of convolutional neural networks.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.\" \u2013 *This mentions that Hubel and Wiesel proposed a cascading model, which is relevant to convolutional neural networks.* - [XXX] :-: Hubel and Wiesel (XXX)",
        "child_keyword": "cascading model"
      }
    ],
    "relu activation function": [],
    "invariant neural network": [
      {
        "title": "Receptive fields of single neurones in the cat's striate cortex",
        "url": "https://doi.org/10.1113/jphysiol.1959.sp006308",
        "abstract": "No abstract found",
        "citations": 4459,
        "claim": "\"In 1969, Kunihiko Fukushima introduced a multilayer visual feature detection network, inspired by the above-mentioned work of Hubel and Wiesel, in which \"All the elements in one layer have the same set of interconnecting coefficients; the arrangement of the elements and their interconnections are all homogeneous over a given layer.\" \" \u2013 *This sentence describes Fukushima's network and its inspiration, which is relevant to the development of convolutional networks.* - [\"Receptive fields of single neurones in the cat's striate cortex\"] :-: Hubel, DH; Wiesel, TN (1959)",
        "reasoning": "This paper is associated with 'invariant neural network', a child keyword of 'convolutional neural network', because the original claim about Fukushima's network relates to the development of convolutional networks, and 'invariant neural network' was derived from 'convolutional neural network' based on the identification of an early shift-invariant neural network.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"A shift-invariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988.\" \u2013 *This identifies an early shift-invariant neural network for image character recognition.* - XXX - [XXX] :-: Wei Zhang et al. (1988)",
        "child_keyword": "invariant neural network"
      },
      {
        "title": "Parallel distributed processing model with local space-invariant interconnections and its optical architecture",
        "url": "https://doi.org/10.1364/ao.29.004790",
        "abstract": "This paper proposes a parallel distributed processing model with local space-invariant interconnections, which is more readily implemented by optics and is able to classify patterns correctly, even if they have been shifted or distorted. Error backpropagation is used as a training algorithm. Computer simulation results presented indicate that the processing is effective and the network can deal with the shifted or distorted patterns. Moreover, the optical implementation architecture using matched filters for the model is discussed.",
        "citations": 201,
        "claim": "\"Zhang, Wei (1990). \"Parallel distributed processing model with local space-invariant interconnections and its optical architecture\"\" \u2013 *This is the full reference for reference 18 in the above claim.* - <[18] - [\"Parallel distributed processing model with local space-invariant interconnections and its optical architecture\"] :-: Zhang (1990)>.",
        "reasoning": "This paper is associated with 'invariant neural network' because it is the full reference for an early shift-invariant neural network proposed by Wei Zhang et al. for image character recognition, a specific example that led to the derivation of 'invariant neural network' as a child keyword of 'convolutional neural network'.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"A shift-invariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988.\" \u2013 *This identifies an early shift-invariant neural network for image character recognition.* - XXX - [XXX] :-: Wei Zhang et al. (1988)",
        "child_keyword": "invariant neural network"
      },
      {
        "title": "Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements",
        "url": "https://doi.org/10.1109/tssc.1969.300225",
        "abstract": "A new type of visual feature extracting network has been synthesized, and the response of the network has been simulated on a digital computer. This research has been done as a first step towards the realization of a recognizer of handwritten characters. The design of the network was suggested by biological systems, especially, the visual systems of cat and monkey. The network is composed of analog threshold elements connected in layers. Each analog threshold element receives inputs from a large number of elements in the neighbouring layers and performs its own special functions. It takes care of one restricted part of the photoreceptor layer, on which an input pattem is presented, and it responds to one particular feature of the input pattem, such as brightness contrast, a dot in the pattern, a line segment of a particular orientation, or an end of the line. This means that the network performs parallel processing of the information. With the propagation of the information through the layered network, the input pattern is successively decomposed into dots, groups of line segments of the same orientation, and the ends of these line segments.",
        "citations": 230,
        "claim": "\"Fukushima, K. (1969). \"Visual feature extraction by a multilayered network of analog threshold elements\"\" \u2013 *Claims that the rectifier has become popular with CNNs and deep neural networks. Cites the paper in which Fukushima introduced a multilayered network.* - <[42] - [\"Visual feature extraction by a multilayered network of analog threshold elements\"] :-: Fukushima (1969)>.",
        "reasoning": "The paper on visual feature extraction by Fukushima is associated with 'invariant neural network', a child of 'convolutional neural network', because Fukushima introduced a multilayered network that served as an early inspiration for convolutional neural networks, which later developed into shift-invariant networks like the one proposed by Wei Zhang et al. also under this keyword.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"A shift-invariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988.\" \u2013 *This identifies an early shift-invariant neural network for image character recognition.* - XXX - [XXX] :-: Wei Zhang et al. (1988)",
        "child_keyword": "invariant neural network"
      }
    ],
    "neocognitron": [
      {
        "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
        "url": "https://doi.org/10.1007/bf00344251",
        "abstract": "A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by \u201clearning without a teacher\u201d, and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname \u201cneocognitron\u201d. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of \u201cS-cells\u201d, which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of \u201cC-cells\u201d similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any \u201cteacher\u201d during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.",
        "citations": 5233,
        "claim": "\"The neocognitron is a hierarchical, multilayered artificial neural network proposed by Kunihiko Fukushima in 1979.\" \u2013 *States the foundational paper for the neocognitron.* - [\"Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position\"] :-: Kunihiko Fukushima (1980)",
        "reasoning": "The paper is associated with 'neocognitron' because it is the foundational paper proposing the original Neocognitron model, a concept directly derived from the parent keyword 'convolutional neural network' based on architectural modifications and shift-invariant properties.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer.\" \u2013 *This describes the architecture of the shift-invariant neural network as a modification of the Neocognitron.* - XXX - [XXX] :-: XXX (XXX)",
        "child_keyword": "neocognitron"
      },
      {
        "title": "Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements",
        "url": "https://doi.org/10.1109/tssc.1969.300225",
        "abstract": "A new type of visual feature extracting network has been synthesized, and the response of the network has been simulated on a digital computer. This research has been done as a first step towards the realization of a recognizer of handwritten characters. The design of the network was suggested by biological systems, especially, the visual systems of cat and monkey. The network is composed of analog threshold elements connected in layers. Each analog threshold element receives inputs from a large number of elements in the neighbouring layers and performs its own special functions. It takes care of one restricted part of the photoreceptor layer, on which an input pattem is presented, and it responds to one particular feature of the input pattem, such as brightness contrast, a dot in the pattern, a line segment of a particular orientation, or an end of the line. This means that the network performs parallel processing of the information. With the propagation of the information through the layered network, the input pattern is successively decomposed into dots, groups of line segments of the same orientation, and the ends of these line segments.",
        "citations": 230,
        "claim": "\"Previously in 1969, he published a similar architecture, but with hand-designed kernels inspired by convolutions in mammalian vision.\" \u2013 *Claims the existence of a previous paper with a similar architecture.* - [\"Visual Feature Extraction by a Multilayered Network of Analog Threshold Elements\"] :-: Kunihiko Fukushima (1969)",
        "reasoning": "This paper is associated with 'neocognitron' because it's the \"similar architecture\" claimed in the paper's description, and 'neocognitron' is a child of 'convolutional neural network', derived from the claim that shift-invariant neural networks are modified Neocognitrons.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer.\" \u2013 *This describes the architecture of the shift-invariant neural network as a modification of the Neocognitron.* - XXX - [XXX] :-: XXX (XXX)",
        "child_keyword": "neocognitron"
      },
      {
        "title": "Cognitron: A self-organizing multilayered neural network",
        "url": "https://doi.org/10.1007/bf00342633",
        "abstract": "A new hypothesis for the organization of synapses between neurons is proposed: \u201cThe synapse from neuron x to neuron y is reinforced when x fires provided that no neuron in the vicinity of y is firing stronger than y \u201d. By introducing this hypothesis, a new algorithm with which a multilayered neural network is effectively organized can be deduced. A self-organizing multilayered neural network, which is named \u201ccognitron\u201d, is constructed following this algorithm, and is simulated on a digital computer. Unlike the organization of a usual brain models such as a three-layered perceptron, the self-organization of a cognitron progresses favorably without having a \u201cteacher\u201d which instructs in all particulars how the individual cells respond. After repetitive presentations of several stimulus patterns, the cognitron is self-organized in such a way that the receptive fields of the cells become relatively larger in a deeper layer. Each cell in the final layer integrates the information from whole parts of the first layer and selectively responds to a specific stimulus pattern or a feature.",
        "citations": 703,
        "claim": "\"In 1975 he improved it to the Cognitron\" \u2013 *Claims a paper exists for the Cognitron architecture* - [\"Cognitron: A self-organizing multilayered neural network\"] :-: Kunihiko Fukushima (1975)",
        "reasoning": "The paper is associated with 'neocognitron' because it's a child keyword of 'convolutional neural network,' derived from a claim that the paper describes a modification of the Neocognitron architecture, a successor of the earlier Cognitron architecture.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer.\" \u2013 *This describes the architecture of the shift-invariant neural network as a modification of the Neocognitron.* - XXX - [XXX] :-: XXX (XXX)",
        "child_keyword": "neocognitron"
      },
      {
        "title": "Receptive fields of single neurones in the cat's striate cortex",
        "url": "https://doi.org/10.1113/jphysiol.1959.sp006308",
        "abstract": "No abstract found",
        "citations": 4459,
        "claim": "\"The neocognitron was inspired by the model proposed by Hubel & Wiesel in 1959. They found two types of cells in the visual primary cortex called simple cell and complex cell , and also proposed a cascading model of these two types of cells for use in pattern recognition tasks.\" \u2013 *States the inspiration of the Neocognitron as a paper by Hubel & Wiesel* - [\"Receptive fields of single neurones in the cat's striate cortex\"] :-: DH Hubel and TN Wiesel (1959)",
        "reasoning": "This paper is associated with the keyword 'neocognitron' because it discusses the inspiration for the Neocognitron, a concept directly linked to 'convolutional neural network' as a child keyword through its architectural modification based on the Neocognitron.",
        "parent_keyword": "convolutional neural network",
        "child_claim": "\"It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer.\" \u2013 *This describes the architecture of the shift-invariant neural network as a modification of the Neocognitron.* - XXX - [XXX] :-: XXX (XXX)",
        "child_keyword": "neocognitron"
      }
    ],
    "recurrent neural network": [
      {
        "title": "A logical calculus of the ideas immanent in nervous activity",
        "url": "https://doi.org/10.1007/bf02478259",
        "abstract": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
        "citations": 17112,
        "claim": "\"The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles. The current activity of such networks can be affected by activity indefinitely far in the past.\" \u2013 *Identifies the McCulloch-Pitts paper as considering networks with cycles, a key concept in RNNs.* - [\"A logical calculus of the ideas immanent in nervous activity\"] :-: Warren S. McCulloch; Walter Pitts (1943)",
        "reasoning": "The paper is associated with 'recurrent neural network' because it explores networks with cycles, a fundamental characteristic of RNNs that allows them to be influenced by past activity.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms",
        "url": "https://doi.org/10.2307/1419730",
        "abstract": "Abstract : Part I attempts to review the background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons. In Chapter 2, a brief review of the main alternative approaches to the development of brain models is presented. Chapter 3 considers the physiological and psychological criteria for a suitable model, and attempts to evaluate the empirical evidence which is available on several important issues. Chapter 4 contains basic definitions and some of the notation to be used in later sections are presented. Parts II and III are devoted to a summary of the established theoretical results obtained to date. Part II (Chapters 5 through 14) deals with the theory of three-layer series-coupled perceptrons, on which most work has been done to date. Part III (Chapters 15 through 20) deals with the theory of multi-layer and cross-coupled perceptrons. Part IV is concerned with more speculative models and problems for future analysis. Of necessity, the final chapters become increasingly heuristic in character, as the theory of perceptrons is not yet complete, and new possibilities are continually coming to light.",
        "citations": 1858,
        "claim": "\"Later, in Principles of Neurodynamics (1961), he described \"closed-loop cross-coupled\" and \"back-coupled\" perceptron networks, and made theoretical and experimental studies for Hebbian learning in these networks, [ 17 ] :\u200aChapter 19, 21 and noted that a fully cross-coupled perceptron network is equivalent to an infinitely deep feedforward network.\" \u2013 *This sentence continues to build upon Rosenblatt's work and describes different configurations with Hebbian learning and their relation to deep feedforward networks.* - <Paper in the claim - [\"PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS\"] :-: Frank Rosenblatt (1961)>.",
        "reasoning": "The paper is associated with 'recurrent neural network' because it describes \"closed-loop cross-coupled\" perceptron networks that function as infinitely deep feedforward networks, a concept related to the iterative processing inherent in recurrent neural networks.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 84172,
        "claim": "\"Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.\" \u2013 *Identifies the inventors and approximate year of LSTM invention.* - [35] - [\"Long Short Term Memory\"] :-: Sepp Hochreiter; J\u00fcrgen Schmidhuber (1995)",
        "reasoning": "The paper discusses the invention of Long Short-Term Memory (LSTM) networks, which are a type of recurrent neural network.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Bidirectional recurrent neural networks",
        "url": "https://doi.org/10.1109/78.650093",
        "abstract": "In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported.",
        "citations": 8956,
        "claim": "\"Bidirectional recurrent neural networks (BRNN) uses two RNN that processes the same input in opposite directions.\" \u2013 *Mentions the foundational paper on BRNNs.* - [37] - [\"Bidirectional recurrent neural networks\"] :-: Schuster, Mike, and Kuldip K. Paliwal (1997)",
        "reasoning": "The paper is associated with 'recurrent neural network' because it is the foundational paper on bidirectional recurrent neural networks, which are a specific type of recurrent neural network.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation",
        "url": "https://doi.org/10.3115/v1/d14-1179",
        "abstract": "Kyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014.",
        "citations": 22133,
        "claim": "\"The papers most commonly cited as the originators that produced seq2seq are two papers from 2014.\" \u2013 *Highlights the foundational seq2seq papers.* - [46] - [\"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\"] :-: Cho, Kyunghyun; van Merrienboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua (2014)",
        "reasoning": "The paper is associated with 'recurrent neural network' because it introduces a recurrent neural network-based encoder-decoder model, a core component of the foundational seq2seq architecture.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "Sequence to Sequence Learning with Neural Networks",
        "url": "https://doi.org/10.48550/arxiv.1409.3215",
        "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
        "citations": 13846,
        "claim": "\"The papers most commonly cited as the originators that produced seq2seq are two papers from 2014.\" \u2013 *Highlights the foundational seq2seq papers.* - [47] - [\"Sequence to Sequence Learning with Neural Networks\"] :-: Sutskever, Ilya; Vinyals, Oriol; Le, Quoc Viet (2014)",
        "reasoning": "The paper \"Sequence to Sequence Learning with Neural Networks\" is identified as relevant because it's a foundational seq2seq paper, and seq2seq models are built upon recurrent neural networks, thus linking the paper to that broader keyword.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      },
      {
        "title": "System Modelling and Optimization",
        "url": "https://doi.org/10.1007/bfb0035455",
        "abstract": "No abstract found",
        "citations": 124,
        "claim": "\"In 1993, such a system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. [ 34 ]\" \u2013 *This sentence refers to a system solving a deep learning task with a very deep unfolded RNN, implying a foundational contribution.* - <Paper in the claim (from references if available) - [\"System modeling and optimization\"] :-: J\u00fcrgen Schmidhuber (1993)>.",
        "reasoning": "The paper is associated with 'recurrent neural network' because it describes a system solving a deep learning task with a very deep unfolded RNN, demonstrating a foundational contribution to the field.",
        "parent_keyword": null,
        "child_claim": null,
        "child_keyword": null
      }
    ],
    "rnn": [
      {
        "title": "A logical calculus of the ideas immanent in nervous activity",
        "url": "https://doi.org/10.1007/bf02478259",
        "abstract": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
        "citations": 17112,
        "claim": "\"The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles.\" \u2013 *This sentence mentions a specific paper and its contribution to considering networks with cycles, a key characteristic of RNNs.* - [\"A logical calculus of the ideas immanent in nervous activity\"] :-: McCulloch, Warren S.; Pitts, Walter (1943)",
        "reasoning": "The paper is associated with 'rnn' because it's identified by a claim discussing McCulloch and Pitts' work on cyclic networks, a key characteristic of recurrent neural networks, the parent keyword of 'rnn', which was derived through a claim about early influential RNN architectures.",
        "parent_keyword": "recurrent neural network",
        "child_claim": "\"Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology .\" \u2013 *Identifies two early and influential RNN architectures.* - XXX - [\"Jordan network\"] :-: XXX (1986)",
        "child_keyword": "rnn"
      },
      {
        "title": "Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms",
        "url": "https://doi.org/10.2307/1419730",
        "abstract": "Abstract : Part I attempts to review the background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons. In Chapter 2, a brief review of the main alternative approaches to the development of brain models is presented. Chapter 3 considers the physiological and psychological criteria for a suitable model, and attempts to evaluate the empirical evidence which is available on several important issues. Chapter 4 contains basic definitions and some of the notation to be used in later sections are presented. Parts II and III are devoted to a summary of the established theoretical results obtained to date. Part II (Chapters 5 through 14) deals with the theory of three-layer series-coupled perceptrons, on which most work has been done to date. Part III (Chapters 15 through 20) deals with the theory of multi-layer and cross-coupled perceptrons. Part IV is concerned with more speculative models and problems for future analysis. Of necessity, the final chapters become increasingly heuristic in character, as the theory of perceptrons is not yet complete, and new possibilities are continually coming to light.",
        "citations": 1858,
        "claim": "\"Later, in Principles of Neurodynamics (1961), he described \"closed-loop cross-coupled\" and \"back-coupled\" perceptron networks, and made theoretical and experimental studies for Hebbian learning in these networks\" \u2013 *Highlights Rosenblatt's continued work and study of these networks.* - <Paper in the claim - [\"PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS\"] :-: Rosenblatt (1961)>.",
        "reasoning": "This paper is associated with 'rnn' because it's cited in a claim discussing the historical development of recurrent neural networks, specifically connecting to the 'recurrent neural network' parent keyword through the identification of influential early RNN architectures (Jordan and Elman networks) which directly led to the creation of the 'rnn' child keyword.",
        "parent_keyword": "recurrent neural network",
        "child_claim": "\"Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology .\" \u2013 *Identifies two early and influential RNN architectures.* - XXX - [\"Jordan network\"] :-: XXX (1986)",
        "child_keyword": "rnn"
      },
      {
        "title": "Neural networks and physical systems with emergent collective computational abilities.",
        "url": "https://doi.org/10.1073/pnas.79.8.2554",
        "abstract": "Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.",
        "citations": 18686,
        "claim": "\"In the 1982 paper, Hopfield applied this recently developed theory to study the Hopfield network with binary activation functions.\" \u2013 *Highlights Hopfield's application of the SK model to neural networks with binary activation functions.* - <Paper in the claim - [\"Neural networks and physical systems with emergent collective computational abilities\"] :-: Hopfield (1982)>.",
        "reasoning": "This research paper is associated with 'rnn' because it discusses Hopfield's application of a model to neural networks, and 'rnn' is a child keyword of 'recurrent neural network' derived from a claim identifying early and influential RNN architectures.",
        "parent_keyword": "recurrent neural network",
        "child_claim": "\"Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology .\" \u2013 *Identifies two early and influential RNN architectures.* - XXX - [\"Jordan network\"] :-: XXX (1986)",
        "child_keyword": "rnn"
      },
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 84172,
        "claim": "\"Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.\" \u2013 *Identifies the inventors and invention year of LSTM networks.* - [36] - [\"Long Short-Term Memory\"] :-: Hochreiter, Sepp ; Schmidhuber, J\u00fcrgen (1997)",
        "reasoning": "The paper is associated with 'rnn' because it discusses Long Short-Term Memory (LSTM) networks, a type of recurrent neural network ('rnn') as evidenced by the claim identifying LSTM's inventors and invention year, connecting to 'rnn' via the established parent-child relationship between 'recurrent neural network' and 'rnn'.",
        "parent_keyword": "recurrent neural network",
        "child_claim": "\"Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology .\" \u2013 *Identifies two early and influential RNN architectures.* - XXX - [\"Jordan network\"] :-: XXX (1986)",
        "child_keyword": "rnn"
      },
      {
        "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation",
        "url": "https://doi.org/10.3115/v1/d14-1179",
        "abstract": "Kyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014.",
        "citations": 22133,
        "claim": "\"The papers most commonly cited as the originators that produced seq2seq are two papers from 2014.\" \u2013 *Highlights two papers related to the origin of seq2seq.* - [46] - [\"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\"] :-: Cho, Kyunghyun; van Merrienboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua (2014)",
        "reasoning": "This paper is associated with 'rnn' because it's identified as originating from a claim highlighting influential works on seq2seq, which are built upon recurrent neural networks ('rnn'), a child keyword derived from 'recurrent neural network' according to claims about early RNN architectures.",
        "parent_keyword": "recurrent neural network",
        "child_claim": "\"Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology .\" \u2013 *Identifies two early and influential RNN architectures.* - XXX - [\"Jordan network\"] :-: XXX (1986)",
        "child_keyword": "rnn"
      },
      {
        "title": "Sequence to Sequence Learning with Neural Networks",
        "url": "https://doi.org/10.48550/arxiv.1409.3215",
        "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
        "citations": 13846,
        "claim": "\"The papers most commonly cited as the originators that produced seq2seq are two papers from 2014.\" \u2013 *Highlights two papers related to the origin of seq2seq.* - [47] - [\"Sequence to Sequence Learning with Neural Networks\"] :-: Sutskever, Ilya; Vinyals, Oriol; Le, Quoc V. (2014)",
        "reasoning": "This paper is associated with 'rnn' because it's identified as one of the two originator papers relating to the 'seq2seq' architecture, and 'seq2seq' utilizes recurrent neural networks, a concept within the 'recurrent neural network' family, for which 'rnn' is a direct child keyword.",
        "parent_keyword": "recurrent neural network",
        "child_claim": "\"Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology .\" \u2013 *Identifies two early and influential RNN architectures.* - XXX - [\"Jordan network\"] :-: XXX (1986)",
        "child_keyword": "rnn"
      },
      {
        "title": "System Modelling and Optimization",
        "url": "https://doi.org/10.1007/bfb0035455",
        "abstract": "No abstract found",
        "citations": 124,
        "claim": "\"In 1993, such a system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time. [34]\" \u2013 *This claim describes a system solving a deep learning task with a large RNN.* - [\"System modeling and optimization\"] :-: J\u00fcrgen Schmidhuber (1993)",
        "reasoning": "The paper is associated with 'rnn' because it details a system that utilizes a large recurrent neural network, a key aspect of RNNs as highlighted by the claim linking 'recurrent neural network' (parent) to 'rnn' (child) via influential early RNN architectures, and the paper's subject solved a deep learning task using such a system.",
        "parent_keyword": "recurrent neural network",
        "child_claim": "\"Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology .\" \u2013 *Identifies two early and influential RNN architectures.* - XXX - [\"Jordan network\"] :-: XXX (1986)",
        "child_keyword": "rnn"
      }
    ]
  },
  "remaining_to_process": [
    "filter optimization",
    "shared weights",
    "convolution kernel",
    "feature maps",
    "receptive field",
    "visual cortex",
    "vanishing gradients",
    "exploding gradients",
    "regularization",
    "vision processing",
    "visual cell types",
    "pattern recognition",
    "visual field",
    "multilayer visual feature detection network",
    "interconnecting coefficients",
    "homogeneous arrangement of elements",
    "relu (rectified linear unit) activation function",
    "lateral inhibition",
    "signal processing",
    "filter",
    "speech recognition",
    "correlation",
    "time delay neural network (tdnn)",
    "shift-invariance",
    "weight sharing",
    "gradient descent",
    "pyramidal structure",
    "local optimization",
    "spectrograms",
    "phoneme recognition",
    "2-d cnn",
    "hand-written zip code numbers",
    "1-d cnn",
    "back-propagation",
    "convolution kernel coefficients",
    "hand-written numbers",
    "alphabets recognition",
    "shift-invariant pattern recognition neural network",
    "medical image object segmentation",
    "breast cancer detection",
    "mammograms",
    "max pooling",
    "cresceptron",
    "spatial averaging",
    "inhibition",
    "saturation",
    "downsampling unit",
    "patch",
    "tdnns (time delay neural networks)",
    "lenet-5",
    "hand-written number classification",
    "check reading systems",
    "shift-invariant neural network",
    "convolutional interconnections",
    "image feature layers",
    "electromyography",
    "de-convolution",
    "graphics processing unit (gpu)",
    "convolutional neural network (cnn)",
    "deep feedforward networks",
    "deep belief networks",
    "gpgpu",
    "image recognition",
    "alexnet",
    "imagenet large scale visual recognition challenge",
    "thread-level parallelism",
    "simd-level parallelism",
    "intel xeon phi",
    "gradient computation",
    "neural network training",
    "chain rule",
    "loss function",
    "weights",
    "stochastic gradient descent",
    "adaptive moment estimation",
    "reverse mode of automatic differentiation",
    "reverse accumulation",
    "optimal control theory",
    "adjoint state method",
    "robbins-monro algorithm",
    "squared error loss",
    "multilayer perceptron (mlp)",
    "jacobian matrix",
    "mlps",
    "dynamic model estimation",
    "nationalism prediction",
    "social communications prediction",
    "local minimum",
    "global minimum",
    "nettalk",
    "alvinn",
    "lenet",
    "td-gammon",
    "boltzmann machine",
    "reinforcement learning",
    "machine vision",
    "natural language processing",
    "language structure learning",
    "event-related potential",
    "photonic processor",
    "robbins\u2013monro algorithm",
    "ensemble learning",
    "classifiers",
    "voting ensembles",
    "stacking ensembles",
    "multiexpert systems",
    "multistage",
    "positive samples",
    "negative images",
    "object detection",
    "image processing",
    "facial detection",
    "facial recognition",
    "rectified linear unit",
    "activation function",
    "artificial neural networks",
    "deep neural nets",
    "computer vision",
    "computational neuroscience",
    "ramp function",
    "half-wave rectification",
    "softplus activation function",
    "rectification",
    "biological neural networks",
    "visual feature extraction",
    "hierarchical neural networks",
    "convolutional neural networks (cnns)",
    "object recognition",
    "average pooling",
    "intensity equivariance",
    "restricted boltzmann machines",
    "sparse representation",
    "unsupervised pre-training",
    "deep networks",
    "functional analysis",
    "cross-correlation",
    "deconvolution",
    "integral",
    "shift",
    "commutativity",
    "probability",
    "statistics",
    "acoustics",
    "spectroscopy",
    "geophysics",
    "engineering",
    "physics",
    "differential equations",
    "euclidean space",
    "periodic functions",
    "discrete-time fourier transform",
    "discrete convolution",
    "numerical analysis",
    "numerical linear algebra",
    "finite impulse response filters",
    "impulse response",
    "linear time-invariant system",
    "shift invariant system",
    "time-invariant system",
    "operations research",
    "applied mathematics",
    "global maximum",
    "minimization problem",
    "non-linear function",
    "non-convex function",
    "local maximum",
    "plant propagation",
    "seeds",
    "cuttings",
    "plant parts",
    "ripening",
    "dispersal",
    "detachment",
    "pruning",
    "horticulture",
    "agriculture",
    "plant breeding",
    "forest crops",
    "fibre crops",
    "herbal medicine",
    "traditional medicine",
    "vegetative parts",
    "asexually-reproducing plants",
    "feedforward neural network",
    "filter (kernel) optimization",
    "deep learning",
    "fully-connected layer",
    "space invariance",
    "translation equivariance",
    "overfitting",
    "weight decay",
    "dropout",
    "biological processes",
    "image classification",
    "automated learning",
    "brain",
    "neurons",
    "visual stimuli",
    "firing",
    "contralateral visual field",
    "convolutional network",
    "cnn architecture",
    "supervised learning algorithms",
    "unsupervised learning algorithms",
    "convolution in time",
    "temporal dimension",
    "frequency shifts",
    "2-d cnn system",
    "kernel coefficients",
    "breast cancer detection in mammograms",
    "tdnns",
    "speaker-independent isolated word recognition system",
    "handwritten digit classification",
    "image character recognition",
    "fully connected layer",
    "generalization ability",
    "medical image segmentation",
    "convolution-based design",
    "convolved signals",
    "graphics processing units (gpus)",
    "relu activation function",
    "convolution",
    "shift invariance",
    "hierarchical artificial neural network",
    "handwritten character recognition",
    "convolutional neural networks",
    "hand-designed kernels",
    "convolutions in mammalian vision",
    "cognitron",
    "unsupervised learning",
    "self-organized learning",
    "visual primary cortex",
    "simple cell",
    "complex cell",
    "s-cells",
    "c-cells",
    "local feature extraction",
    "local feature integration",
    "selective attention",
    "news organization",
    "24-hour news coverage",
    "cable news channel",
    "viewership",
    "programming",
    "bias (left-wing, pro-israeli)",
    "editorial policy",
    "cable news network",
    "newscast",
    "cnn airport",
    "cnn newsource",
    "bureaus",
    "local stations",
    "regional networks",
    "foreign-language networks",
    "recurrent neural networks (rnns)",
    "sequential data",
    "feedforward neural networks",
    "recurrent connections",
    "temporal dependencies",
    "recurrent unit",
    "hidden state",
    "vanishing gradient problem",
    "long short-term memory (lstm)",
    "gated recurrent units (grus)",
    "transformers",
    "self-attention mechanisms",
    "recurrent semicircles",
    "cerebellar cortex",
    "parallel fiber",
    "purkinje cells",
    "granule cells",
    "recurrent, reciprocal connections",
    "golgi's method",
    "excitatory loops",
    "vestibulo-ocular reflex",
    "reverberating circuit",
    "mcculloch-pitts neuron model",
    "cycles",
    "recurrent inhibition",
    "negative feedback",
    "motor control",
    "neural feedback loops",
    "close-loop cross-coupled perceptrons",
    "hebbian learning rule",
    "closed-loop cross-coupled perceptron networks",
    "back-coupled perceptron networks",
    "associative memory",
    "ising model",
    "glauber dynamics",
    "sherrington\u2013kirkpatrick model",
    "hopfield network",
    "spin glass",
    "iterated nets",
    "jordan network",
    "elman network",
    "neural history compressor",
    "very deep learning",
    "bidirectional recurrent neural networks (brnn)",
    "encoder-decoder sequence transduction",
    "seq2seq architecture",
    "attention mechanisms",
    "unsupervised stack of rnns",
    "description length",
    "conscious chunker",
    "subconscious automatizer",
    "history compression",
    "backpropagation",
    "neural networks",
    "gradient magnitudes",
    "forward propagation",
    "network depth",
    "hyperbolic tangent activation function",
    "exploding gradient problem",
    "supervised deep artificial neural networks",
    "many-layered feedforward networks",
    "recurrent networks",
    "backpropagation through time",
    "cycles (in networks)",
    "binary activation functions",
    "continuous activation functions",
    "chunkers",
    "automatizers",
    "automatic differentiation",
    "generative model",
    "vanishing gradient",
    "gated recurrent units",
    "rnn",
    "in-place algorithm",
    "treesort",
    "comparison-based sorting algorithm",
    "heap",
    "data structure",
    "stable sort",
    "treesort algorithm",
    "heap data structure",
    "binary tree",
    "priority queue",
    "max-heaps",
    "min-heaps",
    "sorting algorithm",
    "implicit data structure",
    "array"
  ]
}