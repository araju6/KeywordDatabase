{
  "keywords": {
    "Recurrent Neural Network": [
      {
        "title": "A logical calculus of the ideas immanent in nervous activity",
        "url": "https://doi.org/10.1007/bf02478259",
        "abstract": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
        "citations": 16988,
        "reasoning": "The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles. \u2013 *This claim directly identifies a foundational paper and its key contribution to the concept of RNNs through the introduction of cycles in networks.* - [\"A logical calculus of the ideas immanent in nervous activity\"] :-: McCulloch, Warren S.; Pitts, Walter (1943)"
      },
      {
        "title": "Learning Patterns and Pattern Sequences by Self-Organizing Nets of Threshold Elements",
        "url": "https://doi.org/10.1109/t-c.1972.223477",
        "abstract": "Various information-processing capabilities of self-organizing nets of threshold elements are studied. A self-organizing net, learning from patterns or pattern sequences given from outside as stimuli, \"remembers\" some of them as stable equilibrium states or state-transition sequences of the net. A condition where many patterns and pattern sequences are remembered in a net at the same time is shown. The stability degree of their remembrance and recalling under noise disturbances is investigated theoretically. For this purpose, the stability of state transition in an autonomous logical net of threshold elements is studied by the use of characteristics of threshold elements.",
        "citations": 440,
        "reasoning": "Similar networks were published by Kaoru Nakano in 1971, [ 19 ] [ 20 ] Shun'ichi Amari in 1972, [ 21 ] and William A. Little [ de ] in 1974, [ 22 ] who was acknowledged by Hopfield in his 1982 paper. \u2013 *This claim suggests relevant work by Nakano, Amari and Little in the early 70s, of architectures that were similar to Rosenblatt* - [\"Learning patterns and pattern sequences by self-organizing nets of threshold elements\"] :-: Shun'ichi Amari (1972)"
      },
      {
        "title": "The existence of persistent states in the brain",
        "url": "https://doi.org/10.1016/0025-5564(74)90031-5",
        "abstract": "No abstract found",
        "citations": 628,
        "reasoning": "Similar networks were published by Kaoru Nakano in 1971, [ 19 ] [ 20 ] Shun'ichi Amari in 1972, [ 21 ] and William A. Little [ de ] in 1974, [ 22 ] who was acknowledged by Hopfield in his 1982 paper. \u2013 *This claim suggests relevant work by Nakano, Amari and Little in the early 70s, of architectures that were similar to Rosenblatt* - [\"The Existence of Persistent States in the Brain\"] :-: William A. Little (1974)"
      },
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83100,
        "reasoning": "Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains. \u2013 *This sentence identifies the inventors and the year of invention of LSTM networks* - [\"Long Short-Term Memory\"] :-: Sepp Hochreiter; J\u00fcrgen Schmidhuber (1995)"
      }
    ],
    "vanishing gradient": [
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83100,
        "reasoning": "Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains. \u2013 *This sentence identifies the inventors and the year of invention of LSTM networks* - [\"Long Short-Term Memory\"] :-: Sepp Hochreiter; J\u00fcrgen Schmidhuber (1995)"
      },
      {
        "title": "The Difficulty of Learning Long-Term Dependencies with Gradient Flow in Recurrent Nets",
        "url": "https://doi.org/10.18034/ei.v8i2.570",
        "abstract": "In theory, recurrent networks (RN) can leverage their feedback connections to store activations as representations of recent input events. The most extensively used methods for learning what to put in short-term memory, on the other hand, take far too long to be practicable or do not work at all, especially when the time lags between inputs and instructor signals are long. They do not provide significant practical advantages over, the backdrop in feedforward networks with limited time windows, despite being theoretically fascinating. The goal of this article is to have a succinct overview of this rapidly evolving topic, with a focus on recent advancements. Also, we examine the asymptotic behavior of error gradients as a function of time lags to provide a hypothetical treatment of this topic. The methodology adopted in the study was to review some scholarly research papers on the subject matter to address the difficulty of learning long-term dependencies with gradient flow in recurrent nets. RNNs are the most general and powerful sequence learning algorithm currently available. Unlike Hidden Markov Models (HMMs), which have proven to be the most successful technique in a variety of sequence processing applications, they are not limited to discrete internal states and can represent continuous, dispersed sequences. As a result, they can address problems that no other method can. Conventional RNNs, on the other hand, are difficult to train due to the problem of vanishing gradients.",
        "citations": 13,
        "reasoning": "Hochreiter 's diplom thesis of 1991 formally identified the reason for this failure in the \"vanishing gradient problem\", which not only affects many-layered feedforward networks, but also recurrent networks . \u2013 *Connects Hochreiter's identification of the vanishing gradient problem with recurrent networks* - [\"Gradient flow in recurrent nets: the difficulty of learning long-term dependencies\"] :-: Hochreiter, Bengio, Frasconi, Schmidhuber (2001)"
      },
      {
        "title": "The problem of learning long-term dependencies in recurrent networks",
        "url": "https://doi.org/10.1109/icnn.1993.298725",
        "abstract": "The authors seek to train recurrent neural networks in order to map input sequences to output sequences, for applications in sequence recognition or production. Results are presented showing that learning long-term dependencies in such recurrent networks using gradient descent is a very difficult task. It is shown how this difficulty arises when robustly latching bits of information with certain attractors. The derivatives of the output at time t with respect to the unit activations at time zero tend rapidly to zero as t increases for most input values. In such a situation, simple gradient descent techniques appear inappropriate. The consideration of alternative optimization methods and architectures is suggested.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
        "citations": 216,
        "reasoning": "Hochreiter 's diplom thesis of 1991 formally identified the reason for this failure in the \"vanishing gradient problem\", which not only affects many-layered feedforward networks, but also recurrent networks . \u2013 *Connects Hochreiter's identification of the vanishing gradient problem with recurrent networks* - [\"The problem of learning long-term dependencies in recurrent networks\"] :-: Bengio, Frasconi, Simard (1993)"
      }
    ],
    "GRUs": [
      {
        "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation",
        "url": "https://doi.org/10.3115/v1/d14-1179",
        "abstract": "Kyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014.",
        "citations": 21914,
        "reasoning": "The papers most commonly cited as the originators that produced seq2seq are two papers from 2014 \u2013 *This sentence introduces that two papers from 2014 are credited as originators of seq2seq architectures* - [\"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\"] :-: Kyunghyun Cho et al. (2014)"
      }
    ]
  },
  "remaining_to_process": [
    "Recurrent Neural Networks (RNNs)",
    "Sequential Data",
    "Feedforward Neural Networks",
    "Recurrent Connections",
    "Temporal Dependencies",
    "Recurrent Unit",
    "Hidden State",
    "Unsegmented, Connected Handwriting Recognition",
    "Speech Recognition",
    "Natural Language Processing",
    "Neural Machine Translation",
    "Vanishing Gradient Problem",
    "Long Short-Term Memory (LSTM)",
    "Gated Recurrent Units (GRUs)",
    "Transformers",
    "Self-Attention Mechanisms",
    "Parallelizability",
    "Recurrent semicircles",
    "Recurrent reciprocal connections",
    "Reverberating circuit",
    "McCulloch-Pitts neuron model",
    "Recurrent inhibition",
    "Neural feedback loops",
    "close-loop cross-coupled perceptrons",
    "back-coupled perceptron networks",
    "Hebbian learning",
    "fully cross-coupled perceptron network",
    "Ising model",
    "Glauber dynamics",
    "Sherrington\u2013Kirkpatrick model",
    "Hopfield network",
    "spin glass",
    "binary activation functions",
    "continuous activation functions",
    "Elman network",
    "Neural history compressor",
    "Bidirectional recurrent neural networks (BRNN)",
    "Bidirectional LSTM",
    "Encoder-decoder",
    "Sequence transduction",
    "Seq2seq",
    "Attention mechanisms",
    "Neural History Compressor",
    "Unsupervised Stack of RNNs",
    "Description Length",
    "Automatic Differentiation",
    "Backpropagation",
    "Generative Model",
    "Chunking",
    "Automatizer",
    "vanishing gradient problem",
    "backpropagation",
    "neural networks",
    "loss function",
    "forward propagation",
    "network depth",
    "gradient magnitude",
    "hyperbolic tangent activation function",
    "exploding gradient problem",
    "deep artificial neural networks",
    "feedforward networks",
    "recurrent networks",
    "backpropagation through time",
    "Recurrent neural networks (RNNs)",
    "Sequential data",
    "Feedforward neural networks",
    "Recurrent connections",
    "Temporal dependencies",
    "Recurrent unit",
    "Hidden state",
    "Vanishing gradient problem",
    "Long short-term memory (LSTM)",
    "Gated recurrent units (GRUs)",
    "Self-attention mechanisms",
    "Recurrent neural network",
    "Neuroscience",
    "Cerebellar cortex",
    "Parallel fiber",
    "Purkinje cells",
    "Granule cells",
    "Vestibulo-ocular reflex",
    "Short-term memory",
    "Cycles (in networks)",
    "Epilepsy",
    "Causalgia",
    "Negative feedback mechanism",
    "Motor control",
    "Close-loop cross-coupled perceptrons",
    "Back-coupled perceptron networks",
    "Spin glass",
    "Binary activation functions",
    "Continuous activation functions",
    "Cognitive psychology",
    "Unsupervised stack of RNNs",
    "Description length",
    "Conscious chunker",
    "Subconscious automatizer",
    "Automatic differentiation",
    "Jordan network",
    "vanishing gradient"
  ]
}