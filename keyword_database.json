{
  "keywords": {
    "convolutional neural network": [
      {
        "title": "Neural Network Recognizer for Hand-Written Zip Code Digits",
        "url": null,
        "abstract": "This paper describes the construction of a system that recognizes handprinted digits, using a combination of classical techniques and neural-net methods. The system has been trained and tested on real-world data, derived from zip codes seen on actual U.S. Mail. The system rejects a small percentage of the examples as unclassifiable, and achieves a very low error rate on the remaining examples. The system compares favorably with other state-of-the art recognizers. While some of the methods are specific to this task, it is hoped that many of the techniques will be applicable to a wide range of recognition tasks.",
        "citations": 147,
        "reasoning": "\"Denker et al. (1989) designed a 2-D CNN system to recognize hand-written ZIP Code numbers.\" \u2013 *This sentence identifies the work of Denker et al. as designing a CNN system for a specific task.* - [\"Neural network recognizer for hand-written zip code digits\"] :-: Denker, J S, Gardner, W R, Graf, H. P, Henderson, D, Howard, R E, Hubbard, W, Jackel, L D, BaIrd, H S, and Guyon (1989)",
        "child_claim": "\"Denker et al. (1989) designed a 2-D CNN system to recognize hand-written ZIP Code numbers.\" \u2013 *This sentence identifies the work of Denker et al. as designing a CNN system for a specific task.* - [\"Neural network recognizer for hand-written zip code digits\"] :-: Denker, J S, Gardner, W R, Graf, H. P, Henderson, D, Howard, R E, Hubbard, W, Jackel, L D, BaIrd, H S, and Guyon (1989)"
      }
    ],
    "Backpropagation": [
      {
        "title": "Learning representations by back-propagating errors",
        "url": "https://doi.org/10.1038/323533a0",
        "abstract": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1 .",
        "citations": 26932,
        "reasoning": "\"Rumelhart; Hinton; Williams (1986). \\\"Learning representations by back-propagating errors\\\" (PDF) . Nature . 323 (6088): 533\u2013 536.\" \u2013 *This sentence references a foundational paper on backpropagation.* - [\"Learning representations by back-propagating errors\"] :-: Rumelhart, Hinton, Williams (1986)",
        "child_claim": "\"Rumelhart; Hinton; Williams (1986). \\\"Learning representations by back-propagating errors\\\" (PDF) . Nature . 323 (6088): 533\u2013 536.\" \u2013 *This sentence references a foundational paper on backpropagation.* - [\"Learning representations by back-propagating errors\"] :-: Rumelhart, Hinton, Williams (1986)"
      },
      {
        "title": "Learning Internal Representations by Error Propagation",
        "url": "https://doi.org/10.21236/ada164453",
        "abstract": "This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion",
        "citations": 16342,
        "reasoning": "\"Rumelhart, David E. ; Hinton, Geoffrey E. ; Williams, Ronald J. (1986b). \\\"8. Learning Internal Representations by Error Propagation\\\" . In Rumelhart, David E. ; McClelland, James L. (eds.). Parallel Distributed Processing\u00a0: Explorations in the Microstructure of Cognition . Vol.\u00a01\u00a0: Foundations.\" \u2013 *This sentence references a foundational paper on backpropagation.* - [\"Learning Internal Representations by Error Propagation\"] :-: Rumelhart, Hinton, Williams (1986)",
        "child_claim": "\"Rumelhart, David E. ; Hinton, Geoffrey E. ; Williams, Ronald J. (1986b). \\\"8. Learning Internal Representations by Error Propagation\\\" . In Rumelhart, David E. ; McClelland, James L. (eds.). Parallel Distributed Processing\u00a0: Explorations in the Microstructure of Cognition . Vol.\u00a01\u00a0: Foundations.\" \u2013 *This sentence references a foundational paper on backpropagation.* - [\"Learning Internal Representations by Error Propagation\"] :-: Rumelhart, Hinton, Williams (1986)"
      }
    ]
  },
  "remaining_to_process": [
    "Feedforward neural network",
    "Filter optimization",
    "Shared weights",
    "Kernel",
    "Regularization",
    "Convolution kernel",
    "Feature maps",
    "Overfitting",
    "Receptive field",
    "Vision processing",
    "Visual cortex",
    "Neuron",
    "Visual field",
    "Multilayer visual feature detection network",
    "Interconnecting coefficients",
    "Homogeneous arrangement",
    "ReLU activation function",
    "Neocognitron",
    "Lateral inhibition",
    "Shift invariance",
    "Speech recognition",
    "Correlation",
    "Time delay neural network",
    "Shift-invariance",
    "Weight sharing",
    "Gradient descent",
    "Phoneme recognition",
    "Spectrograms",
    "Time invariance",
    "Frequency invariance",
    "2-D CNN",
    "hand-written ZIP Code numbers",
    "back-propagation",
    "convolution kernel coefficients",
    "hand-written numbers",
    "1-D CNN",
    "alphabets recognition",
    "shift-invariant pattern recognition neural network",
    "medical image object segmentation",
    "breast cancer detection",
    "mammograms",
    "Max pooling",
    "Cresceptron",
    "Spatial averaging",
    "Downsampling unit",
    "Convolutional neural network",
    "LeNet-5",
    "Image recognition",
    "Shift-invariant neural network",
    "Image character recognition",
    "Convolutional interconnections",
    "Back-propagation",
    "Generalization ability",
    "Medical image segmentation",
    "Mammograms",
    "De-convolution",
    "Electromyography",
    "Graphics processing units (GPUs)",
    "GPGPU",
    "Deep belief networks",
    "Deep feedforward networks",
    "Convolutional neural networks (CNNs)",
    "Cross-correlation",
    "Functional Analysis",
    "Deconvolution",
    "Neural Network",
    "Gradient",
    "Loss Function",
    "Chain Rule",
    "Stochastic Gradient Descent",
    "Adaptive Moment Estimation",
    "Reverse Mode of Automatic Differentiation",
    "Reverse Accumulation",
    "Chain rule",
    "Optimal control theory",
    "Adjoint state method",
    "Squared error loss",
    "Multilayer perceptron (MLP)",
    "Stochastic gradient descent",
    "Jacobian matrix",
    "Reverse mode of automatic differentiation",
    "MLPs (Multi-Layer Perceptrons)",
    "Gradient Descent",
    "NETtalk",
    "ALVINN",
    "LeNet",
    "TD-Gammon",
    "Speech Recognition",
    "Machine Vision",
    "Natural Language Processing",
    "Language Structure Learning",
    "Event-Related Potential (ERP)",
    "Photonic Processor",
    "Convolution",
    "Robbins\u2013Monro algorithm"
  ],
  "claim_chains": {
    "Convolution": [
      {
        "claim": "Convolution in time",
        "parent_keyword": "convolutional neural network"
      },
      {
        "claim": "Convolution in time",
        "parent_keyword": "Backpropagation"
      }
    ],
    "Backpropagation": [
      {
        "claim": "Backpropagation Applied to Handwritten Zip Code Recognition",
        "parent_keyword": "Convolution"
      },
      {
        "claim": "Backpropagation Applied to Handwritten Zip Code Recognition",
        "parent_keyword": "convolutional neural network"
      }
    ],
    "Robbins\u2013Monro algorithm": [
      {
        "claim": "Robbins\u2013Monro algorithm",
        "parent_keyword": "Backpropagation"
      }
    ]
  }
}