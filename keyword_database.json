{
  "keywords": {
    "Recurrent Neural Network": [
      {
        "title": "A logical calculus of the ideas immanent in nervous activity",
        "url": "https://doi.org/10.1007/bf02478259",
        "abstract": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.",
        "citations": 16988,
        "reasoning": "The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles. *This claim directly references a paper that introduced a neuron model with cycles, a key element in recurrent networks* - [\"A logical calculus of the ideas immanent in nervous activity\"] :-: McCulloch, Warren S.; Pitts, Walter (1943)"
      },
      {
        "title": "Principles of Neurodynamics.",
        "url": "https://doi.org/10.2307/2312103",
        "abstract": "No abstract found",
        "citations": 2300,
        "reasoning": "Later, in Principles of Neurodynamics (1961), he described \"closed-loop cross-coupled\" and \"back-coupled\" perceptron networks, and made theoretical and experimental studies for Hebbian learning in these networks, \u2013 *This sentence describes Rosenblatt's book and his further development of recurrent perceptrons and Hebbian learning.* - [\"Principles of Neurodynamics\"] :-: Rosenblatt (1961)"
      },
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83100,
        "reasoning": "Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains. \u2013 *Identifies the inventors of LSTM and its impact.* - [\"Long Short-Term Memory\"] :-: Sepp Hochreiter and J\u00fcrgen Schmidhuber (1995)"
      }
    ],
    "vanishing gradient": [
      {
        "title": "Long Short-Term Memory",
        "url": "https://doi.org/10.1162/neco.1997.9.8.1735",
        "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
        "citations": 83100,
        "reasoning": "Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains. \u2013 *Identifies the inventors of LSTM and its impact.* - [\"Long Short-Term Memory\"] :-: Sepp Hochreiter and J\u00fcrgen Schmidhuber (1995)"
      },
      {
        "title": "The Difficulty of Learning Long-Term Dependencies with Gradient Flow in Recurrent Nets",
        "url": "https://doi.org/10.18034/ei.v8i2.570",
        "abstract": "In theory, recurrent networks (RN) can leverage their feedback connections to store activations as representations of recent input events. The most extensively used methods for learning what to put in short-term memory, on the other hand, take far too long to be practicable or do not work at all, especially when the time lags between inputs and instructor signals are long. They do not provide significant practical advantages over, the backdrop in feedforward networks with limited time windows, despite being theoretically fascinating. The goal of this article is to have a succinct overview of this rapidly evolving topic, with a focus on recent advancements. Also, we examine the asymptotic behavior of error gradients as a function of time lags to provide a hypothetical treatment of this topic. The methodology adopted in the study was to review some scholarly research papers on the subject matter to address the difficulty of learning long-term dependencies with gradient flow in recurrent nets. RNNs are the most general and powerful sequence learning algorithm currently available. Unlike Hidden Markov Models (HMMs), which have proven to be the most successful technique in a variety of sequence processing applications, they are not limited to discrete internal states and can represent continuous, dispersed sequences. As a result, they can address problems that no other method can. Conventional RNNs, on the other hand, are difficult to train due to the problem of vanishing gradients.",
        "citations": 13,
        "reasoning": "Hochreiter 's diplom thesis of 1991 formally identified the reason for this failure in the \"vanishing gradient problem\", [ 2 ] [ 3 ] which not only affects many-layered feedforward networks , [ 4 ] but also recurrent networks . \u2013 *Identifies the foundational work that formally defined the vanishing gradient problem, and identifies a paper that covers the issue of gradient flow in recurrent nets* - [\"Gradient flow in recurrent nets: the difficulty of learning long-term dependencies\"] :-: <Hochreiter, Bengio, Frasconi, Schmidhuber> (2001)"
      },
      {
        "title": "The problem of learning long-term dependencies in recurrent networks",
        "url": "https://doi.org/10.1109/icnn.1993.298725",
        "abstract": "The authors seek to train recurrent neural networks in order to map input sequences to output sequences, for applications in sequence recognition or production. Results are presented showing that learning long-term dependencies in such recurrent networks using gradient descent is a very difficult task. It is shown how this difficulty arises when robustly latching bits of information with certain attractors. The derivatives of the output at time t with respect to the unit activations at time zero tend rapidly to zero as t increases for most input values. In such a situation, simple gradient descent techniques appear inappropriate. The consideration of alternative optimization methods and architectures is suggested.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>",
        "citations": 216,
        "reasoning": "The latter are trained by unfolding them into very deep feedforward networks, where a new layer is created for each time-step of an input sequence processed by the network (the combination of unfolding and backpropagation is termed backpropagation through time ). \u2013 *Indicates the context in which recurrent networks suffer from vanishing gradients* - [\"The problem of learning long-term dependencies in recurrent networks\"] :-: <Bengio, Frasconi, Simard> (1993)"
      },
      {
        "title": "On the difficulty of training Recurrent Neural Networks",
        "url": "https://doi.org/10.48550/arxiv.1211.5063",
        "abstract": "There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.",
        "citations": 3533,
        "reasoning": "The latter are trained by unfolding them into very deep feedforward networks, where a new layer is created for each time-step of an input sequence processed by the network (the combination of unfolding and backpropagation is termed backpropagation through time ). \u2013 *Indicates the context in which recurrent networks suffer from vanishing gradients* - [\"On the difficulty of training Recurrent Neural Networks\"] :-: <Pascanu, Mikolov, Bengio> (2012)"
      }
    ]
  },
  "remaining_to_process": [
    "Recurrent neural networks",
    "Sequential data",
    "Recurrent connections",
    "Temporal dependencies",
    "Recurrent unit",
    "Hidden state",
    "Vanishing gradient problem",
    "Long short-term memory (LSTM)",
    "Gated recurrent units (GRUs)",
    "Transformers",
    "Self-attention mechanisms",
    "Neuroscience",
    "Cerebellar cortex",
    "Purkinje cells",
    "Granule cells",
    "Vestibulo-ocular reflex",
    "Feedback",
    "Feedforward",
    "Reverberating circuit",
    "Short-term memory",
    "McCulloch-Pitts neuron model",
    "Cycles",
    "Epilepsy",
    "Causalgia",
    "Recurrent inhibition",
    "Motor control",
    "Neural feedback loops",
    "Macy conferences",
    "close-loop cross-coupled perceptrons",
    "back-coupled perceptron networks",
    "Hebbian learning",
    "Associative Memory",
    "Ising model",
    "Glauber dynamics",
    "Sherrington\u2013Kirkpatrick model",
    "Spin glass",
    "Hopfield network",
    "Binary activation functions",
    "Continuous activation functions",
    "Bidirectional recurrent neural networks (BRNN)",
    "Encoder-decoder sequence transduction",
    "seq2seq architecture",
    "attention mechanisms",
    "transformers",
    "Neural History Compressor",
    "Unsupervised Stack of RNNs",
    "Description Length",
    "Conscious Chunker",
    "Subconscious Automatizer",
    "vanishing gradient problem",
    "backpropagation",
    "gradient magnitudes",
    "loss function",
    "forward propagation",
    "network depth",
    "hyperbolic tangent activation function",
    "exploding gradient problem",
    "supervised deep artificial neural networks",
    "feedforward networks",
    "recurrent networks",
    "backpropagation through time",
    "long-term dependencies",
    "Recurrent Neural Networks (RNNs)",
    "Sequential Data",
    "Feedforward Neural Networks",
    "Recurrent Connections",
    "Temporal Dependencies",
    "Recurrent Unit",
    "Hidden State",
    "Vanishing Gradient Problem",
    "Long Short-Term Memory (LSTM)",
    "Gated Recurrent Units (GRUs)",
    "Self-Attention Mechanisms",
    "Recurrent neural network",
    "Parallel fiber",
    "Feedforward structure",
    "Negative feedback",
    "recurrent connections",
    "Hebbian learning rule",
    "closed-loop cross-coupled perceptron networks",
    "Associatron",
    "statistical mechanics",
    "equilibrium",
    "Sherrington-Kirkpatrick model",
    "spin glass",
    "binary activation functions",
    "continuous activation functions",
    "Cognitive psychology",
    "LSTM",
    "BRNN",
    "Bidirectional LSTM",
    "Speech recognition",
    "Machine translation",
    "Language modeling",
    "Image captioning",
    "Encoder-decoder",
    "Sequence transduction",
    "Attention mechanisms",
    "Neural history compressor",
    "Unsupervised stack of RNNs",
    "Description length",
    "Generative model",
    "Automatic differentiation",
    "Backpropagation",
    "Conscious chunker",
    "Subconscious automatizer",
    "vanishing gradient",
    "Recurrent Semicircles",
    "Cerebellar Cortex",
    "Parallel Fiber",
    "Purkinje Cells",
    "Granule Cells",
    "Recurrent reciprocal connections",
    "Golgi's Method",
    "McCulloch-Pitts Neuron Model",
    "Recurrent Inhibition",
    "Negative Feedback",
    "Motor Control",
    "Neural Feedback Loops",
    "Macy Conferences",
    "Text-to-speech synthesis",
    "Multilingual Language Processing",
    "Convolutional neural networks (CNNs)",
    "Automatic image captioning",
    "seq2seq",
    "Description Length Minimization",
    "Automatic Differentiation",
    "Generative Model",
    "Jordan network",
    "Kaoru Nakano (1971)",
    "Shun'ichi Amari (1972)",
    "William A. Little (1974)"
  ]
}